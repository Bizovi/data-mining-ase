<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Decision Science Course – references</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02_causality/roadmap.html" rel="next">
<link href="./01_fundamentals/learning.html" rel="prev">
<link href="./img/favicon.jpeg" rel="icon" type="image/jpeg">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TNKQREJHQW"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-TNKQREJHQW', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean"
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Decision Science Course</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./references.html" aria-current="page">
 <span class="menu-text">2a. Bayes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./about_me.html">
 <span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://blog.economic-cybernetics.com/"><i class="bi bi-journal-bookmark" role="img">
</i> 
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/bizovi/decision-making"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/bizovi-mihai-56982abb/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">i. Introduction</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./philosophy.html" class="sidebar-item-text sidebar-link">ii. Course Philosophy</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./01_fundamentals/background.html" class="sidebar-item-text sidebar-link">1. Business Decisions and Probability</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_fundamentals/roadmap.html" class="sidebar-item-text sidebar-link">1. Syllabus &amp; Roadmap</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_fundamentals/background.html" class="sidebar-item-text sidebar-link">2. Data Science in Business Context</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_fundamentals/prerequisites.html" class="sidebar-item-text sidebar-link">3. Prerequisites: Why did you study all of that?</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_fundamentals/stat_foundations.html" class="sidebar-item-text sidebar-link">4. Foundations of Probability</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_fundamentals/learning.html" class="sidebar-item-text sidebar-link">5. Learning from Data: Intuition and Bias</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./references.html" class="sidebar-item-text sidebar-link active">2a. Applied Bayesian Statistics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_causality/roadmap.html" class="sidebar-item-text sidebar-link">1. Syllabus &amp; Roadmap</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link active">X. Resources</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html#groups-and-partial-pooling" class="sidebar-item-text sidebar-link">3. Groups and Partial Pooling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html#gaussian-linear-regression" class="sidebar-item-text sidebar-link">4. Gaussian Linear Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html#generalized-linear-models" class="sidebar-item-text sidebar-link">5. Generalized Linear Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html#hierarchical-glms" class="sidebar-item-text sidebar-link">6. Hierarchical GLMs</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./roadmap.qmd#module-iib-hypotheses-and-ab-testing" class="sidebar-item-text sidebar-link">2b. Hypotheses and A/B Testing</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_causality/roadmap_abtest.html" class="sidebar-item-text sidebar-link">1. Syllabus &amp; Roadmap</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_causality/ab_testing.html" class="sidebar-item-text sidebar-link">2. A/B Testing study resources</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./03_ml/roadmap.html" class="sidebar-item-text sidebar-link">3a. Probabilistic Machine Learning</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_ml/roadmap.html" class="sidebar-item-text sidebar-link">1. Syllabus &amp; Roadmap</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./04_engineering/roadmap.html" class="sidebar-item-text sidebar-link">4. Full Stack Data Apps</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_engineering/roadmap.html" class="sidebar-item-text sidebar-link">1. Syllabus &amp; Roadmap</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">X. Resources</a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#prerequisites-probability-and-python" id="toc-prerequisites-probability-and-python" class="nav-link active" data-scroll-target="#prerequisites-probability-and-python">Prerequisites: Probability and Python</a></li>
  <li><a href="#building-blocks-of-bayesian-statistics" id="toc-building-blocks-of-bayesian-statistics" class="nav-link" data-scroll-target="#building-blocks-of-bayesian-statistics">Building Blocks of Bayesian Statistics</a></li>
  <li><a href="#groups-and-partial-pooling" id="toc-groups-and-partial-pooling" class="nav-link" data-scroll-target="#groups-and-partial-pooling">Groups and Partial Pooling</a></li>
  <li><a href="#gaussian-linear-regression" id="toc-gaussian-linear-regression" class="nav-link" data-scroll-target="#gaussian-linear-regression">Gaussian Linear Regression</a></li>
  <li><a href="#generalized-linear-models" id="toc-generalized-linear-models" class="nav-link" data-scroll-target="#generalized-linear-models">Generalized Linear Models</a></li>
  <li><a href="#hierarchical-glms" id="toc-hierarchical-glms" class="nav-link" data-scroll-target="#hierarchical-glms">Hierarchical GLMs</a></li>
  <li><a href="#bayesian-machine-learning" id="toc-bayesian-machine-learning" class="nav-link" data-scroll-target="#bayesian-machine-learning">Bayesian Machine Learning</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content page-columns page-full column-body" id="quarto-document-content">



<p>I assume you read the conceptual stuff on this website, understood its use-cases, and the big picture of what to learn. Now, the question is <strong>where and how to start</strong> practicing, while catching up on the maths and programming.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="01_fundamentals/img/karate-kid.webp" title="Practice" class="img-fluid figure-img" style="width:90.0%">
</figure>
<p></p><figcaption class="figure-caption">Decision-Making, ML, and Causal Inference is hard. Practice the fundamentals with patience and care, develop competence. Then, a beautiful world will open up to you!</figcaption><p></p>
</figure>
</div>
</div></div><p>There are too many courses, tutorials, and datasets for practicing ML/Stats on the web: from didactic toy examples to industrial-scale machine learning. Since it is so hard to strike a balance between clarity, simplicity, and the use-case being interesting, realistic – you fill find here a list of <strong>carefully curated resources</strong>. Most of them have have <strong>code</strong>, <strong>data</strong>, and explained <strong>theory</strong> or methodology in a freely available e-book.</p>
<p>There is little point in replicating well-executed examples from other authors, just for the sake of consistency of code and style. I will, however, migrate examples not available in our frameworks or language of choice. Other times we can benefit from a significant improvement over the original presentation or improving code quality.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Right level of granularity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Instead of recommending whole books, which in this field are huge: 500-900p of dense material – in the listings below, you have them grouped by tool, use-case, in manageable-sized chunks.</p>
<p>They are ordered and organized in a way which facilitates a more linear, gradual, composable development of skills and understanding. <strong>Think of them as lego pieces.</strong></p>
</div>
</div>
<p>I made sure to include interesting examples and archetypal applications for each statistical tool and theoretical topic, so you can immediately apply the concepts you read about or watched during the lectures. However, the responsibility to practice falls entirely on the learner – I can just do my best to make your journey less frustrating and more efficient.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Often, you will have to combine various aspects of an use-case, model, method – from different sources, taking the best from each author.</p>
</div></div><section id="prerequisites-probability-and-python" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="prerequisites-probability-and-python">Prerequisites: Probability and Python</h2>
<p>We start from the foundations of probability, however, I do not cover calculus, linear algebra, or other mathematical tools for data analysis. We do much more programming than mathematics, therefore, the ability to code in a programming language like Python or R is a must.</p>
<div class="callout-warning callout callout-style-default callout-captioned page-columns page-full">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Need a crash course in probability and statistics?
</div>
</div>
<div class="callout-body-container callout-body page-columns page-full">
<p>If you understand and can explain the following ideas in a simple, yet rigorous way – you’re ready for the journey. Otherwise, if it feels shaky, here are some readings:</p>
<ul>
<li><strong>Probability Triple</strong> and <strong>Random Variables</strong> - a quasi formal introduction is written in <a href="https://course.economic-cybernetics.com/01_fundamentals/stat_foundations.html">this chapter</a> of the course website. From my experience, not many students have this understanding after their probability theory classes.</li>
<li><strong>Collectivity</strong> (“physical” structure), <strong>Statistical Population</strong>, <strong>Sample</strong> - where defining the population is the contract for our experiment, and the sampling process is critical. It is a much more nuanced topic than it looks, explained well and in great detail <a href="https://openintro-ims.netlify.app/data-design.html">here</a> and <a href="https://crumplab.com/statistics/04-SamplesPopulations.html">here</a>.</li>
<li><strong>Parameter</strong> (Estimand), <strong>Estimator</strong>, <strong>Estimation/Statistic</strong> - same resources as above <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>Stories behnid distributions: <span class="math inline">\(Bin(n, \theta), Pois(\lambda), Exp(\lambda), N(\mu, \sigma^2), \chi^2_k, t_k\)</span>. What about <span class="math inline">\(Beta(a, b)\)</span>, <span class="math inline">\(Gamma(k, \theta)\)</span>, and <span class="math inline">\(Dirichlet(\bar{\theta})\)</span> – what are they good for?
<ul>
<li>Look at some <a href="https://mathstat.slu.edu/~speegle/_book/probchapter.html#simulationsprob">examples with simulations</a> and <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view">stories / applications with more math</a> from Joe Blitzstein.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Although the perspective I take is Bayesian, I will take some time to cover and re-contextualize the Neyman-Pearson and Fisherian “frequentism”.</p></li></div></div>
</div>
</section>
<section id="building-blocks-of-bayesian-statistics" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="building-blocks-of-bayesian-statistics">Building Blocks of Bayesian Statistics</h2>
<div class="page-columns page-full"><p>We start simple, by modeling a single random variable <span class="math inline">\(Y\)</span>, choosing the appropriate distribution for each phenomenon, a prior for the parameters, doing simulations – then sampling from the posterior with <code>pymc</code>, <code>numpyro</code>, and <code>bambi</code>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;The R equivalents would be <code>stan</code> and <code>brms</code>, <code>rstanarm</code>. <code>bambi</code> and <code>brms</code> are a high-level API for most common models.</p></li></div></div>
<p>Limiting? Yes, as in reality we care about the relationship between random variables. However, we can get a lot of insight from thoughtful modeling the data generating process, which will serve as building blocks in more complicated and realistic models.</p>
<div class="callout-note callout callout-style-default callout-captioned page-columns page-full">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bayes Rule. Update your beliefs, often!
</div>
</div>
<div class="callout-body-container callout-body page-columns page-full">
<div class="page-columns page-full"><p>Any introduction to the subject will work out, a few excellent ones being Chapter 1/2 of <a href="http://www.stat.columbia.edu/~gelman/book/">BDA3</a>, Chapter 1/2 of <a href="https://www.bayesrulesbook.com/chapter-2.html">Bayes Rules</a>, and Chapter 1/2 of <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> If you prefer videos, enjoy the 3Blue1Brown <a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">visual masterpiece</a> on how to think like a Bayesian or <a href="https://www.coursera.org/learn/statistical-inferences/lecture/R6nV5/bayesian-thinking">here</a>.</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;You will notice in the callouts that I point out where to read the theory – for a conceptual understanding and to figure out the mathematical details</p></li></div></div>
<ul>
<li>(BDA3, Ch1): <strong>Football spreads</strong>, that can be estimated from <a href="http://www.stat.columbia.edu/~gelman/book/data/football.asc">data about matches</a>. What is the probability that a team wins? Are experts right, on average?
<ul>
<li>If you’re into betting and sports, can you replicate the analysis on other datasets? What are your options for data collection?</li>
<li>For brevity, I won’t elaborate much from now on, how to take an use-case and example to its limit. <strong>If you’re passionate about a particular topic – go for it!</strong></li>
</ul></li>
<li>(BDA3, Ch1): <strong>Spelling correction</strong>, based on <a href="http://norvig.com/ngrams/">empirical frequencies</a> provided by Peter Norvig. As in the previous case-study, you will have to code it up and figure it out for yourself – it is good for a warm-up, but challenging enough to keep you occupied.</li>
<li>(<a href="https://projects.iq.harvard.edu/stat110/home">Probability 110</a>): <strong>Medical testing</strong> for rare diseases, hypothetical example with code in my <a href="https://github.com/Bizovi/decision-making/blob/main/playground/02_bayes.ipynb">course repository</a>. We use the same idea to reason about how confident are we our code has no bugs.
<ul>
<li>If you remember the Covid-19 rapid tests and their confusion matrices printed on instructions, you could’ve applied the same idea!</li>
</ul></li>
</ul>
<p>For the simplest models, one approach of comparing different hypotheses, is Bayes Factors. However, these do not translate well in practice for more sophisticated, multilevel models. You can look in the following courses <a href="https://www.coursera.org/learn/statistical-inferences/supplement/IPkZK/assignment-2-2-bayesian-statistics">here</a> and <a href="https://www.coursera.org/learn/bayesian/home/week/3">here</a> for the theory and examples.</p>
</div>
</div>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">Or maybe you’re passionate about biology, where you could apply it for Mendelian genetics and think about the mystery of deadly genes persistence</span></div></div>
<p>We applied Bayes rule and got some insightful results in three totally different domains. However, we weren’t doing neither statistics, nor inference – but got into the right mindset. It is a good opportunity to brush off the shelves some computational and mathematical tools. Now it’s time for full-luxury Bayes and the simplest cases of inference.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Beta-Binomial Model. Estimating proportions
</div>
</div>
<div class="callout-body-container callout-body">
<p>I know, I know, the coin-tossing – simple, yet fundamental and found anywhere there is a binary outcome <span class="math inline">\(Y_i \in \{0, 1\}\)</span>. There are many ways to estimate the success probability <span class="math inline">\(\theta\)</span>, when we observe <span class="math inline">\(k\)</span> successes from <span class="math inline">\(n\)</span> trials.</p>
<ul>
<li><a href="https://www.bayesrulesbook.com/chapter-3.html">In Bayes’ Rules</a> is a detailed exposition of the theory, with examples about Michelle’s election support and Milgram’s experiment.</li>
<li>Share of biking traffic in different neighborhoods (BDA3, Ch2, Pr. 8)</li>
<li>A/B Testing for proportions in <a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC_current.ipynb">BMH</a>. Just remember that experiment design is much more nuanced than performing such inference or a test.</li>
<li>Political attitudes in 2008 Election <a href="http://www.stat.columbia.edu/~gelman/book/data/">data</a> (BDA3, Ch2, Pr. 21)</li>
<li>Proportion of female births, given prior information. (BDA3, Ch2)</li>
</ul>
<p>There are many more applications, but the ones below require more research and work. They are open ended and you can take these topics very far.</p>
<ul>
<li>The debate on the hot hand phenomenon is not yet over. <a href="https://blogs.cornell.edu/info2040/2018/11/29/hothand/">Here</a> are the <a href="https://statmodeling.stat.columbia.edu/2015/07/09/hey-guess-what-there-really-is-a-hot-hand/">bayesians weighting in</a> and some <a href="https://www.thecut.com/2016/08/how-researchers-discovered-the-basketball-hot-hand.html">new research</a>.</li>
<li>Basketball shot percentages and true shooting, brilliantly explained by <a href="https://youtube.com/playlist?list=PLtzZl14BrKjTJZdubjNEY5jU0fGOiy51x">thinking basketball</a> in a playlist about NBA statistics.</li>
<li>Important problem in ecology: estimating size of population based on samples (BDA3, Ch3, Pr. 6). The challenge is that in <span class="math inline">\(Bin(\theta, N)\)</span> both parameters are unknown. Here is an <a href="https://pluto.huji.ac.il/~galelidan/52558/Material/Raftery.pdf">old paper</a>.</li>
<li>Confidence intervals and <a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC_current.ipynb">lower bound</a> for reddit posts like/dislike ratio. Read more about simple ranking algorithms and the issues of sample size: <a href="https://www.evanmiller.org/how-not-to-sort-by-average-rating.html">reddit</a>, <a href="https://news.ycombinator.com/item?id=1781013">hacker news</a>. It is a good opportunity to work with the reddit API in order to collect data about posts and comments.</li>
</ul>
</div>
</div>
<p>The next step is learning how to model count data, which will open up to us applications of a different flavor. It is not a coincidence that when learning linear regression, we will extend it to poisson and logistic regression.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">You can notice how the issues of sample size creep in, as well as how to properly model variation within and between groups. I recommend you look up again the CLT, in the next section</span></div></div>
<p>Note that <strong>prior choice</strong> and justification is an art and science: you will have to learn and practice how to articulate assumptions and encode your domain knowledge into the priors. There is no universal recipe, but there are some guiding principles.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Poisson Distribution. Gamma-Poisson Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>Counts of independent events in a unit of (space/time/…), with a low probability. You can review the maths <a href="https://www.bayesrulesbook.com/chapter-5.html#gamma-poisson-conjugate-family">here</a>. Below is a list of applications you can practice on:</p>
<ul>
<li>Deaths by horses in Prussian Army. Here is the <a href="https://rpubs.com/SmilodonCub/567089">historical data</a> and a <a href="https://towardsdatascience.com/poisson-distribution-from-horse-kick-history-data-to-modern-analytic-5eb49e60fb5f">blog post</a> if you need a refresher on Poisson distribution.</li>
<li>Asthma mortality (BDA3, Ch2)</li>
<li><a href="https://www.briancallander.com/posts/bda3/chapter_02_exercise_13.html">Airplane fatal accidents</a> and passenger deaths</li>
<li>Estimating WWII <a href="https://en.wikipedia.org/wiki/German_tank_problem#Bayesian_analysis">production of German</a> tanks based on samples captured</li>
<li>Comparing football teams and <a href="https://allendowney.github.io/ThinkBayes2/chap08.html">goals in football matches</a></li>
<li><a href="https://allendowney.github.io/ThinkBayes2/hospital_birth_rate.html">Comparing birth rates</a> in two hospitals</li>
</ul>
<p>Check out the link functions for more sophisticated models. Also, in the examples above, we estimate the groups separately (corresponding to no pooling) – there are better ways. Also, you will see a poisson example of how to take into account the sample size</p>
</div>
</div>
<p>The next examples are a little detour, to appreciate the flexibility of the modeling approach we’re taking. We’re building upon previous models, by inferring which rate <span class="math inline">\(\lambda_1\)</span> or <span class="math inline">\(\lambda_2\)</span> is the most plausible at a given point in time. This way, we add complexity and realism to the model, by incorporating knowledge about the phenomenon we’re interested in.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">Ideally, we would leverage models which work well with time-series, like Hidden Markov Models. There is also a large literature in mining subsequences in a time series.</span></div></div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Poisson changepoint detection
</div>
</div>
<div class="callout-body-container callout-body">
<p>Estimating rates, modeling a structural shift/change is a relevant, challenging, and unsolved problem in many fields. The models below are too simplistic to be useful in practice, but they capture the essence of real dynamics: things change not only continuously, but also structurally.</p>
<ul>
<li><a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters">Coal Mining disasters</a>, pymc</li>
<li><a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC_current.ipynb">Text Messages</a>, pymc</li>
<li><a href="https://sidravi1.github.io/blog/2018/05/22/what-happened-in-2006">U.S. School Shootings</a>, is there an increase in attempts and occurences?</li>
</ul>
</div>
</div>
<p>By now, you encountered many simple Bayesian models, so it’s useful to contrast it with the Fisherian and Neyman-Pearson (frequentist) approaches. The motivation is simple: the likelihood approach is widely used in Machine Learning (and Statistical Learning) teaching and practice. A nuanced understanding of frequentist methods will improve the way you ask statistical questions, perform and design experiments.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">In the courses I teach, I dedicate quite a lot of time on how not to fall into the most common pitfalls when applying frequentist methods. It’s an useful skill when critically reading the literature.</span></div></div>
<p>When we get to the topic of A/B testing and experiment design, we will unavoidably stumble upon a few fascinating philosophical questions in relation to the nature of evidence. A quick overview of the below resources will give you most background you need to understand the heck those philosophical positions are about.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bayesian vs Frequentist vs Fisherian Inference
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>I like very much the following metaphor, explained <a href="https://www.coursera.org/learn/statistical-inferences/lecture/qC3A1/frequentism-likelihoods-bayesian-statistics">here</a>, of looking at these 3 approaches as a: Path of Action, Path of Devotion, Path of Knowledge.</li>
<li>Watch this <a href="https://www.youtube.com/watch?v=LYcu3LoGqKc">lecture</a> by Zoltan Dienes to get a sense of the orthodox, Neyman-Pearson approach: its power and limitations.</li>
<li>Likelihood - either from previous books or <a href="https://rpsychologist.com/likelihood/">this interactive visualization</a>. You can use the <a href="https://www.coursera.org/learn/statistical-inferences/lecture/8yZDk/likelihoods">lecture and lab</a> from TU Eindhoven courses mentioned below.</li>
<li>Bayesian vs Frequentist inference - <a href="https://hastie.su.domains/CASI_files/PDF/casi.pdf">chapter 2, 3, 4</a> in Computer Age Statistical Inference. This <a href="https://www.youtube.com/watch?v=NHFfJEvzPIo">lecture</a> by Zoltan Dienes contrasts Bayes Factors vs classical methods in t-test situations.</li>
<li>The best teaching of (mostly) frequentist statistics I know of is in these two courses by TU Eindhoven and it is well worth your time:
<ul>
<li>Improving your statistical questions, <a href="https://www.coursera.org/learn/improving-statistical-questions">coursera</a></li>
<li>Improving your statistical inferences, <a href="https://www.coursera.org/learn/statistical-inferences">coursera</a></li>
</ul></li>
</ul>
</div>
</div>
<p>We already worked with multiple parameters, even touching upon the relationship between two variables: counts and time <span class="math inline">\(Y_t\)</span>, but not really – it’s more helpful to think about that in terms of stochastic processes. Therefore, we need a new tool, which is a link function, a nonlinear transformation <span class="math inline">\(g(x)\)</span> which maps <span class="math inline">\(X\)</span> to the correct support of <span class="math inline">\(Y\)</span>. I recommend to introduce this before jumping into linear regression (LR), in order not to have the (flawed) impression that the LR is the only game in the town.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Link functions. Golf case-study
</div>
</div>
<div class="callout-body-container callout-body">
<p>The domain/geometry inspired, custom model is presented in <a href="https://www.pymc.io/projects/examples/en/latest/case_studies/putting_workflow.html">pymc</a> version, <a href="https://mc-stan.org/users/documentation/case-studies/golf.html">stan</a> by Andrew Gelman, and <a href="https://avehtari.github.io/ROS-Examples/Golf/golf.html">stan</a> by Aki Vehtari. It is modeling the relationship between distance and the probability of put, which is nonlinear and the sigmoid function won’t work well for this case.</p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>This is an appropriate point to introduce linear regression and logistic regression. It is not too early, given the importance of those tools, however the presentation should be practical and pretty high level, as there is much nuance to deep-dive later.</p>
</div></div><p>Of course, we cannot forget about the Normal/Gaussian distribution, which is so prevalent in nature and pops up everywhere in the statistical practice. It is the building block of many following models. Remember the key idea of the expectation of independent random variables and estimating the mean from samples. Also, keep in mind any time you’re doing regression that it’s all about the conditional expectation <span class="math inline">\(\mathbb{E}_\theta[Y|X=x]\)</span>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
InverseGamma-Normal
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>You can find the theory and mathematical exposition in Bayes Rules, with a case-study of football concussions <a href="https://www.bayesrulesbook.com/chapter-5.html#normal-normal-conjugate-family">study</a></li>
<li>Speed of light experiment (BDA3, Ch3), <a href="http://www.stat.columbia.edu/~gelman/book/data/">data</a>. You will find here and in any statistics textbook the cases of known and unknown variance, and how the <span class="math inline">\(t_k\)</span> test is derived from the first principles.</li>
<li>As in the case of proportions, we can use the model above to model the difference between the means of two independent groups.</li>
</ul>
<p>TODO: more examples from science, business, and human behavior</p>
</div>
</div>
</section>
<section id="groups-and-partial-pooling" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="groups-and-partial-pooling">Groups and Partial Pooling</h2>
<p>We already worked with groups in the case of difference in means (proportions and continuous variables) and making inferences for three and more groups, treating them as separate. We will see that such an approach is called “no pooling”.</p>
<p>In the traditional teaching of statistics, the above would be covered by t-test, tests for proportions, and when it comes to groups, by ANOVA. If you were lucky, these would’ve been treated as particular versions of linear regression, like in “Most common statistical tests are linear models”.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>One more reason for this is that groups and categorical variables do not receive the deserved, nuanced exposition in linear regression. Also, comparing groups is so widespread, that having a tool to deal with the challenges which it poses is immediately useful in your practice inside any firm.</p>
</div></div><p>In this section, the goal is to show the idea of hierarchical models and <strong>partial pooling</strong>. I agree with BDA3 approach to teach it before regression, as the latter needs a lot of nuance and a long time to learn to apply properly.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Beta-Binomial for groups. Normal Approximation
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is no point in repeating all from the first section, as it is straightforward to apply for groups, as you’ll see when we compare it with a hierarchical approach.</p>
<p>However, it is a good chance for a frequentist detour, to the Agresti-Coull confidence intervals, <span class="math inline">\(\chi^2\)</span> tests of independence, and nonparametric tests for proportions in multiple groups.</p>
<ul>
<li>Erica Ma has a great <a href="https://youtu.be/Pt37qA351yk">talk</a> for hypothesis testing for 2+ groups.</li>
<li>I think the authoritative resource on Bayesian versions of the distribution-free methods for hypothesis testing is “Bayesian Statistics for Experimental Scientists”, found <a href="https://mitpress.mit.edu/9780262044585/bayesian-statistics-for-experimental-scientists/">here</a>. Unfortunately, it is expensive, so I suggest you look at the table of contents and search for the implementations elsewhere.</li>
</ul>
</div>
</div>
<p>Foreshadowing the module 2b on A/B testing is the topic of Multi-Armed Bandits. There are cases in which we are testing multiple groups, e.g.&nbsp;which images to show on the website, and we do it at scale. Moreover, we want to experiment countinuously and automatically, trading off between exploration and exploitation to maximize the payoff or minimize regret.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">MABs are a big topic in itself, and a very narrow, particular example of reinforcement learning. It can be very powerful when carefully designed and applied appropriately.</span></div></div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
A/B Testing and Multi-Armed Bandits
</div>
</div>
<div class="callout-body-container callout-body">
<p>I suggest you start from the didactic examples in Bayesian Methods for Hackers, <a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Ch6_Priors_PyMC_current.ipynb">Chapter 6</a>. If you have an use-case in which this fits perfectly, you can check out the theory and more variants to implement it in more specialized resources.</p>
</div>
</div>
<p>We encountered the problem of sample size when ranking reddit posts, but it deserves a few lectures in itself. Once you master a few methods of reasoning about <span class="math inline">\(n\)</span>, you can cut through so much bullshit in media, research, and literature.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The most dangerous equation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I think that <a href="http://assets.press.princeton.edu/chapters/s8863.pdf">“The most dangerous equation”</a> is a must read for anyone, not just practicing scientists and statisticians. You can see below two resources for the mathematical background of LLT, CLT, and some key properties of estimators.</p>
<ul>
<li>Central Limit Theorem (Kolmogorov), deMoivre - the most dangerous equation. Asymptotics. <a href="https://mathstat.slu.edu/~speegle/_book/SimulationRV.html#centrallimittheorem">the theorem and simulations here</a></li>
<li>Estimator properties: Bias, Consistency, Efficiency - you can find an accessible explanation with R code in <a href="https://openforecast.org/sba/estimatesProperties.html">openforecast</a></li>
</ul>
<p>Continuing on the reddit examples, there are some amazing case-studies in the “Calling Bullshit” website and book. One of them is exactly such a ranking problem: <a href="https://www.callingbullshit.org/case_studies/case_study_barbecue.html">best barbecue in the states</a>. I recommend you watch the whole playlist and work through the case studies: it is fun and an essential skill – to call out the bullshit.</p>
</div>
</div>
<p>For the continuity with the previous “Building Blocks” section, here are a few example for groups, where the dependent variable is following the Poisson distribution. In this case, the novelty is choosing a gamma prior based on the sample size information of each group. If you think we can do better than this trick – you’re totally right.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Gamma-Poisson for groups
</div>
</div>
<div class="callout-body-container callout-body">
<p>The models become more complicated as we attempt to estimate parameters for <strong>each group</strong> of <span class="math inline">\(n\)</span> observations:</p>
<ul>
<li>Kidney Cancer rates, with priors chosen in accordance to the sample size (BDA3, Ch2). <a href="https://robinryder.wordpress.com/2019/09/13/reproducing-the-kidney-cancer-example-from-bda/">An R visualization</a></li>
<li><a href="https://sidravi1.github.io/blog/2018/06/15/empirical-and-hierarchical-bayes">Guns and suicides</a>, with ideas from empirical and hierarchical Bayes.</li>
</ul>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Just as a remark, <strong>complete pooling</strong> is when we ignore the fact that we have groups, and make one, global estimate. Of course, it would fall in the category of underfitting or model mis-specification, if the categories or groups are relevant.</p>
</div></div><p>Finally, we’re ready to see how partial pooling and hierarchical (multilevel) models are such an important and powerful innovation, to the point where some practitioners argue (and I agree), that it should be the default way of (regression) modeling. Meaning, a strong justification is needed why your model doesn’t need that structure or modeling of heterogeneity. Keep in mind this advice, but always start with the most simple models when iterating towards the final one.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Hierarchical Models for Groups
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Gaussian</strong>. The main example is a classic: modeling the level of radon in houses on the first floor and basement, where the groups are counties (BDA3). We will see this example again in the Hierarchical GLMs section, where predictors at house-level and county-level are added.</p>
<ul>
<li><a href="https://github.com/omarfsosa/tech-talk-hierarchical-models">Omar Sosa - Practical introduction to Bayesian hierarchical modeling</a> with numpyro, focuses on exactly the idea we need.</li>
<li>Primary code reference: <a href="https://www.pymc.io/projects/examples/en/latest/case_studies/multilevel_modeling.html">pymc</a> - A Primer on Bayesian Methods for Multilevel Modeling</li>
</ul>
<p><strong>Beta-Bionmial</strong> examples:</p>
<ul>
<li>Eric Ma tutorial at pycon, about <a href="https://bambinos.github.io/bambi/notebooks/hierarchical_binomial_bambi.html">baseball batting</a>, and the equivalent <a href="https://num.pyro.ai/en/stable/examples/baseball.html">numpyro code</a>. Another baseball example is a <a href="https://www.pymc.io/projects/examples/en/latest/case_studies/hierarchical_partial_pooling.html">classic by Efron</a>, implemented in pymc.</li>
<li>Police shooting training – detecting race bias. A full Bayesian workflow in <a href="https://bambinos.github.io/bambi/notebooks/shooter_crossed_random_ANOVA.html">bambi</a>.</li>
<li>Another classic example is about Rat Tumors experiment, implemented in <a href="https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-hierarchical-binomial-model.html">pymc</a>, from BDA3, Ch5.</li>
</ul>
<p>A great <strong>Gamma-Poisson</strong> example is presented by Richard McElreath in Statistical Rethinking lectures: “Starbucks coffee-shops waiting time in queue”.</p>
</div>
</div>
<p>One of my favorite business applications of the ideas outlined in this section, is in the seemingly innocent problem of computing the lifetime value of a customer. Just to show how tricky this problem is, there is a niche, but extensive literature on the probem, starting from 2000-2005 by Rossi, Fader, and Hardie, which is entering into the mainstream only now, by 2020-2023.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The below is by far not the only model: there are versions for contractual and non-contractual settings with different assumptions. The culmination of this research, in my opinion, is Eva Ascarza’s 2013 <a href="http://www.evaascarza.com/ascarza_hardie_13.pdf">paper</a>.</p>
</div></div><div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Estimating Customer Lifetime Value
</div>
</div>
<div class="callout-body-container callout-body">
<p>Model: Combining Gamma-Poisson and Beta-Binomial, with parameters at customer level. The math and code in pymc3 can be found <a href="https://sidravi1.github.io/blog/2018/07/08/fader-hardie-clv">here</a>.</p>
</div>
</div>
</section>
<section id="gaussian-linear-regression" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gaussian-linear-regression">Gaussian Linear Regression</h2>
<p>There is a converging consensus that Linear Regression is the most important thing to master in statistics. I mean it in the most general sense – not only knowing model details, assumptions, extensions and limitations, but how to use it effectively in practice in an end-to-end workflow.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">To avoid naming confusions in the context of GLMs we’re going to study, and the fact that Regression can be done by many classes of models, I call the good old Linear Regression – “Gaussian Linear Regression”</span></div></div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Examples. Introduction to the workflow
</div>
</div>
<div class="callout-body-container callout-body">
<p>I highly recommend the (freely available) book “Regression and Other Stories”, by Andrew Gelman, Jennifer Hill and Aki Vehtari. The latter has a <a href="https://avehtari.github.io/ROS-Examples/">course website</a> for this book, with the examples coded in stan.</p>
<p>Also, the second resource, which I would say is even beter, takes on the perspective of causal inference from the very beginning of studying regression. In the <a href="https://xcelab.net/rm/statistical-rethinking/">course homepage</a> you will find links to the pymc port, slides, and video lectures from 2023.</p>
<ul>
<li>Eugene-Springfield community sample data: OCEAN personality traits as related to drug usage – no categorical variables, <span class="math inline">\(Y\)</span> distribution normal-ish. Demo in <a href="https://bambinos.github.io/bambi/notebooks/ESCS_multiple_regression.html">bambi</a>.</li>
<li>Educational outcomes for hearing-impaired children, in <a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-1-educational-outcomes-for-hearing-impaired-children">pymc</a></li>
<li>Dangers of spurious correlation and accounting for them are displayed in the <a href="https://num.pyro.ai/en/stable/tutorials/bayesian_regression.html">marriages and waffles</a> example by McElreath – written in numpyro, but there is also a pymc version.</li>
<li><a href="https://www.pymc.io/projects/examples/en/latest/case_studies/moderation_analysis.html">pymc moderation analysis</a>: muscle mass and age</li>
<li>This case study about 100m runs from Calling Bullshit, shows the <a href="https://www.callingbullshit.org/case_studies/case_study_gender_gap_running.html">dangers of extrapolation</a> beyond the range of <span class="math inline">\(X\)</span> that the models were trained on.</li>
</ul>
</div>
</div>
<p>I have a third favorite book, which is freely available, called “Beyond Multiple Linear Regression”. It covers all that we discuss here, but from a frequentist perspective. Despite that, the examples are amazing and it has enough of theory to make it a self-contained resource – therefore, makes a perfect candidate for a port in pymc/numpyro. For a review of linear regression, take a look at the <a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html">Kentucky derby horse race</a>.</p>
<div class="callout-tip callout callout-style-default callout-captioned page-columns page-full">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Common statistical tests are linear models
</div>
</div>
<div class="callout-body-container callout-body page-columns page-full">
<div class="page-columns page-full"><p>Now, once you got the absolute basics of regression, it’s important to realize that a lot of seemingly unrelated statistical tests in frequentist statistics are particular versions of linear models<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;This will instantly systematize the zoo of tests, when to use them and what they do. DO NOT TRY TO MEMORIZE THEM! Instead, think very carefully about your use-case.</p></li></div></div>
<ul>
<li><a href="https://www.pymc.io/projects/examples/en/latest/case_studies/BEST.html">pymc</a> - Krutsche fake data drug trial, t-test, <a href="https://bambinos.github.io/bambi/notebooks/t-test.html">bambi</a>.</li>
<li><a href="https://lindeloev.github.io/tests-as-linear/#1_the_simplicity_underlying_common_tests">Common statistical tests are linear models</a> and the <a href="https://www.georgeho.org/tests-as-linear/">python port</a></li>
</ul>
</div>
</div>
<p>Up until this point we took the sampling from the posterior provided by the probabilistic programming languages for granted, as magically converging to the true distribution. We also used very rudimentary methods of comparing models (via Bayes Factors).</p>
<p>However, things go wrong in practice all the time: poorly specified and parametrized models lead to computational problems. It is important to be able to diagnose the chains, and even more, actively critique and interogate your models. At some point, you have to get an understanding of what MCMC, HMC does under the hood – and when you would trade off the accuracy for faster, approximate inference.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Splines and Nonlinear Transformations
</div>
</div>
<div class="callout-body-container callout-body">
<p>The linearity assumption in linear regression is often under-emphasized, as pointed out by the trio of RoS in a <a href="https://learnbayesstats.com/episode/20-regression-and-other-stories-with-andrew-gelman-jennifer-hill-aki-vehtari/">podcast episode</a>. Linear Models are linear in parameters, but we should always think of the appropriate transformations of <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span>.</p>
<ul>
<li><a href="https://bayesiancomputationbook.com/markdown/chp_05.html#id36">Splines</a> from Osvaldo Martin, on Bike Ridership (UCI data, bike sharing)</li>
<li><a href="https://www.pymc.io/projects/examples/en/latest/case_studies/spline.html">Splines</a>, from Statistical Rethinking, on Cherry Blossoms Data</li>
<li>Be careful and recognize when you should be using a log-log model, expecially since power laws are so widespread in nature and human behavior.</li>
</ul>
</div>
</div>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">There are objections to the Bias-Variance decomposition when seen as a tradeoff, in the context of Deep Learning – however, in the most general sense, it is a universal problem not only in statistics, but also for human cognition.</span></div></div>
<p>By introducing splines and nonlinear transformations, we can easily increase the dimensionality of <span class="math inline">\(X\)</span>, with respect to <span class="math inline">\(n\)</span> – which causes massive problems for inference. This is the appropriate moment to introduce a fundamental tradeoff between model complexity and out-of-sample predictive performance.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Model Critique and Evaluation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Checking domain and modeling assumptions, critiquing the models we build are one of the most difficult aspect of statistical practice. It is hard to give a tutorial for this, so I just suggest you read Richard McElreath’s Statistical Rethinking chapters on this topic, along with the more theoretical BDA3, Part II.</p>
<ul>
<li>For an intuitive process which can structure your modeling and critique, I suggest McElreath’s “Drawing the Bayesian Owl”.</li>
<li><a href="https://bayesiancomputationbook.com/markdown/chp_02.html">Chapter 2</a> of Bayesian Computation in Python, by Osvaldo Martin (BCP) is one of the best at exemplifying these concepts.</li>
<li>Prior and posterior checks <a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html#posterior-predictive">pymc docs</a></li>
<li>An end-to-end example in BCP, modeling the <a href="https://bayesiancomputationbook.com/markdown/chp_09.html">flight delays</a>.</li>
</ul>
<p>The frequentists and statistical learning approach have well-established tools for asessing the out-of-sample predictive performance. The fact that training is fast, they can easily use cross-validation and even bootstrap. A recent innovation in Bayes is LOO-PIT, an approximation to leave-one-out cross-validation, which leverages the fact that we have posterior sampling.</p>
<ul>
<li>A reminder of bootstrap, that is what the frequentists might do - <a href="https://hastie.su.domains/CASI_files/PDF/casi.pdf">chapter 10</a> or <a href="https://hastie.su.domains/Papers/ESLII.pdf">page 249</a>. It is a powerful tool to have in your arsenal anyways.</li>
<li><a href="https://bayesiancomputationbook.com/markdown/chp_11.html#loo-in-depth">Here</a> are the technical details of LOO-PIT, in the context of model comparison.</li>
</ul>
</div>
</div>
<p>Once we are able to critique and evaluate a single model, it makes perfect sense to compare models of increasing complexity and with alternative assumptions. The following uses the same tools as before, but in a iterative workflow.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Model Comparison and Selection
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-model-selection.html">pymc</a>. - Model selection with fake data and polynomials</li>
<li>Chapter 3 and 4 of BCP are beautiful, where linear and logistic regression are introduced, along with questions of interactions, nonlinear transformations, hierarchical models and <a href="https://bayesiancomputationbook.com/markdown/chp_04.html">model choice</a>.</li>
<li>Bias-Variance tradeoff - For an intuitive explanation, watch <a href="https://work.caltech.edu/lectures.html">lecture 8, slides</a>. See how this <a href="https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update">tradeoff needs an update</a> for the modern machine learning.</li>
</ul>
</div>
</div>
<p>There are cases when the problem is in sampling and one common cause, besides mis-specification, are models with bad posterior geometry (from the perspective of samplers). As a rule of thumb “round and smooth” is good. A prevalent solution in literature is to use better parametrizations, but in order to fix your problem, you first have to be aware that it might be the case.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">These examples in 2D are intuitive, however things become much more complicated and counterintuitive in multidimensional parameter spaces. The long story short, in practice, is to use heavier regularization (via stronger priors) in those cases.</span></div></div>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bad posterior geometry. Reparametrization
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Technical: (mu, sigma) - Neal’s Funnel <a href="https://num.pyro.ai/en/stable/examples/funnel.html">numpyro</a>, there is equivalent pymc</li>
<li>More solutions in numpyro for bad <a href="https://num.pyro.ai/en/stable/tutorials/bad_posterior_geometry.html">posterior geometry</a></li>
</ul>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Regularization and Variable Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p>A big topic in ML is regularization and encouraging sparse solutions (models). The point is that in highly-dimensional spaces, only a small subset of variables is relevant. Bayesians have sneaky methods of constructing prior which act as a mechanism for variable selection.</p>
<p>There is an additional topic of robuts methods (to outliers). Some machine learning models are robust(ish) out of the box: quantile regression, gradient boosted trees. Bayesians use heavier-tailed distribution to achieve a similar effect.</p>
<ul>
<li><a href="https://www.kaggle.com/code/melondonkey/bayesian-spike-and-slab-in-pymc3/notebook">Spike and Slab kaggle</a></li>
<li><a href="https://austinrochford.com/posts/2021-05-29-horseshoe-pymc3.html">Horseshoe prior</a> pymc3</li>
<li><a href="https://bambinos.github.io/bambi/notebooks/t_regression.html">Robust LR</a>, bambi, simulated data, also is in pymc</li>
</ul>
</div>
</div>
<p>I can’t resist to suggest a case-study from finance, the good old portfolio returns optimization. The methods used in practice are much more sophisticated than the below, but it’s useful to be aware of the fundamentals. The Wishart distribution, used as a prior for positive semi-definite matrices (covariation), can be an useful building block in more complicated models to capture the covariation structure, let’s say, from different time series.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">As a very advanced aside, I am fascinated by Michael I. Jordan’s approach to multidimensional time series modeling, where the covariation structure is evolving in time.</span></div></div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Portfolio optimization a la Markowitz
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Technical: Wishart and Portfolios from <a href="https://nbviewer.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Ch6_Priors_PyMC3.ipynb">BMH</a> – can also do optimization ala markowitz, but with uncertainty from posteriors</li>
</ul>
</div>
</div>
<p>If you got to this last topic, probably months have passed. There are a reason universities dedicate entire semesters for regression – there are so many nuances to get right, from both practical and theoretical perspective. One of these complexities is how to handle missing data, which deserves a course on its own.</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Modeling how Data goes Missing
</div>
</div>
<div class="callout-body-container callout-body">
<p>In Bayesian Statistics, we also have to be very explicit and declare our assumptions about how the data went missing – which is a hard, but beneficial thing to do. The good news – missingness is treated like an unobserved variable and is subject to the same inferences we’ve been doing before. We just need to pick the right model of missingness for each individual case.</p>
<ul>
<li>Missing data imputation, <a href="https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html">pymc</a>, both for linear and hierarchical</li>
<li>Discrete missing data imputation <a href="https://num.pyro.ai/en/stable/tutorials/discrete_imputation.html">numpyro</a>, with simulated data and causal graphs</li>
<li>Pymcon - missing data <a href="https://gist.github.com/junpenglao/7c505c6c76f99c928a4e2c1161cff43a">tutorial</a> in pymc3</li>
<li><a href="http://stronginference.com/missing-data-imputation.html">Missing Data Imputation</a></li>
</ul>
</div>
</div>
</section>
<section id="generalized-linear-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="generalized-linear-models">Generalized Linear Models</h2>
<p>Not every phenomena we model is continuous or appropriate for gaussian linear regression, but we can easily extend it with the building blocks we learned at the beginning. It’s all about those link functions and relaxing some of the assumptions of the linear regression. I want to point out how the following mirrors the simple, single-variable models we first practiced on.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">I do not want to make the transition from gaussian case to logistic, poisson and other variants sound trivial. Their behavior, evaluation, and interpretation is different and distinct. You can not equivocate them.</span></div></div>
<p>As a theoretical background of what connects these models into one single family, hence the name of GLMs, I suggest you read the section in the “Beyond MLR” about the <a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-glms.html">exponential family</a>. Richard McElreath has similar information-theoretic arguments in his lectures on Statistical Rethinking.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Logistic Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>As a fun aside about ML, logistic regression is the only model which is calibrated out-of-the-box, meaning the scores for the classification can be interpreted as probabilities, due to its specific loss function.</p>
<p>For the rest of the cases, you would need to calibrate via an isotonic or piecewise-linear regression, on a separate, validation dataset. So, going back to the practice, here are some examples using the logistic regression:</p>
<ul>
<li>Beetles survival by concentration of chemical, via <a href="https://bambinos.github.io/bambi/notebooks/alternative_links_binary.html">bambi</a> – tries out different link functions and is a simple example to get started with.</li>
<li>Vote intention by age in the U.S., via <a href="https://bambinos.github.io/bambi/notebooks/logistic_regression.html">bambi</a>, 2016 ANES data, 1200 samples.</li>
<li>Socio-economic influences on income bracket (<span class="math inline">\(\ge \$50k\)</span>), also does model comparison, implementation in <a href="https://bambinos.github.io/bambi/notebooks/model_comparison.html">bambi</a></li>
<li>Multinomial regression <a href="https://bambinos.github.io/bambi/notebooks/categorical_regression.html">iris</a>, via bambi, with the most boring dataset, but you’ll see that you have a solution for the multiclass classification without the multiple comparisons.</li>
</ul>
</div>
</div>
<p>When you have a numeric target variable, think twice if it is appropriate for the gaussian distribution. Sometimes, what we model is really, counts, or non-negative, or monotonically increasing with respect to a <span class="math inline">\(X_i\)</span> (that alone deserves its own case-study). Sometimes, even poisson doesn’t work out due to overdispersion or zero-inflation, and it is not a rare exception, especially when modeling aspects of customer behavior.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Poisson Regression
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.bayesrulesbook.com/chapter-12.html">Number of laws</a> regarding equality, which includes a discussion for the issue of overdispersion. Not sure at all that this would be a prototypical DGP story for a Poisson.</li>
<li>Stop and frisk data from BDA3 and RoS, the <a href="https://omarfsosa.github.io/poisson_regression_in_python">frequentist version</a></li>
<li><a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html">Campus crime</a> and estimating household size in Philippines from BeyondMLR.</li>
<li><a href="https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-poisson-regression.html">Alcohol and meds interaction</a>, with simulated data.</li>
</ul>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Overdispersion. Negative Binomial. Zero-Inflation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The negative binomial distribution is often found in customer behavior, in contractual or non-contractual settings, when it comes to purchases, subscription renewals.</p>
<ul>
<li><a href="https://avehtari.github.io/ROS-Examples/Roaches/roaches.html">Cockroaches and pest management</a>, where Negative-Binomial, Poisson and Zero-Inflated NBD is investigated.</li>
<li><a href="https://num.pyro.ai/en/stable/examples/zero_inflated_poisson.html">Fishing catches in a park</a>, in numpyro</li>
<li><a href="https://bambinos.github.io/bambi/notebooks/negative_binomial.html">Students’ absence</a>, UCLA data, application of negative binomial, written in bambi</li>
</ul>
</div>
</div>
<p>In the <a href="./01_fundamentals/stat_foundations.html">probability fundamentals</a> section, we discussed the use-case of estimating proportions and named the field of compositional data analysis. It is often found in practice, but disguises itself, which causes the wrong method to be applied.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">If you encountered such a problem, when you care about proportions, mix of stuff, or compositions, not their absolute values or quantities, you can check out <a href="http://www.sediment.uni-goettingen.de/staff/tolosana/extra/CoDa.pdf">these lecture notes</a></span></div></div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Proportions. Compositional Data Analysis
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://joshuacook.netlify.app/post/dirichlet-regression-pymc/">Dirichlet regression</a>, pymc3 - fake proportions dataset, but take some real ones from compositional data analysis books</p>
</div>
</div>
</section>
<section id="hierarchical-glms" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="hierarchical-glms">Hierarchical GLMs</h2>
<p>We finally reached to the most exciting point in our journey so far, where we can properly model and explain sources of variation at multiple levels (of analysis). This is where we relax the assumption of iid, replacing it with one of exchangeability.</p>
<p>The point is that correlated data causes problems in modeling, when we don’t account properly for it: be it groups, clusters, categorical variables, time series, geography, etc. BeyondMLR has a very well thought, <a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-corrdata.html">practical motivation</a> for multilevel models.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Hierarchical Gaussian Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>We come back to the radon example, by adding one covariate at each level: house and county. This explains a part of variation which was missed before and leverages pooling to take care of low sample size in some counties.</p>
<ul>
<li>Radon: Primary code reference: <a href="https://www.pymc.io/projects/examples/en/latest/case_studies/multilevel_modeling.html">pymc</a> - A Primer on Bayesian Methods for Multilevel Modeling
<ul>
<li><a href="https://bambinos.github.io/bambi/notebooks/radon_example.html">bambinos</a> a higher level API, models the log-radon. Alternatively, <a href="https://mc-stan.org/users/documentation/case-studies/radon_cmdstanpy_plotnine.html">McStanPy</a> implementation</li>
<li><a href="https://github.com/omarfsosa/tech-talk-hierarchical-models">Omar Sosa - Practical introduction to Bayesian hierarchical modeling</a> with numpyro</li>
</ul></li>
<li><a href="https://num.pyro.ai/en/stable/tutorials/bayesian_hierarchical_linear_regression.html">Bayesian Multilevel Regression</a> numpyro on <a href="https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression">OSIC Pulmonary Fibrosis Progression</a> data, which assesses the risks of smoking and not only.</li>
<li>BeyondMLR presents a really interesting psychological study about <a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-multilevelintro.html#cs:music">stage anxiety</a> for music performers at a conservatory.</li>
<li>A repeated analysis of Stack’s facial feedback hypothesis, in the context of replication crisis, via <a href="https://bambinos.github.io/bambi/notebooks/Strack_RRR_re_analysis.html">bambi</a> – full workflow.</li>
</ul>
</div>
</div>
<p>Longitudinal data is a special case of multilevel models, but has the distinct feature, that at the group level, the data isn’t iid, but comes as a realization of the stochastic process.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">Often called Panel Data in economics and social sciences. You will see a different terminology in that literature: of mixed models.</span></div></div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Longitudinal Data and Studies
</div>
</div>
<div class="callout-body-container callout-body">
<p>These examples model the trend with a covariate, as a function of time – which is a happy case when we can do that. However, in practice, things become more complicated if we have to model and identify a stochastic process like <span class="math inline">\(AR(k)\)</span>, or <span class="math inline">\(MA(n)\)</span>.</p>
<ul>
<li>Longitudinal data with drinking teens, alcohol consumption per cohorts, <a href="https://www.pymc.io/projects/examples/en/latest/case_studies/longitudinal_models.html">pymc</a>.</li>
<li><a href="https://bambinos.github.io/bambi/notebooks/sleepstudy.html">Sleep study and reaction times</a> in time by subject. The same modeling idea is in <a href="https://bambinos.github.io/bambi/notebooks/multi-level_regression.html">pig growth study</a>, both implemented in bambi.</li>
<li>Beyond MLR <a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-lon.html#cs:charter">charter schools longitudinal</a> study.</li>
</ul>
</div>
</div>
<p>If in the previous sections, I didn’t have a strong preference for the Bayesian Approach, in the case of multilevel models, I strongly believe Bayesian Inference to be superior and less prone to overfitting and numerical instability.</p>
<p>Another aspect of this, is that most mixed models packages will make certain decisions for us, without our consent, which could influence our results and conclusions. As you’ll see in these tutorials, we are forced to construct the model and declare, justify every choice of prior and model structure.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Hierarchical Logistic Regression
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Beyond MLR, college basketball referee foul differential <a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-GLMM.html#cs:refs">here</a></li>
<li><a href="https://num.pyro.ai/en/stable/examples/ucbadmit.html">Graduate Admissions</a>, from McElreath, &nbsp;UC Berkeley in Fall 1973 (numpyro)</li>
<li>Rasch item-response model for Nba fouls in crunch time, <a href="https://www.pymc.io/projects/examples/en/latest/case_studies/item_response_nba.html">pymc</a></li>
</ul>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Hierarchical Poisson Regression
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.bayesrulesbook.com/chapter-18.html#hierarchical-poisson-negative-binomial-regression">Airbnb number of reviews</a></li>
<li>Estimating the strength of a <a href="https://www.pymc.io/projects/examples/en/latest/case_studies/rugby_analytics.html">rugby team</a></li>
<li><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/sta4.544">Paper investigating seat-belt use rates</a>, with data probably taken from the department of transportation <a href="https://crashstats.nhtsa.dot.gov/#!/">crashes website</a></li>
</ul>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Models with 3 Levels
</div>
</div>
<div class="callout-body-container callout-body">
<p>At last, it is useful to see an example where we have an even more complicated multilevel structure, as it will be the case in some practical applications.</p>
<ul>
<li>Beyond MLR <a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-3level.html#cs:seeds">3-level seed germination</a></li>
</ul>
</div>
</div>
<p>This is really, the end of the Module 2a. Applied Bayesian Statistics. These tools are so general and powerful, that they will last you a lifetime. However, there is so much more to learn and so many more challenges to tackle – that there is no end to the mastery of Multilevel GLMs.</p>
<p>Even after you learned all the technical intricacies, we’re hit again with the reality that association doesn’t imply causation. So, after being competent with this amazing tool – we have to go back to ideas from causal inference, if we want to get a more than associative insight about our theory and understanding of phenomena.</p>
</section>
<section id="bayesian-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-machine-learning">Bayesian Machine Learning</h2>
<p>As a bouns, I’m adding two extremely flexible, general-purpose models, which you can treat as in the Machine Learning practices, but are Bayesian to the core. These are extremely useful if we care more about predictive accuracy, either for decision-making, or as a part of causal inference process, where a certain complicated relationship details aren’t important to us, just the conditional average treatment effects.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bayesian Additive Regression Trees
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://bayesiancomputationbook.com/markdown/chp_07.html">BART</a> from osvaldo, on bike shares</li>
<li><a href="https://learnbayesstats.com/episode/80-bayesian-additive-regression-trees-sameer-deshpande/">Podcast Episode</a> and an <a href="https://github.com/skdeshpande91/flexBART%20in%20R">R package</a></li>
<li>Nonparametric methods - what is the benefit, name a few equivalents. Be able to explain a signed-rank transformation. <a href="https://steverxd.github.io/Stat_tests/">This</a> is the simplest and the most accessible explanation I know of so far.</li>
</ul>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Gaussian Processes
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Gelman: <a href="https://omarfsosa.github.io/speedy_gaussian_processes">Birthdays</a>, hilbert space approximation <a href="https://github.com/omarfsosa/hsgp">repo</a>, also in <a href="https://avehtari.github.io/casestudies/Birthdays/birthdays.html">stan</a></li>
</ul>
</div>
</div>
</section>
<section id="bibliography" class="level2">
<h2 class="anchored" data-anchor-id="bibliography">Bibliography</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode yml code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> </span><span class="fu">title </span><span class="kw">:</span><span class="at"> Bayesian Data Analysis</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">title_short</span><span class="kw">:</span><span class="at"> bda3</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> book</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">edition</span><span class="kw">:</span><span class="at"> </span><span class="dv">3</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">author</span><span class="kw">:</span><span class="at"> Andrew Gelman</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">year</span><span class="kw">:</span><span class="at"> </span><span class="dv">2013</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">link</span><span class="kw">:</span><span class="at"> http://www.stat.columbia.edu/~gelman/book/</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">lectures</span><span class="kw">:</span><span class="at"> https://avehtari.github.io/BDA_course_Aalto/Aalto2022.html</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> </span><span class="fu">title</span><span class="kw">:</span><span class="at"> Bayesian Methods for Hackers</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">title_short</span><span class="kw">:</span><span class="at"> bmh</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> book</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">edition</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">author</span><span class="kw">:</span><span class="at"> Cameron Davidson</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">year</span><span class="kw">:</span><span class="at"> </span><span class="dv">2015</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">link</span><span class="kw">:</span><span class="at"> https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">github</span><span class="kw">:</span><span class="at"> https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> </span><span class="fu">title</span><span class="kw">:</span><span class="at"> Statistical Rethinking</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">title_short</span><span class="kw">:</span><span class="at"> rethinking</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> book</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">edition</span><span class="kw">:</span><span class="at"> </span><span class="dv">2</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">author</span><span class="kw">:</span><span class="at"> Richard McElreath</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">year</span><span class="kw">:</span><span class="at"> </span><span class="dv">2021</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">link</span><span class="kw">:</span><span class="at"> https://xcelab.net/rm/statistical-rethinking/</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">lectures</span><span class="kw">:</span><span class="at"> https://github.com/rmcelreath/stat_rethinking_2023</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> </span><span class="fu">title</span><span class="kw">:</span><span class="at"> The Most Dangerous Equation</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">title_short</span><span class="kw">:</span><span class="at"> danger-eqn</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> article</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">author</span><span class="kw">:</span><span class="at"> Howard Wainer</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">year</span><span class="kw">:</span><span class="at"> </span><span class="dv">2009</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">link</span><span class="kw">:</span><span class="at"> http://assets.press.princeton.edu/chapters/s8863.pdf</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> </span><span class="fu">title </span><span class="kw">:</span><span class="at"> Introduction to Probability</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">title_short</span><span class="kw">:</span><span class="at"> probability-blitzstein</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> book</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">edition</span><span class="kw">:</span><span class="at"> </span><span class="dv">2</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">author</span><span class="kw">:</span><span class="at"> Joe Blitzstein</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">year</span><span class="kw">:</span><span class="at"> </span><span class="dv">2019</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">link</span><span class="kw">:</span><span class="at"> https://projects.iq.harvard.edu/stat110/home</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">lectures</span><span class="kw">:</span><span class="at"> https://projects.iq.harvard.edu/stat110/youtube</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./01_fundamentals/learning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">5. Learning from Data: Intuition and Bias</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02_causality/roadmap.html" class="pagination-link">
        <span class="nav-page-text">1. Syllabus &amp; Roadmap</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">The course I wish I had when getting started. Built with ❤️ by Mihai Bizovi</div>   
  </div>
</footer>



</body></html>