---
title: "Scientific process is all you need"
format:
  html:
    toc-location: left
lang: en
---

<!-- 
::: {.column-margin} 
The way many curricula are designed, produces three profiles of professionals in the industry: 

- the applied mathematician, from programs with a heavy focus on maths
- the data-saavy computer scientist, from programs with a focus on ML/AI 
- the business-saavy analyst, from programs with a focus on economics
:::



## A proliferation of processes for data science

People doing data mining (analytics), ML, and applied statistics, all recognize the importance of following a process. They design processes and workflows for their domain of expertise, which is indubitably helpful. However, it is confusing for a beginner to see so many of them -- as I show on the margin.

::: {.column-margin}
- Data Mining, ML, and Analytics:
    - CRISP-DM (cross-industry process for data mining)
    - Tuckey's exploratory data Analysis
    - Hadley's Wickham's tidyverse process
    - C. Kozyrkov's 12 steps of ML
    - Andrew Ng's error analysis
- Experiment design and A/B testing, besides steps given in a stat101 class
    - C. Kozyrkov's 12 steps of statistics
    - A. Gelman's workflow, described in "Regression and other stories"
- Statistical modeling and bayesian approaches:
    - A. Fleischhacker's business analyst's workflow
    - R. Alexander's process for telling stories with data
    - R. McElreath's "drawing an owl"
- Scientific process in its standard formulation
:::

For a very long time, I chose a process depending on the type of question and problem at hand. I still do, but I'm increasingly unease about it, as every well-intentioned author seems to make their own slight variations. You know the joke: "I decided to unify 10 existing standards -- now we have 11".

::: {.callout-tip}
## Scientific process in the broad sense
A natural question to ask is: "What is the mechanism which could generate all these processes?" You've probably seen the scientific method roughly described as: Observations - Hypotheses - Experiment - Data collection - Hypothesis Testing - Conclusions.

I don't think this is sufficient for us, nor that we can force other methodologies onto this template. We need a few adaptations to our field and a different structural-functional organization. The [diagram](https://undsci.berkeley.edu/understanding-science-101/how-science-works/the-real-process-of-science/) from Berkeley's "Understanding Science 101" is a big step in the right direction.
:::

## Scientific process, reinterpreted

Let's take the example of a firm and think of it as a complex adaptive system or much simpler, as an agent. Hopefully, the firm has formulated its mission, vision, has articulated a rough strategy and has set reasonable objectives and goals. People will try to make decisions and act in such a way to increase performance, generate good outcomes which are aligned with its strategy. Value added, IN - (Process) - OUT (products, services)

The firm gathers data about its current and past state, interacts with the environment, and its actions influence the environment. Also, the firms observes this environment. It also interacts with other agents in the network (be it customers, suppliers, manufacturers, government, etc). Let's call the first element an "Agent-Arena" relationship. Depending on the point of view, we can talk about different Agent-Arena relationsihps, for example, from a team or department or function in value chain perspective. We don't have to model the whole thing here -- just what is of interest to us.

At the core of scientific process is testing ideas. We will extend the notion of the experiment to include building prototypes, products, features, systems, including ML / predictive ones. Of course, we keep here the data collection (measurement, sampling), and building statistical models. But the essence stays the same: we test our expectations against the actual outcomes. This generalization means that our ML model prototype can be used inside the firm to make decisions. Also, the data comes from the agent-arena space (that is, our firm, the real world).

Another key part of the scientific process is discovering, explaining, developing theories. Any method will do, from observation to data mining, pattern recognition or just sheer experience in the domain / field. More rigorously, it's scientific theories, and causal models associated with them. Here we ask questions, formulate hypotheses, justify constructs. Much of the inspiration comes from the real life, problems, challenges and opportunities of the firm. These ideas, hypotheses are going to be tested and refined based on the outcomes of our experiments. Note that exploratory data analysis can also be model-driven.

The last part is the collaboration and community: our colleagues, decision-makers, domain experts, stakeholders. They inform the firm's strategy, policies, interventions. The ideas which are tested have to be communicated. We also get feedback from them, guardrails, support, etc. Their experience is also a body of knowledge, an inspiration for the next idea and hypothesis.

This graph can generate all the methodologies:
- e.g. McElreath's causal inference - between estimands / construct, simulations, process models.
- e.g. The scientific process formulated as you know it
- crisp-dm
- cassie's 12 steps to ML -->
