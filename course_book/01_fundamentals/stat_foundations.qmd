---
format:
  html:
    toc-location: body
---

# Statistical Foundations: WIP

In order to be a successful Data Scientist, one has to speak the language of probability and statistics. It is the foundation on which we can build towards more realistic and avanced models, with the purpose of improving **decision-making under uncertainty**. This foundation is a prerequisite for all three perspectives: analytics/mining, machine learning and causal inference.

> In order to build adequate models of economic and other complex phenomena, we have to take into account their inherent *stochastic nature*. 

Data is just the appearance, an external manifestation of some `latent processes` (seen as random mechanisms). Even though we won't know the exact outcome for sure, we can model general regularities and relationships as a result of the large scale of phenomena. 


::: {.callout-tip}
## A word of encouragement

Reviewing the fundamentals of statistics doesn't have to be boring! We can put "the classics" in context of modern statistics, big data challenges, and use simulation instead of heavy mathematics and proof-based approaches.

Moreover, theoretical ideas underlying statistical practice, which we often take for granted, deserve an explicit articulation. This will improve our awareness, understanding, and grasp of the field -- such that we can become more effective practitioners.
:::

```{mermaid}
%%| label: fig-mermaid
%%| fig-width: 9
%%| fig-cap: |
%%|   In this chapter and set of lectures, I attempt to reformulate fundamental
%%|   statistical ideas, concepts, and tools, in order to show their relevance in 
%%|   the day-to-day practice and decision-making. A secondary objective is to 
%%|   fill in the conceptual gaps left by the fact that it's hard to make sense 
%%|   of it all the first time we're encountering it in the classroom.

flowchart TD
	Motiv[Why again?] --> ProbT[Probability Triple] --> SI[Uncertainty]
	SI --> MS[Mathematical Statistics] --> GP[Prague Golemn]

	Motiv --> Estim[Estimators] --> Prop[Desired Properties] --> US[Example: US Schools] --> Cond[Conditioning] --> Marg[Marginalisation] --> Ex[Exchangeability]

	Motiv --> DM[Data Mining Process] --> G[Prediction & Geocentrism] --> Sci[Scientific Process]

	Ex --> Dead[Dead Salmon]
	GP --> Dead
	Sci --> Dead

	Dead --> Mod[Modern Stats]
```

The plan for this lecture is the following: we start by constructing the probability triple, formalize sources of uncertainty, and investigate the building blocks of a statistical model. I then highlight in a few case-studies what can go wrong in the process of statistical inference and how difficult it is to choose the right model. The most technical, heavy, and mathematical part is about estimators and their properties, but it is necessary both for hypothesis testing and machine learning. To make it more accessigle and intuitive, we will use simulation and visualization during the labs to get an understanding and intuition on how estimators behave. We wrap up the lecture by looking at the statistical process in firms and put it in contrast with the process for ML, predictive systems. Last, but not least, there is one more cautionary tale about multiple testing -- which will be our gateway into truly modern statistics.



::: {.callout-tip}
## Complementary resources if you feel like starting over
- Realize that lots of common statistical tests are particular versions of linear models [^11]. It takes a few hours to go through the theory and the code in the referenced book.
- Here's the approach taken by [^24], which integrates data analysis and simulation in learning of statistics.
- Upgrade your statistical thinking for the 21st century challenges and be aware of the pitfalls and problems in the field. A good reference is [^12], which takes multiple perspectives (frequentist, bayesian, causal inference), while going through the workhorse models and methods.
- Be comfortable with exploring, wrangling, visualizing and analyzing data in R or/and Python, get familiar with reproducible research best practices. A good starting point is [^45], which is a course in collaboration with the RStudio team.

There are a gazillion books, courses on statistics, which basically do/teach the same thing. For reference and your curiosity, I curated a few which stand out with the right balance of data, code, simulation, theoretical rigor and real-world applications:

- Cetinkaya-Runde, Hardin - [Introduction to Modern Statistics](https://openintro-ims.netlify.app/index.html) goes through all the fundamentals in a clear, concrete, extensive way, with code!
- Crump - [Answering questions with data](https://www.crumplab.com/statistics/) -- introductory statistics for Psychology Students has an interesting approach, focused on the challenges of Psychology
- Thulin - [Modern Statistics with R](http://www.modernstatisticswithr.com/) goes through the whole process, with R, with additional topics, normally not present in a statistics course 
- Holmes, Huber - [Modern Statistics for Modern Biology](https://www.huber.embl.de/msmb/index.html) is for biologists, but we can see how central to the field are multidimensional methods, clustering and high-performance computing, working with big and messy data
:::


## The probability triple

In the [previous lecture](background.qmd#why-did-you-study-all-of-that), I mentioned why did we study probability theory. However, there is one more useful metaphor: remember how important is logic in mathematics, computer science, and philosophy; it's one of the prerequisites in each of those, an essential tool for reasoning. Then, probability theory is the **logic of uncertainty**, a formal language. [^fuzzy]

[^fuzzy]: If you studied fuzzy set theory, you might have a case for it being the candidate, however, it fell out of favor in practice -- so I would suggest to focus on probability and Bayesian reasoning. 


Often, probability and mathematical statistics are bundled together, as they make perfect sense in sequence, but they have different objectives in mind. Probability theory is concerned with the construction of the probability triple $(\Omega, \mathcal{F}, \mathbb{P})$, defining useful tools like random variables and probability/cumulative density functions, extending those to joint, conditional probabilities. Then, introducing the machinery to operate all of that, like expectation, variance, moment-generating functions -- in order to reason about distributional properties.


1. A **random experiment** ($\mathscr{E}$) is a set of *conditions which are favorable for an event* in a given form with the following properties:
    - Possible results and outcomes are known apriori and exhaustably, for example a coin/die toss, quantity sold
    - It's never known which of the results of $\mathscr{E}$ will manifest or appear once we run the experiment
    - Despite that, **there is a perceptible regularity**, which can be eventually measured and quantified, that is, encoding the idea of a probabilistic "law" in the results. That regularity could be a result of the large scale of the phenomena.
    - Repeatability of the conditions in which the experiment runs, like the comparability and perservation of context. This is optional in the Bayesian perspective, where we're not thinking in long-run frequency terms. 
2. **Elementary event** as an auxiliaty construction: one of the possible results of $\mathscr{E}$, usually conventionally denoted by $\omega_i \in \Omega$
3. **Universal set**  $\Omega = \{ \omega_1, \omega_2, \dots \}$  is also called (Outcome/ State/ Selection space). It suggests the idea of complementarity and stochasticity: we don't know which $\omega_i$ will manifest, thus is a key object for a further formalization of probability measures. 
4. We care not only about **an event** $A = \bigcup\limits_{i = 1}^n \omega_i$ and its realization, but also about other events in the Universal Set, because they might add information about the probability of occurring of our event of interest.
5. The **event space** $\mathcal{F}$ should be defined on sets of subsets of $\Omega$ and this is where measure theory shines. For technical reasons, we usually can't define a probability measure on all sets of subsets. [^sigma-algebra]
6. **Probability as an extension of the measure**: chance of events realizing. Note that the perceptible regularity can be thought as the ability to assign a probability (number between 0 and 1) to elementary events: $\mathbb{P}(\omega_i)$. This is why additivity properties are key, as we care about random events, not only elementary events. This is where the debate between frequentists and Bayesians kicks in.
7. A **probability triple** $(\Omega, \mathcal{F}, \mathbb{P})$ is the fundamental object the whole probability theory is constructed upon. We have to thank Kolmogorov for taking the informal, gambling-type probability and putting it onto solid, axiomatic foundations -- that is, making it open to mathematical enquiry.


[^sigma-algebra]: The reasons for this are very technical, and the concept of a sigma-algebra is essential in resolving the resulting paradoxes. If you're interested in these technical details, you can check out my relatively accessible introudction to [measure theory and the Caratheodori extension theorem](https://blog.economic-cybernetics.com/post/2018-06-01-probabiliy/).  Even though you can go a long way as a practitioner with standard tools in probability theory, deeply understanding its **measure-theoretic** foundations could open up a whole new world to the researcher. It's easy to take the results from statistics and probability for granted, but it's useful to be aware what hides beneath the surface.

Thank you for bearing with me through the theory you have probably seen before, but we're not done. We're still in the land of set theory, and it is very hard to operate that way in practice -- so, we need a new concept which will allow us to use the tools of mathematical analysis in probability, in order to make it feasible for practical uses.

::: {.callout-important}
## The breakthrough idea of a Random Variable

We started from some phenomena of interest and a random experiment. The random variable is a necessary abstraction in order to mathematically define quantifiable characteristics of the objects. Meaning, we start working with numbers instead of some qualitative properties. Now, we're in business!
:::

::: {.callout-important}
## Random Variable is not a variable, nor random

A random variable is quantificator of elementary events, a function defined on the outcome space which maps the elementary events to the real number line. That mapping can't be done in any way we wish, it has to perserve the informational structure of the sample space. That is one of the technical reasons for sigma-algebras we mentioned before and is related to the idea of **measurability**.


\begin{align}
X(\omega):\Omega \rightarrow \mathbb{R} \\
s.t. ~~ \{\omega \in \Omega | X(\omega) \leq r, \forall r \in \mathbb{R} \} \in \mathcal{F}
\end{align}

:::

Let's figure out what the hell do we mean by that fine print condition, using the diagram below. The idea of conservation of the informational structure is actually equivalent to the one of measurablility. If this property doesn't hold, it's not possible to explicitly and uniquely refer to the sets (events) of interest.

The idea is that the preimage defined above $X^{-1}((-\infty,r]) = E \in \mathcal{F}$ on the following interval corresponds to an event E which should be in the event space $\mathcal{F}$. Because the only thing that varies is the limit of the interval r, the randomness comes from it. Also, it automatically suggests the idea of the Cumulative Distribution Function, which is $F_X(X \le r)$.



![Idea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable. It is the construct that enables us to define the statistical population (some relevant aspect of it to us)! ](img/random_variable.png "Random Variable"){width="90%"}


As a motivation of why do we have to understand all of this, when for most practical applications we can get by just fine with using the results and tools from probability, I will introduce two examples: one of componsitional data analysis[^cda] and time series analysis. What I want to say, is that for more "exotic" applications, we might need to tweak that probability triple because of the nature of the problem, which has downstream consequences for all the statistical machinery we use in those applications.

::: {.callout-tip}
## The curious case of Compositional Data Analysis

Sometimes, the data doesn't "live" in our normal, intuitive, euclidian space 
$\mathbb{R}^n$. There are cases when the object of our analysis are proportions or compositions: think of what a material is made of, the proportion of the demand for different sizes of a shoe or garment.

We don't necessarily care about their absolute value, but about their relative proportion. If we blindly apply traditional methods, or even statistical summaries, we will quickly hit weird results and paradoxes. So, we have to tweak existing methods we have make sense for compositions.

Compositional data analysis solves those issues by defining a probability triple over the simplex (instead of $\mathbb{R}^n$): $(\mathcal{S}^n, \mathcal{F}, \mathbb{P})$. This leads to a different definition of the event space $\mathcal{F}$, which is also a sigma-algebra and a different definition of the probability measure $\mathbb{P}$. 
:::

[^cda]:  ![*Source: [Dumuid](https://www.mdpi.com/1660-4601/17/7/2220)*: Data in a Simplex, which is later translated to $R^n$ by a carefully constructed basis expansion](img/composition.png "Simplex"){width="90%"}

<!-- Three additional critical concepts are: joint probability, which is basically the storytelling behind the data generating process,  conditioning (basically reducing uncertainty and adding information) and marginalization (esp of nuisance parameters -- explain what are those).  -->

Remember our exaple of [pigeon superstition](background.qmd#implicit-learning-intuition-and-bias) in the context of learning? It is not surprising to me that measure theory becomes important in Learning Theory, which is exatly those carefully formulated principles that will prevent our automated learners to become supersitious. [^ml-measure]

[^ml-measure]: Even though most courses from which I studied don't mention it explicitly (Yaser Abu-Mostafa, Shai Ben-David, Reza Shadmehr), according to Mikio's Brown [answer](https://www.quora.com/Is-Measure-Theory-relevant-to-Machine-Learning/answer/Mikio-L-Braun?srid=KONR) it's essential in the idea of **uniform convergence** and its bounds, where *"you consider the probability of a supremum over an infinite set of functions, but out of the box measure theory only allows for constructions with countably infinite index sets"*. 

For the next example, you don't have to understand what Gaussian Processes are or are used for. However, later in the course, we will discuss nonparametric methods for hypothesis testing. Their usefulness comes from the fact that we make less distributional assumptions about our population, therefore getting more robust results, in contrast with choosing a wrong model or distribution. 

It's not that these methods don't have parameters, but the parametrization varies depending on how much data we have, which makes them very flexible in a wide variety of applications, where we just don't know what is a reasonable distribution or parametric functional form for the relationship that we model.

::: {.callout-tip}
## Stochastic Processes, Graphs and Trees

If we're thinking about a regression from the nonparametric perspective: that is, over a set of abstract functions: $f(x) \in \mathscr{C}^2:X \rightarrow \mathbb{R}$, [^gp] we might want to know how a draw of samples from an infinite set of continuous differentiable functions might look like. 

$$
f(x) \sim GP(\mu(x); K(x,x'))
$$

The questions arises: how to define a PDF (probability density function) in this space? In my bachelor thesis, I got away with using Gaussian Processes, which are a very special class of stochastic processes. In this special case I could informally define an apriori distribution by defining the mean vector and Kernel (covariance function), then condition it on observed data with a Normal Likelihood. 

$$
p(f(x) \, |\left \{ x\right \})=\frac{p(\left \{ x\right \}| \, f) \, \mathbf{p(f)}}{p(\left \{ x\right \})}
$$

 
:::

[^gp]: ![*Source: [Bizovi](https://blog.economic-cybernetics.com/post/2018-06-01-probabiliy/)*: A posterior distribution of the Gaussian Processes, when conditioned on data](img/gp-conditioned.png "GP Posterior"){width="90%"}



## Sources of Uncertainty

Our focus is on agents that interact intelligently to achieve their objectives over
time. Given the past sequence of observations o1, . . . , ot and knowledge about the
environment, the agent must choose an action at that best achieves its objectives
in the presence of various sources of uncertainty,1
including: 1 We focus on discrete time problems. Continuous time problems
are studied in the field of control theory. D. E. Kirk, Optimal Control Theory: An Introduction. Prentice-Hall,
1970.

Our focus is on agents that interact intelligently to achieve their objectives over
time. Given the past sequence of observations o1, . . . , ot and knowledge about the
environment, the agent must choose an action at that best achieves its objectives
in the presence of various sources of uncertainty,1
including: 


1. outcome uncertainty, where the effects of our actions are uncertain,
2. model uncertainty, where our model of the problem is uncertain,
3. state uncertainty, where the true state of the environment is uncertain, and
4. interaction uncertainty, where the behavior of the other agents interacting in the
environment is uncertain.

```{mermaid}
flowchart LR
	Env(Environment) -- observation --> A(Agent)
	A -- action --> Env
```


## Mathematical Statistics in a nutshell

importance of the data generating process, see what Ernesto said, then the causal process -- all is parameters, known or unknown 

Uses everything probability has to give, but is concerned with inference, estimation, hypothesis testing, prove theorems and probabilistic properties of estimators.

- Collectivity (can be anything, trees in central park, pixels, people, etc, etc) -- i.e stuff in the real world
- Statistical Population (binding contract)
- Sample (a data collection process is involved here)
- Parameter (average tree height, some aspect or characteristic of interest, unknown number/constant (bayes, a distribution), which is at population level) -- of the DGP, to be estimated from our sample.
- Estimator: t_theta(X) -> theta
	- Confidence intervals: depend on the distribution of the estimator, since X is rv, t(X) is a random variable too -- and it is the job of statisticians to tell us what is that P(t(X)) and their properties: if biased, consistent, efficient
- Estimation/a Statistic: theta_hat (a way to summarise and synthetise data)

Draw a diagram between sample and population. 

The point of statistics: change our opinion about the action we have (or phenomenon we want to understand) to take in the face of evidence.

One big problem is if we got the model right for our use-case and phenomena. A wrong model choice leads to wrong conclusions. It should be informed as much by stats as by the theory and domain. Think of this in the spirit of the scientific method.


## Models are Golemns of Prague

Data + Domain Assumptions + Statistical Assumptions

## Model Specification: Fisher vs Bayes


## What does a statistician want?
From their estimators

## Case Study: De Moivre on US Schooling

Let's remember what we discusssed last time. So, the US gov, tried to find out what makes some schools better than others. They saw that in top schools, small ones were dominating (in terms of nr students, perf. being SAT ~ Norm()). 

They decided to split up big schools into smaller ones. Can you say if it was a good, bad idea and why did they think that?

Students hypothesis:
- more attention to students in smaller classrooms
- private schools, where families are more wealthy
- geographical location, i.e. small towns and villages
- motivation of the underdog, from small towns
- friendships, community and peer competition (easier in smaller schools)
- self-selection: what kind of students go into small schools -- we assume average IQ is the same, including the distribution. Assume random allocation. 
- Skewed (even if not, longer tails): meaning, asymmetric distributions. 
- Finally: variance and standard deviation

About the decision sanity: reality vs data. The conservative decision is to do nothing, as restructuring costs a lot, in money and social changes & consequences, it is risky -- that is, the default action, without seeing any data or evidence, we would do nothing. 

S: A good remark about having a diversity, specializations (professional schools) -- can't have a one-size-fits-it-all policy. But we ignore it, oversimplifying the problem.

All of that is true, but just for 10 minutes, we introduce a (false) dichotomy between small and large schools. The alternative is to split big school -- remember, that outside this box, or bad framing of the problem, there are many many potential interventions (professor education, smaller classrooms, fairer allocation of students, raising up the disadvantaged hood schools). 

The punchline: we have all these factors potentially contributing to the performance. Because of DeMoivre and LLN (mean/sqrt(sd)) -- in a simulation, we notice the following thing: (insert graphs).

> Small schools will dominate both the top and bottom, due to larger variance. This is what it means. 

So, we go back to this idea that statistical models, are golemns, they do exactly what they are told to, and this is dangerous. We have to understand deeply in what contexts and in which ways these little robots fail and give absurd and invalid results without us knowing.

We will go into more detail next week in A/B testing and hypothesis testing, but here we go: once we have the default action and the alternative action, the null hypothesis is (sidenote, we don't care about rote calculation -- R packages does it for us, but we care about experiment design and decisions, try to figure out the causal influences):  in which worlds will I take the default actions (worlds, meaning the values of the unknown parameter of interest, describing the behavior of the populations -- in this case quantifying the impact of school size on SAT scores). The alternative hypothesis, is exactly the opposite, all the other worlds (parameter values) when I will take the alternative action.

So, in this case we will collect data, used to estimate the parameter and its confidence intervels (inference, with the idea of generalizing from the sample to the population). Now, given those confidence intervals, we ask how surprising it is to see the data/estimation, if the null hypothesis was true.

Most stats courses jump over these important aspects. Their starting point is: H0, HA are given, model is given, you just compute. In practice, even for experienced statistician, for a new problem, it is very hard to define those 4 components -- as it depends on the domain knowledge and business objectives. Moreover, even the interpretation of p-values is extremely tricky to communicate. We will develop tools to deal with those challenges. 

## Case Study: Chess example
I played chess for 14 years, professionally, question about difference in genders and what kind of tournaments and policies should we have. Why are there no women rn in top 100 (Historically, it was judit polgar in the top 20) ?

> Use this chance to extract data and analyse. Same problem with marathon runners (from BS big data). 

Causes?
- Bias: inferring natural ability -- nonsense from IQ research
- Misoginism
- Education and lack of encouragement
- Lack of opportunities, community, support
- Historical trajectory -- lack of competition

A lot is explained by how many boys and girls start from a given cohort/age. Similar problem, but slightly different -- for any problem of rank top(n).

We will discuss next time a whole range of other pitfalls, including spurious correlation (nicolas cage and drowning), common cause (babies and storks), ommited variables, mediation, selection bias, reverse causality, even including predictors which shouldn't be there. We need a statistical reasoning for all of these potential pitfalls. 

> explore python cli app + scraping + arrow + duckdb, see the trajectories of players and so on (chess.com vs fide). For the trajectories, mention the idea of longitudinal data, survival models, growth models, mixed models -- because of the particularities of DGP and causal process 

> see the difference between ELO vs xbox true skill, how fast it converges to the true ability -- that is a model we can investigate. 

## Case Study: Amazon reviews and ratings
How do you choose 4.7 (300) vs 4.5 (2000) -- according to what bounds, agresti-coull, correction. What chance do you give those with a small sample size? What is the optimal strategy here?

- we can guide ourselfs with the quality and relevance of reviews
- is it representative? (also, outside the box)
- text summary

Q: What is the variance and distribution, it depends, also on our risk appetites (w.r.t.) volume or if is one time (e.g. buy shoes vs coffee shops -- repeated interactions).

Introduce the confusion matrix, which will be discussed at length in ML and hypothesis testing (types of error) -- the point is that sometimes the payoff for correct decison or a mistake is asymmetric. And that matters (fraud vs reco quality) -- so, that is context-dependent. 

> good applications for text mining, statistical tests

## Data Mining and CRISP-DM variants


## Statistical Process and Science


## BYOP: Bring your own problem
Discussion over the how/process

## ML is a geocentric model of the universe

The good news is that sometimes, you just need a reliable prediction, as you're not intervening in the system causing a certain phenomena -- and by retraining ML models you can adapt to minor changes introduced by our interventions. Those ML models, they clearly didn't figure out a scientific explanation of the causal process, e.g. for demand forecasting in uber, and that just the prediction to be used in a larger ecosystem and environment for decision-making. 

Again, that is appropriate in low-stakes, large scale decisons. You might not care so much about the latent, causal process; but of course, you care that it generalizes to the population of interest (that is, a binding contract, a boundary in which your predictions are valid -- if you go outside that range, can easily get absurd predictions -- this is why this kind of system needs checks-and-balances, boxes to constrain the artificial stupidity). It is extraordinarily unlikely that these models translate to novel situations and environments without explicit transfer learning and careful adaptation. 

## Dead salmon: a warning tale
Forward reference to power and multiple testing


## Conditioning, marginalisation and Exchangeability




<!-- Statistical Foundations: Reference, Revies --->
[^11]: Doogue - [Common statistical tests are linear models](https://steverxd.github.io/Stat_tests/)
[^12]: Poldrack - [Statistical Thinking for the 21st Century](https://statsthinking21.github.io/statsthinking21-core-site/) has two companion books, one with R and another in Python
[^24]: Speegle, Clair - [Probability, Statistics, and Data: A fresh approach using R](https://mathstat.slu.edu/~speegle/_book/preface.html)
[^45]: Bryan - [STAT 545](https://stat545.com/) It is also notable for its focus on teaching using modern R packages, Git and GitHub, its extensive sharing of teaching materials openly online, and its strong emphasis on practical data cleaning, exploration, and visualization skills, rather than algorithms and theory.
