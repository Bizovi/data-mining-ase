# Data Science in Business Context

Arguably, we live in a *volatile, uncertain, complex, and ambiguous* world that we have to understand in order to navigate it well. By "world", I mean the economy, society, environment, or any other complex system -- especially when human behavior is involved. It is reasonable to ask: wasn't it always the case, depending on how we define the terms, level of analysis, and point of view? 

I think it's a matter of scale and magnitude in VUCA dimensions, where an accelerating rate of change poses a great challenge to our capacity to adapt. Instead of an evolutionary or philosophical comment on why we need this field of data/decision science, I have two questions on my mind when it comes to living well or improving business outcomes: **what is true** and **how should I act**? [^general]

[^general]: This implies that we already framed the problem and figured out what we want to achieve, and that indeed we chose the right goals and objectives. Moreover, the questions of epistemiology and ethics are a never ending topic for discussion and enquiry.

::: {.callout-tip}
## Decision Science: (not only) truth and action

Let's start from a business setting of an e-commerce, where we want to increase sales, customer satisfaction, and reduce costs. Imagine three scenarios, which neatly fall into the SWOT framework:

- We keep the status quo, doing everyting as before. What is the most likely trajectory of profits? Can we come up with an educated guess? If the trajectory looks good, that is our **strenghts** and compentencies contributing to it, if not, our **weakness**.
- A feared trajectory, that is, if our business is hit by a shock in supply chain, inflation, by competition or customer demand. It's the **threats**.
- A desired or aspirational trajectory. Is it reasonable and realistically achievable? If yes, what strategy and tactics should we implement, how sould we act? This is our **opportunity**.

After this exercise, we defined more precisely where we stand, that is, quantify the current state of the firm. We framed the problem in terms of most relevant outcomes and we're in the process of figuring out what is the optimal goals to aim for. Obviously, we need a mechanism, measurements to know that we're on track and to recognize when to get there. Now, let's go back to our two questions and unpack them:

- **What is true?** In the most general sense, we're not asking for a mathematical and logical truth, but if it's plausible, probable, deserves serious consideration, is backed by evidence. I also mean that we understand the underlying causal mechanisms. Not least, an assessment of the current situation. The metaphor which I suggest for this is "seeing clearly", through the fog, illusions, and biases. [^scout]
- **How should I act?** What is plausible doesn't entirely answer this question, we can't derive an ought from is. In business settings, I would think about action in terms of strategic alignment and optimisation.

The only missing pieces from this mental model that I argue for is tremendously important: **iteration** and **feedback**. [^cybe] Due to VUCA, we can't be sure our actions are optimal, or even that we're solving the right problem, therefore fast iteration and feedback ensures we're not taking too much risk, that we find out early about problems in our thinking and action, that we can change course to steer the ship back on track.

:::


[^scout]: Recommended reading: Scout Mindset by Julia Galef
[^cybe]: Sounds an awflul lot like Cybernetics, doesn't it? Especially if we have the idea of a firm as a complex adaptive system as a pressuposition for this discussion.


<!-- You might've heard in the news that data scientist is the sexiest job of 21st century, that AI is going to take over, Deep Reinforcement Learning models are beating people at Dota and Chess, solving almost-impossible protein-folding problems. But what does it actually mean, if we step outside the hype/buzzwords, use a plain language, and apply these ideas in a more down-to-earth, day-to-day problems and challenges in businesses? -->

Now that it's more clear what I meant in the course introduction by improving business outcomes and bringing value to organizations, I didn't yet explain what does analytics, data science, machine learning, and AI do or are, and how to they fit in the picture we painted so far.

You might've noticed that every lecture starts with a brief motivation, and then will have this kind of flowchart of arguments and ideas. It is supposed to be your guide and a roadmap, so that you don't get lost in various detours taken and keep the big picture in mind. My recommendation is that you go back to this diagram at the end of the reading or lecture, try to remember individual arguments and think for yourself how are they related, what is the golden thread connecting them. [^thread]

[^thread]: This is a course in which we look at the widest possible range of methods and models, without going into depth, as I don't know which ones will be useful for your particular applications and problems. The idea is to find the optimal tool for the job when you encounter it and learn the details later, how to actually do it. We dive into more detail when presenting "workhorse-models", proven by practice to apply well in a large variety of use-cases. 


```{mermaid}
%%| label: fig-mermaid
%%| fig-width: 9
%%| fig-cap: |
%%|   A mind map and roadmap of the ideas and topics in this lecture. 
%%|   It is designed to give you a wide context in which AI is used and misused.
%%|   The classroom version is designed to be participatory and conversational, 
%%|   but I will try to do my best to communicate that spirit in written form

flowchart TD
  DM[Decision-Making] --> Domains[Domains] -- case studies --> U[Uber / Rides] --> Ecom[E-Commerce]
  DM --> AI[What is AI?] --> Choice[Stats/ML/Analytics]
  Choice --> Cy[Cybernetics Detour] --> F[Why did you study those?]
  DM --> BS[on Bullshit] --> Fool[Foolishness] --> Caus[Causal vs Correlational] --> BD[Big Data?]

  Ecom --> BYOP[Bring your own problem]
  F --> BYOP
  BD --> BYOP

  style U fill:#f7f5bc
  style Ecom fill:#f7f5bc
```

We start with real-world applications of data science, define what AI, Cybernetics, Big Data, Analytics, Machine Learning mean. Then, we figure out why did we study all those mathy and computer science subjects during the Bachelor's degree. Next, we discuss what can go wrong while drawing conclusions from data -- culminating in a discussion of a problem of choice you're passionate about.



## Data Science in the Wild

At this point, you might get tired of me emphasizing the decision-making aspect of data science as the main point of why it is important. It's time we move from the general and abstact towards particular examples and applications in various industries. This will lead us to acknowledge how **prevalent** is AI (that we haven't fully defined yet) in firms, services and technologies we use every day. [^reverse]

[^reverse]: When reading this section, I want to get you into a mindset of the reverse engineer: step back and think deeply about products and services you use every day, put yourself in the shoes of the business making those decisions and building those systems. What were they thinking about? What challenges were they facing that were an appropriate use-case for ML, Statistics, and AI?


::: {.callout-important}
## During a lecture, I usually ask students
Can you give some examples of businesses, sevices, technologies, problems, and domains which you suspect do have AI/ML algorithms and models behind the scenes?

See below some very good answers and argumentations provided by the students last year, then we'll examine in more details one of them. Of course, there is always that one person, very passionate about sports or blockchain.
:::


- **Dynamic pricing** in Bolt and Uber, which takes into account the weather, especially if it rains, peak hours: balancing demand and supply. It is at the intersection of ML and economic theory, as they are a platform or marketplace. Prices also change with respect to competitors, so we see aspects of an oligopoly behavior. [^uber_jack]  
- **Stock markets** and trading bots: at the intersection of economics, finance and AI. I would add the "good old" and boring portfolio management and venture capital enterprises.
- **Management consulting**: what market to enter, whether and how to build a new product (product development). Lots of use-cases in marketing and market research firms.
- **Medicine Applications**: developing new vaccines and drugs, aided by AI and designing clinical trials for novel treatments.
- **Banks and insurance**: risk management, predicting credit defaults on mortgages and business loans. Chatbots for customer support, for most frequent and simple questions.
- **Automotive**: routine tasks like automated parking, the race towards self-driving, autonomous or semi-autonomous cars, safety warnings. Predictive maintainance is tackling a problem where they leverage predictions to replace risky parts before they go out of function.
- Liverpool F.C. won a title, and a key part of their success was leveraging AI and ML to discover new tactis on the field with the highest payoffs. [^liverpool]
- **NBA** teams invested a lot in the data infrastructure and decision-making capabilities: LA Lakers found the best player at the moment for a particular position they were lacking and would play well along with the team. Rockets won the regular championship divison by going all in on the 3-point shot. Golden State Warriors simply revolutionised basketball with data, before everyone else was doing it -- giving them a competitive edge. [^nba]

[^uber_jack]: When it doesn't work out -- I'm pretty upset at their data scientists and domain experts. Here is where ethical issues creep up: jacking up prices, monopolies, drivers struggling to make a living wage.

[^liverpool]: TODO: Reference article and maybe dataset for more details
[^nba]: TODO: Reference to thinking basketball and other sports' science resources

In all of the examples above, those businesses and systems do have to make decisions, under uncertainty from multiple sources, trying to solve complex problems at a large scale, which would be impossible to do manually even with an army of employees. 

I would like to add a few more examples, from an insider and practitioner's perspective, which might not be as impressive and a bit routine, but no less important. Keep in mind, that if at a closer look, the service seems to do something relatively intelligent very fast, specialized AI might be involved behind the scenes.

- **Demand Planning**: How many SKUs (items) should I order for manufacturing, to satisfy the demand (that last item on the shelf, minimizing lost sales) and to minimize excess inventory.
- **Logistics** and **Supply Chain**: routing, distribution center operations and automations for order fulfillment, return management
- **Recommender systems** for music, videos, books, products, news in social media, services, platforms, and e-commerces like facebook, instagram, tiktok, youtube, spotify, amazon, emag. You can find recommendations in surprising places, like google maps.
- **Programmatic Advertisement**: finding best placement for ads on various platforms, right now dominated by meta and google

:::{.callout-tip}
## Check out the Case Studies for a Deeper Understanding

We will explore some challenges and applications from this list in a series of case studies and labs. The idea is to improve our ability to identify opportunities and formulate problems from the point of view of an organisation, such that we can match those with the methods, models and algorithms discussed in the course. 

I'll have to introduce a lot of new concepts when the language we developed so far will turn out to be insufficient to talk about and understand what's happening inside these firms. Thus, each case study is an opportunity to play around with a novel idea. [^skip-to-cases]

- [In the first deep-dive](bolt.qmd), we will look at Uber and Bolt, with publicly available information, trying to figure out what do their data scientists do. 
- Then, we will look at the [lingerie e-commerce](adoreme.qmd) where I work at, AdoreMe and a different set of problems we're facing.
:::

[^skip-to-cases]: If you have a pretty good idea about what is AI, Analytics, ML, Deep Learning, Big Data, Causal Inference and when to use one approach or another, feel free to skip the history and terminology and go straight to the case studies.




## What is AI, Data Science, and ML? 


It's undeniable that there is a lot of excitement when it comes to AI, ML, and data science -- to the point of calling it the sexiest job of 21st century. Data science is an umbrella term, with interdisciplinary at its core, drawing inspiration from multiple fields, sets of tools, practices, methods and it continuously evolves. [^questions]  It is designed to help us tackle increasingly more complicated problems at a large scale. There is also a reasonable worry about ways in which these systems can go wrong or awry.

You will often see a Venn diagram where data science sits at the intersection of mathematics -- statistics, computer science -- software engineering, and domain knowledge. I think this is not sufficient to characterise data science, therefore, will try to elaborate **what** it does, and **how** (which is as important). 

[^questions]: There are many questions still unanswered: How does this landscape of Data Science look like? What are the roles and jobs? What is the process for building smarter, data-driven software systems; drawing more reliable inferences and conclusions from data and theory? How does a day in data scientist's life look like?


::: {.callout-important}
## AI in a Nutshell
For all pragmatic intents and purposes, especially in businesses, AI is about **Decision-Making under Uncertainty at Scale** [^cassie-ai-def]. 

One important keyword here is `uncertainty`, as there is no point in building AI solutions based on complex models if we don't have uncertainty. We have to be able to change our mind and actions in the face of evidence.

On the other hand, `scale` is the reason ML and Deep Learning is so powerful, because you can take lots of small decisions in an automated way, with little curation or guidance from humans. This is why many traditionally "paperwork" industried like legal and accounting embrace digitalisation and automation now. 
:::

Ultimately, why would I build a system which predicts demand for products in a direct-to-consumer ecommerce like Allbirds, Macy's, or AdoreMe? Either in a marketplace like Emag or Amazon? Why would I try to find out the factors which contribute to a successful advertisement?


::: {.callout-tip}
## Here are some answers from students

- In order to **allocate resources** to the stuff which generates growth and profit. Avoid being scattered around (which I would call bad strategy), resulting in costs over targets and inefficiency.
- In short, we attempt to allocate resources and efforts **efficiently**.
- We can view **information as a competitive advantage**, anticipating and predicting so that we can plan and prepare, outperform competitors.
:::

When we talk about uncertainty, it's important to recognize its sources: [^uncertainty-sources] one coming from incomplete information, that we always work with samples in one way or another. For example, even if at a certain point in time we might have real-time data, everything evolves and quickly becomes outdated. Everything is in a state of flux and change. Even in the current state, we don't know for sure where we stand -- sometimes, in economics, this problem is called `nowcasting`. When talking about the future, making a good prediction is one of the most difficult things. 

For example, who would've predicted the pandemic and all its implications on the supply chain and society? It's important to note the difference between this kind of **black swan events** and the irreducible, fundamental uncertainty, which can't be captured by any explanatory factors.

In a happy case, we can quantify and reduce it by conditioning the model, that is, joint distribution of random variables with a given structure, on data. That would result in inferences and evidence with respect to our hypothesis and model of the world.[^generalization] At the very least we can try to quantify how uncertain are we. 

So, we still have to make decisions. Those have to have a level of **robustness** and resilience to shocks, in the face of uncertainty. I would go even further, to suggest that we should **aim for antifragility**, meaning, the system improves after a shock or negative event, but that is very hard to implement and operationalize, therefore, it falls outside the scope of this course -- to the realm of systems' design. 


[^uncertainty-sources]: We will talk more formally about sources of uncertainty in the next lecture, while reviewing the [fundamentals of statistics](stat_foundations.qmd#sources-of-uncertainty).

[^generalization]: We want to say something intelligent about the population, technically, to generalize. However, there is ambiguity, as objectives and the meaning/semantics of data fields are not always mathematically clear or without conflict. 

::: {.callout-warning}
## When you don't need AI and Statistics

As a though experiment, itmagine we have an equation or program, with well-defined rules, which perfectly predicts the price on stock markets, or perfectly predicts how many items will a client buy and how she will respond if we change the price (an intervention). We won't need machine learning, causal inference, or AI there.

Of course, we don't have that kind of program. It's only somewhat true in cases when we have a well-tested theory, which stood the test of time and went through the scientific process to become the best theory with respect to all others. For example newtonian physics, relativity, quantum mechanics, evolution. 
:::

However, when we talk about human behavior, we should resist the temptation and arrogance to say that we have a well-defined theory, be it normative or positive. Our preferences change, and we can "decide" in which direction they change or persist. 

Regardless of the business we work at or own, the place in the value chain, we'll have to deal with human behavior: customers, employees, decision-makers, engineers. We need other kind of tools to infer perceptible regularities and patterns in their behavior. We will be forced in one way or another to learn from data and observation.

::: {.callout-tip}

## A model is a simplified representation of reality

We need models to make sense of the world around us, because it is so complex and uncomprehensible if we are to represent it faithfully in a simulation. Therefore, we focus on relevant, interestig, essential aspects to us, we simplify by baking in domain knowledge, assumptions, and data into the models and algorithms. 

So, we can collect data, apply algorithms to train models, in order to make inferences about some relevant quantities. That will help us in making evidence-based decisions which gets us closer to our objective in an efficient way.  
::: 

::: {.callout-important}
## Weak AI is Domain-Specific 

By now, you probably figured out that we're not talking about General AI, trying to surpass human intelligence in general reasoning and problem-solving. Thus, we're talking about weak or specialized AI, which depends very much on the domain.

AI in an a fashion e-commerce, like AdoreMe, where we sell lingerie, will have a very different flavor from the tools and methods used in genomics, medicine, social science or psychology. 

Despite the fact that there are a lot of shared fundamentals, when it comes to the principles of building models, it is not straightforward to take something which works in one domain and apply it in another. Significant tweaks and adaptations are needed, which are dependent on the specificities of that domain.  

The good news is that when these transdisciplinary groups of people work together and successfully adapt a method, it is often a breakthrough in the field borrowing the theory and technology.
:::


### Cybernetics is what we call AI


::: {.callout-tip}
## The meaning of AI changed in the meanwhile

You can make sense of the terminology and general confusion of terms, by reading M. I. Jordan's brilliant article [^jordan-ai], which tell the history of "AI" and how this confusion arose. He also points out how many of the claims in the field, as of today are a stretch (i.e. the revolution hasn't happened yet) [^jordan-revolution].

Now, if we talk about AI (what we understand by it changed historically), to the definition, we would add the autonomy (weak ai), not self-awareness (general AI). 
:::


What's going on here? What we call now AI (weak AI), is an evolution of cybernetics and pattern recognition. There are two schools of thought: 

- immitative intelligence: try to do same things as humans, GAI research, at the intersection of cognitive science - it is fascinating, also needs philosophy, fascinating
- symbolic: that which died in 60s in the first AI winter
- weak AI: decision-making, at scale, more specialized.  Think in systems, feedback loops -- take a real phenomena and formalize, simplify it to emphasize essential aspects of that phenomena 

> The science of general regularities of information processing and control in animal, machine and human (socitey)

General regularities, meaning true across fields (econ, soc, physics, bio, chem). Control -- basically to reach the objectives, take an action which stirs the systems on a desired trajectory, and info processing is pattern recoginition, how can you model it, infer, integrate data and say something interesting about that phenomena. We're more on a society side, as we talk about human behavior, less about engineering, medicine, biology, ecology and climate science.

Again, it is a conglomeration of fields which went a bit out of fashion and favor: Game theory, Decision theory, general systems' theory, agent-based modeling, systems' dynamics, complexity and chaos, evolutionary algorithms. This stuff is fascinating and inspired many other breakthroughs, but it is extremely difficult to implement in practive -- so we kind of settled on a more general tool, like a hammer, which is pattern recognition (ml, dl, causality, optimization). 

So, if you want to be a data analyst, business analyst, ml engineer, data scientist, data engineer, statistician, product manager in tech, somehow, the cybernetics is a way of thinking (in systems), but we mostly use the tools outlined before. Also, no one can know it all well: so one of the objective of the course, is that you're passionate about ANY of this stuff or Not, to understand what those people do -- it is harder and harder which don't heavily leverage data for decision-making


## Analytics, ML or Statistics?

For practical, pragmatic intents and purposes, we can distinguish 3 ways of thinking, which have to work harmoniously together, in order for a data science project to be successful:

* Analytics and Data Mining - with the main goal of formulating better research questions (i.e. **inspiration**), find interesting and relevant stuff in massive datasets
* Machine Learning - learning from data, finding invariants/patterns, which generalize beyond the sample and training data, i.e. going from experience to expertise in an automated way
* Statistics, and especially causal inference - for making decisions with high stakes, and having greater confidence, rigor in the inferences and conclusions drawn


![Choose your own adventure](img/ds-adventure.png "Analysis"){width="90%"}

So,  xkcd cartoon explains it very well: the first question is - are you making decisionsthis? If no and are just curious, then we have lots of methods for data exploration and we look into analytics (inspiration!!). (ref: Cassie) If we do have to make decisions, and not many, and they are strategic and important, that means we need some rigorous statistics: if we do it at scale, e.g. us doing demand planning and inventory optimization for thousand and thousands of products, hundred of k of sizes - you cannot do it manually. On the other hand, it is not one or another, not a debate between ML 
and stats, you need both: recognize when is the appropriate time to use one or another -- so, choose your own adventure.


One has to cycle through these approaches, gain greater understanding, experience and skill in them, in order to use the appropriate tools in the right context. I recommend the following 4-part presentation [^cassie-mfml] by Cassie Kozyrkov, so that you get a good idea of how "AI" fits into organization and decision-making process. I recommend following her and, basically reading everything she has written on medium [^cassie-ref].

Pay close attention to the process of developing data-driven products [^cassie-steps] and what are the prerequisites for an AI project to be successful (or doomed from the very start). It is important not to skip the relevant steps, understand the roles of people involved: from decision-makers, to statisticians and data engineers. A good blueprint [^pair] for thinking about how to define and plan an AI project is given by Google's PAIR (People and AI research group).

Make no mistake, the data science field is fascinating and the applications exciting, but as you well know from statistics, there are numerous pitfalls we can fall into. I think it is useful to demistify AI, data science, and get humble, down to earth about what it can and can't do -- the power, but also the limitations:
- Just take a look at how many "AI" tools have been built to catch covid, and none helped [^mit-ai-covid]
- One part of the problem is the mismatch between the real/business problem and objectives, and what models optimize for. Vincent Warmerdam brilliantly explains it in "The profession of solving the wrong problem"[^war-wrong-problem] and "How to constrain artificial stupidity" [^war-stupidity].



![Analyst's workflow](img/analyst-workflow.png "Analysis"){width="90%"}

![ML workflow](img/ml-workflow.png "Analysis"){width="90%"}


e.g. pandemics -- decisions at the level of country - state - country - town: what is a good one? For that, I have to set objectives. Even right now, what is the real situation? We need to make an inference, a guess. It goes witout saying about the scale of it -- it's global, if we take into account people's mobility and movement!


So the first, question is: is there some kind of value proposition for AI and do you have data? If yes and yes, we're in business.  (ref: Yaser) The important thing is to clarify what do I mean by Pragmatic: something which can be done by a small team, without huge investment in resources, maybe it's not cutting edge, maybe not beat benchmarks and be gen-purpose, but it will make the life of the organization and the people working in it much better, and of course, driving outcomes/results.


The idea is to get an intuition when it is a good idea to use statistics, when a more powerful tool like ML is needed (but which is more dangerous in overfitting / backfiring) and what does Analytics means (you will or did have some subjects like BI, Data Analytics). What does analytics mean?

The point is to find inspiration and formulate hypotheses, find something interesting and eventually useful. So, you use data, to formulate hypothesis and ask better questions. Then in the decision-making processs, these can be communicated to statisticians and decision-makers, so that they have a clearer directions and promising candidates. This inspiration can be anything from exploratory data analysis, visualization, even to unsupervised ML methods, like dim red, clustering, etc, that is, data mining, a way to automate analytics, such that it finds patterns not readily visible from simple explorations and correlations. 

This doesn't mean that what we found (a difference between 2 groups of clients.). It doesn't mean we found the causal process which makes some clients more profitable than others. 

The same machinery which makes us intelligent and flexible, makes us prone to bullshit and self-deception, self-destructive behavior. Same with overfitting and drawing wrong inferences. We can't say anything yet about causality, but it is a tremendously important role, so there will be always a place for a good analyst in a firm. 

In statistics, first and foremost, we care to take decisions in a safer way, with more confidence (in a very specific sense, that the relationship, the found process is causal, i.e. is true outside or sample, that is, it generalizes) and less self deception, those are important, high-stake decisions. 
This is why we cannot do the mining and statistics on the same set of data. We will get next to the idea of splitting your damn data, such taht they are not contaminated by all the mining. This is why we have a process for experiment design and validating models, interpreting the parameters. If that stat model passes a rigorous critique, it has greater chance of finding a real/causal pattern. 

At the firm level, the kind of decisions that are high-stakes, are strategic, e.g. pricing policies, entering markets, etc. Those are not low-level decisions, distribution channels mix, advertisement allocation spend, whether to deploy a new recommender system.

In contrast, in ML we have lower-cost of mistake decisions, and there are a lot of tiny decisions. Of course, we need lots of data and a clear objective to optimize for. 

I consider it a success if at the end of the course, when you get the problem, you can juggle between these 3 perspectives (stat, ml, analytics) and can figure out in each phase of the project, where do you stand, in what bucket. ML and Data Mining is powerful, but not appropriate everywhere, so, you have to know when not to use it, or when you don't need the stats and just playing around. 

Q: did you have time series, data analysis, econometrics, statistics. Yes, all of them 
Somehow, when I graduated, I got the idea that it's about data analysis, to find something interesting in data, to make a good prediction. Here I will try you to un-teach you that, mentioning that it is just the first step in this kind of problems (the exploratory, detective work, the pattern mining).

Why ML, since we put so much accent onto scientific rigor and trying to infer the causal processes? Sometimes -- you don't have a theory, e.g. in recommender systems, just too complicated. Then ML is a way to go from experience (data) to expertise (program/recipe) in an automated way, eg. which makes predictions: how do you do that, with an algorithm and certain model classes (e.g. trees, regressions).

## Why did you study all of that?

Why did we have to go through all those excruciating months doing mathematical analysis, linear algebra, probability, statistics, econometrics, operations research, and lots of economics? 

It was very frustrating for me, because it wasn't clear how they fit together, what is the common thread, and more importantly, what part of the theory and particular concepts would be helpful in solving the kind of problems we discussed, and which ones are designed to enhance our academic understanding.

::: {.callout-tip}
## Sounds good -- doesn't work?

An important question is what works well in practice and why. On the other hand, what is intellectually fascinating, but not at all straightforward to apply. What is a minimal or most powerful set of the prerequisites that you need? 

Let's draw a map, stop at each field and in a sentence explain why we learned it and how it contributes to AI, Data Science, and ML. We mentioned form the very beginning that it is an interdisciplinary field, but it is not just an union of those subjects -- the inspirations and tools are quite carefully picked. 
:::

```{mermaid}
%%| label: fig-mermaid
%%| fig-width: 9
%%| fig-cap: |
%%|   Think of this as a stuctural organization of the fields and courses
%%|   you studied before. Some are more useful in analytics, some in ML
%%|   and some in making causal inferences, that is, based on data + theory.

flowchart TD
  LA[Linear Algebra] --> OR[Operations Research]
  MA[Mathematical Analysis] --> OR
  MA --> SD[Systems Dynamics]
  %% CS[CS Algorithms] --> OR
  
  PT[Probability] --> MS[Statistics] --> EC[Econometrics]
  EC --> Caus[/Causal Inference\]
  EC --> TS[Time Series]

 %% subgraph 1
  Caus --- DM[/Data Mining\] --- ML[/Machine Learning\] 
  ML --- Caus
 %% end

  OR --> ML
  MS --> ML
  MA --> PT
  SD --> Caus

  Caus --- Econ[[Economics]]  
  Econ --- GT[Game Theory]
  Econ --- DT[Decision Theory]

  style Caus fill:#f7f5bc
  style ML fill:#f7f5bc
  style DM fill:#f7f5bc

  DM --- FSDA[/Full-Stack Apps\]
  FSDA --- DB[Databases/SQL]
  FSDA --- OOP[OOP]
  Econ --- TS

  style FSDA fill:#f7f5bc
```


- **Linear Algebra** is a language of `data`. The vast majority of models and training algorithms can be reduced to operations on matrices. Therefore, it is not a coincidence that is almost the only tool we have, in order to take these models and implement them in code, on a computer. 
  - My perspective over linear algebra is ultimately **computational** and **geometric**, in the sense of the "space" the data points live in and the transformations of that space. 
  - Ultimately, no matter the data type: image, video, text, voice, structured, panel -- all can be represented as **multidimensional arrays** (or tensors, if you wish).
- **Mathematical Analysis** is all about `change`, formalizing how a function behaves with respect to its arguments and parameters. It is an essential building block in **optimization** and deep learning (automated differentiation).
  - I would argue that in order to understand any complex system, be it a firm, an economy, the climate or environnment, we have to model how it **evolves in time**.
  - This suggests the importance of differential equations and systems dynamics, modeling the feedback loops. All of this would be impossible to reason about without the mathematical analysis.
- **Probability** and **Statistics** is the language of `uncertainty`, the only instrument we have, that allows us to say something intelligent about how confident are we.
  - We will explore the role of statistics at lenght, but as a quote, think of it as a tool which enables us to **change our mind** and decisions in the face of evidence.
- **Econometrics**, in my personal opinion, tries to separate the signal for the noise and make inferences about the **causal processes** in economic decisions and phenomena. It specializes statistics in the domain of economics, by infusing economic theory -- because you can't derive a scientific theory from data alone.   
- **Time Series Analysis**, senso largo, bridges the gap between Systems Dynamics (which takes a more theoretical perspective) with statistics and probability (stochastic processes). It adapts those tools to make inferences and predictions about phenomena which evolves in time, that are dynamic in their nature.
  - I like the metaphor of **Data Assimilation**, which is actually an entire field trying to introduce the empirical dimension to differential equations.
- **Operations Research** is about **optimization** with constraints and **efficiency**. However, the problem is that often, we start from an Integer/Linear Programming problem formulation, and that is easy part -- to apply an existing algorithm. The hard part is to reduce a messy real world problem at a large scale to that formulation, especially under uncertainty and nonlinearity. 
- **Economics** touches upon a wide range of aspects of our society and human behavior. In mathematically-oriented courses, you can think of it as **optimization with constraints**, the constraints being given by our positive or normative theory of economic **decision-making**.
  - In this course and in practice, we care more about **business economics**. It's a very different beast from theoretical ideas in macroeconomics (ISLM, DSGE type of models) or microeconomic utilitarianism. 
  - By business economics, we mean marketing, management, corporate finance, decision theory, supply chain, and logistics. 
  - What you have to know about marketing, especially if you are skeptical like me, is that it became innovative, mathematical, rigorous and data-driven. Look at any marketing journal: how much econometrics and ML models are in there.
  - So, if you hold the opinion that marketing and management are fields full of fluff -- I advise you to rethink your positions. In the context of tech firms, you can't bullshit your way through it.
  - Moreover, when you combine marketing with **behavioral economics** and psychology, it introduces another layer of nuance and understanding over our decision-making.
  - When we make decisions, we like to think of ourselfs as objective, but we have lots of **biases** and blind spots which prevent us to see the reality clearly. We often find patterns and regularities which are just noise, not causal. So, the usefulness of this kind of domain knowledge from economics about human behavior helps us to be wiser, that is, to prevent self-deception and self-sabotage towards achieving the goals and objectives. 

::: {.callout-tip}
## A word of encouragement

None of those courses were useless. Think of how can we take parts from each of those prerequisites, which are relevant in ML/DSc, so that we have more tools to solve problems of the complexity we encounter. To reiterate, data science is an umbrella term, borrowing from them all.
:::

 



## Implicit Learning, Intuition and Bias

I would like to give a parallel here, of how people learn. We have a powerful capacity for implicit learning, meaning we can't articulate or explain how we did it, like riding a bike, learning to read and write. There is a famous experiment: Researchers invented words from two languages, with 2 rules, between 3-8 characters, appropriate vowels and so on and they generated 2 sets of words. 

Participants saw the list and they had to say where do they come from (matching) -- it's a good example of experiment design coming from cogsci research. They found that subjects differentiated them much better than chance. During the interviews, when asked to explain how they did it, it was either idk, or giving some rules, when implemented in a software, were indistinguishable from chance (i.e. those rules were NOT exactly how they were thinking), meaning they couldn't really articulate what they did there.

This means that we have a capacity to find patterns and regularities in the real world, due to evolution building into us this powerful machinery of implicit learning. 

Another example: Bait Shyness (ref: Ben-David), to the idea of priors being necessary for learning, when work well
Pigeon Superstition - when doesn't go well, picks up on noise, resulting in superstitios behavior. (insert images)

What is the common thread among these 3 stories: when all goes well we call it intuition, business knack, when goes awry, it's the bias or even worse cases bigot/prejudice: confirmation bias, recency, selection bias, misjudging horison -- we attribute to phenomena causal explanation when it's not. e.g. size of wedding to nr years (social status, community) in marriage, extroversion and attractiveness with competence, due to common cause and confounders. E.g. how religious are people vs years of life - Z: education numerous, numerous example. 
So, a part of wisdom is the ability to differentiate between corr/causal patterns. 

Close the paranthesis from cogsci, we have big data, domain knowledge and methodologies for hypothesis testing -- meaning, formal tools to deal with it. 

We're not here in a behavioral economic class, but we will try to cultivate the kind of active open-mindedness which try to identify those biases in us and correct our behavior and decisions. We try to see clearly and update our beliefs, keeping the kind of scout mindset, and it's easier in a formal field with all the statistical, mathematical tools. However, let's keep in mind how easy researchers are getting fooled (not only by randomness).

The good news is that sometimes, you just need a reliable prediction, as you're not intervening in the system causing a certain phenomena -- and by retraining ML models you can adapt to minor changes introduced by our interventions. Those ML models, they clearly didn't figure out a scientific explanation of the causal process, e.g. for demand forecasting in uber, and that just the prediction to be used in a larger ecosystem and environment for decision-making. 

Again, that is appropriate in low-stakes, large scale decisons. You might not care so much about the latent, causal process; but of course, you care that it generalizes to the population of interest (that is, a binding contract, a boundary in which your predictions are valid -- if you go outside that range, can easily get absurd predictions -- this is why this kind of system needs checks-and-balances, boxes to constrain the artificial stupidity). It is extraordinarily unlikely that these models translate to novel situations and environments without explicit transfer learning and careful adaptation. 


## Calling Bullshit in the age of Big Data
Small data problems in big data -- this is not the end of theory of stats. We have lots of entities with sparse information for each ones -- all about aggregation level, 100m clients, but new ones, without much information or not so many purchases. At a certain level of granularity, the data becomes, discrete, noisy, heteroskedastic. 

Even in ML, e.g. in demand forecasting, we can't fully escape the teory (which is our understanding of the world) -- we need to provide it relevant data, factors which are related, possibly influencing that demand, like weather, holidays, road blockings, factors for demand and supply. We can't just pour all this data into ML and expect the best: garbage in garbage out, it isn't clear that feeding irrelevant data doesn't break our model, st. it picks up on noise and **spurious correlation**, esp. in very powerful DL models -- doesn't help with better decisions.

Bullshit in the formal sense, introduced by Harry Frankfurt in his essay on Bullshit. A liar respects the truth, in BS you try to convince someone of something regardless of the truth, you distract their attention, providing relevance without truth.

In our age, it is more sophisticated than advertisement, tv trying you to buy something, manipulating, we can call it bs in the age of big data, transforming it into smth more quant, using jargon from stats, finance, economics -- when explaining why interest rates rose, what happened in 2008 gfc, lots of numbers and charts, which is intimidating. 

I hope that following this course, you will look behind this veil and use first-principles to figure this sort of stuff out. For the ones interesting, check out the calling bullshit course. It's all about how to lie with statistics, esp. in graphics and visualization. 

There is no magic in AI, no silve bullet, these are concepts inspired from stats, optimization, algorithms. IF we asked the right questions and formulated the problem and objectives well (which should be consistent with business outcomes), to guide us towards an answer. e.g. AI for who enters the quarantine -- what do yo optimize for? Infections, hospitalizations or deaths -- large discussion here? So, a problem well framed is most of the answers + clean and relevant data -- only then, we can claim an element of intelligence right there. Critical thinking becomes that much more important, when we have these powerful quantitative tools at out fingerprints, that is a part of data scientist's job is to constrain artificial stupidity (more exactly, foolishness, because it does perfectly fine what you instructed it to do) and making sure we're solving the right problem (sounds trivial, but ofter we solve the wrong problem, without being aware of it). 


**Discounting bias**
The way we care about immediate things, which are supersalient  -- when it goes well evolutionarily, smoking - when it doesn't add up, all the possible deaths, we underestimate the risk. Connect it to probability theory (tree with paths -- estimating probabilities). 


::: {.callout-tip}
## Causal vs Correlational Patterns

:::



<!-- Data Science Context, in Business, Interdisciplinarity --->
[^pragmatic-ai-gcp]: M. Bizovi - [Pragmatic AI in Google Cloud Platform](https://www.youtube.com/watch?v=02NPR_nDaxQ)
[^jordan-ai]: K. Pretz - [Stop Calling Everything AI](https://spectrum-ieee-org.cdn.ampproject.org/c/s/spectrum.ieee.org/amp/stop-calling-everything-ai-machinelearning-pioneer-says-2652904044), Machine-Learning Pioneer Says 
[^jordan-revolution]:  M. Jordan - [Artificial  Intelligence](https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/9): The Revolution Hasn’t Happened Yet
[^cassie-mfml]: C.Kozyrkov - [Making Friends with Machine Learning](https://youtube.com/playlist?list=PLRKtJ4IpxJpDxl0NTvNYQWKCYzHNuy2xG)
[^cassie-ref]: C. Kozyrkov (Chief Decision Scientist @Google) - https://kozyrkov.medium.com/ 
[^cassie-ai-def]: C. Kozyrkov - [AI is decision-making at scale](https://www.youtube.com/watch?v=bCjMhZZYlP4)
[^cassie-steps]: C.Kozyrkov - [12 Steps to Applied AI](https://medium.com/swlh/12-steps-to-applied-ai-2fdad7fdcdf3)
[^pair]: People and AI Research @Google - [Guidebook](https://pair.withgoogle.com/guidebook/)
[^mit-ai-covid]: W.Heaven - [Hundreds of AI tools have been built to catch covid. None of them helped.](https://www.technologyreview.com/2021/07/30/1030329/machine-learning-ai-failed-covid-hospital-diagnosis-pandemic/)
[^war-wrong-problem]:  V. Warmerdam - [The profession of solving the wrong problem](https://www.youtube.com/watch?v=kYMfE9u-lMo)
[^war-stupidity]: V. Warmerdam - [How to Constrain Artificial Stupidity](https://www.youtube.com/watch?v=Z8MEFI7ZJlA)

[^37]: Thoen - [Agile Data Science with R](https://edwinth.github.io/ADSwR/index.html)
