---
format:
  html:
    toc-location: left
---

## A map for the adventure

I always start with the first module, in which we explore [decisions](01_fundamentals/background.qmd) in businesses, figure out what does AI mean, and where it adds value. If we view statistics as **changing our minds and actions in the face of evidence** -- fundamental statistical concepts and tools will shine in a new light. It will become clear why those models and methodologies were invented in the first place.

- [**Module I**](#module-i-business-decisions-and-probability): Business Decisions and Probability Fundamentals
- [**Module IIa**](#module-iia-bayesian-statistics): Applied Bayesian Statistics
- [**Module IIb**](#module-iib-ab-testing-and-causal-inference): A/B Testing and Causal Inference[^r3]
- [**Module III**](#module-iii-ml-and-deep-learning): Probabilistic ML and Deep Learning
- [**Projects**](#module-iv-full-stack-data-apps): Software Engineering and Full-Stack Data Apps

[^r3]: ![ We will see that formulating a hypothesis is an art in itself, and that experiment design in practice is fascinating and complicated, full of pitfalls and dangers. That's not your stats 101! *(Source: [McElreath](https://youtu.be/KNPYUVmY3NM))*](01_fundamentals/img/meme-causal.jpeg){width="90%"}

<!-- | ![picasso](01_fundamentals/img/safari.svg){width="80%"}| ![big picture](01_fundamentals/img/meme-causal.jpeg){width="80%"}|
|:--:|:--:|
| **[Module I](#module-i-business-context-and-probability):** Business Context and Probability Fundamentals | **[Module II](#module-iia-bayesian-statistics):** Applied Bayesian Statistics and Causal Inference -->


<!-- | ![picasso](01_fundamentals/img/umap.png)| ![big picture](01_fundamentals/img/LE_pyeco1.svg){width="100%"}|
|:--:|:--:|
| **[Module III](#module-iii-ml-and-deep-learning):** ML and Deep Learning | **[Projects](#module-iv-full-stack-data-apps):** Software Engineering and Full-Stack Data Apps -->


These fundamentals are not "just theory", it is what will make or break our project in practice. We will achieve a lot with simple, elegant models; appreciate the importance of asking the right questions, persuasive communication, and storytelling with data. That's why it's essential to take every tool, and **apply** it in the context of an appropriate use-case/application -- meaning, **we'll code a lot**.


::: {.callout-tip}
## Go to "Resources" page for practice!

Conceptual understanding by itself is not enough. So, I curated a [list of resources](references.qmd) to practice on interesting case-studies, datasets, which directly apply the models, tools, and methodologies presented. These are written by experts in the field, are usually well thought, easy to follow, reproducible, and highlight important aspects of a problem and model.

Also, keep an eye on the course [github repo](https://github.com/Bizovi/decision-making), in which we'll do some exciting projects (full stack data apps) and investigate common problems/challenges with a fresh perspective.
:::

Once we have a confident grasp of the fundamentals, we continue with understanding and applying Bayesian Statistics. It is an extremely flexible and composable approach to building explainable models of increasing complexity and realism. To put it bluntly, it will enable us to improve on most challenges in decision-making that businesses face. 

Did you get comfortable with building custom statistical models for inference and prediction? For decisions with high stakes, we often want to do controlled experiments. We'll develop a rigorous methodology for designing and performing A/B tests, learn how to recognize and avoid pitfalls that it's so easy to fall into.


Often, A/B tests and randomised experiments are unfeasible or unethical. We also acknowledged the limitations of inferring an association[^caus1], so it's time to get more powerful tools from causal inference. We can leverage natural experiments. This is truly challenging: it is an art and science, in contrast with the auto-magic pattern recognition of ML. It requires deep thinking and understanding. 

[^caus1]: Realize that we cannot reach a causal conclusion from observational data alone. We need a theory, which is our understanding of how the "world" works -- translated into a statistical model, plus data, which will give us new insight into the causal processes (the evidence).


::: {.callout-important}
## What if I care only about ML?

Even if you're interested only in machine learning, most practitioners will emphasize the importance of mastering regression (generalized linear models) and doing A/B tests to gather evidence that our new model brings an improvement. 

This is how we jump through various buckets, highlighting the golden thread linking them all: decisions and uncertainty. Moreover, the tools we learned in Bayesian Statistics are directly applicable in ML -- the lines between these two fields are indeed very blurry.
:::

Often, we care not just about a single decision or developing better policies, but we have to make tons of little decisions at scale. This is when we switch to a predictive, Machine Learning perspective and walk through our workhorse models, which should serve us decades ahead in a wide range of applications: both predictive and exploratory.

During the labs we'll build from the ground up a tech stack for reproducible data analysis, model and data pipelines, culminating in a full-stack data app (with user interface, backend, database), which solves a real-world problem.

::: {.callout-tip}
## Build an impressive project for your portfolio

That is your final project for the course[^proj] and something you can brag about in your portfolio and github profile. It sounds complicated, but we have the tools to make it easy for us. Don't worry about getting everything right, but focus on a problem and single area from the course you're passionate about: be it data visualization, ML, statistics or sheer engineering curiosity. 
:::

[^proj]: A project can be much simpler: a quarto blog to showcase your research, a python package and CLI application to train a model, a streamlit app to demo a model.

The icing on the cake is miscellaneous topics dear to me and usually not covered in such a course: Demand Forecasting, Recommender Systems, and Natural Language Processing. All extremely useful in business contexts, but significant tweaks are needed to the models discussed before.


## Module I: Business Decisions and Probability

The first module is putting the rest of the course on a solid foundation, emphasizing again and again the key idea of Decision-Making Under Uncertainty at Scale. Understand the problem, the methodology, and the tools at your disposal.

The most difficult part of the module is the statistical fundamentals, as it is very easy to misunderstand and apply these methods mechanically. Therefore, fundamental -- doesn't mean easy, nor basic, nor trivial. Most courses make you solve puzzles, but I ask you to appropriately define, justify, and apply the choice of method and tool in the context of business applications.

::: {.callout-important}
## Do you have a process for Statistics and ML?

All too often we jump into data analysis and modeling without formulating well the objectives and the problem. Focusing on how to implement and use a model  without following a rigorous process can be error-prone and counterproductive. You have to understand these 3 processes and think how would you structure your project in practice for a particular application.
:::

There are many ways to organize this module in lectures. You can find a possible schedule in the table, with associated readings/resources and a mind-map of topics in the diagram below it. Some lectures are conceptual, some *mathematical* and in some we **code and practice**. 

The courses I teach are usually short and intensive, so the following schedule would take two weeks to cover, meaning the next two weeks I can focus on Bayesian statistics, causal inference, or machine learning -- with more emphasis on modeling or engineering, depending on the audience.

::: {.column-page-inset-right}

|   | Lecture Agenda                      | Keywords  | Case Studies |
|---|---------------------------|---------------|--------------|
| 1 | [Data Science in Business Context](https://course.economic-cybernetics.com/01_fundamentals/background.html). Decision-Making Under Uncertainty | AI, Cybernetics, Uncertainty, Scale, Analytics vs Stats vs ML  | survey of applications in various domains |
|2 | [Interdisciplinarity](https://course.economic-cybernetics.com).  Overview of prerequisites. | Calculus, Linear Algebra, Probability, Stats, Economics. Math Iceberg | [Probability vs Statistics](https://johnkerl.org/doc/prbstat/prbstat.html), [Why did you study all of that?]((01_fundamentals/prerequisites.qmd)) |
| 3 | Business Analyst's Workflow. Newsvendor Problem  | action, outcome, strategy, expertise, data, models, simulation, DAGs | TIME INC. printing quantity decisions |
| 4 | Strategy and the Desired Trajectory | SWOT, Systems Dynamics, KPIs, Stock vs Flow | LRB Journal Subscriptions |
| **5** | **Projects Overview**. **Full-stack data apps**. Overview of Data. | NLP, RecSys, Time Series, Classification | Mercari, Favoritas, H&M, Olist (e-commerce)  |
| *6* | *Probability Triple. Random Variables. Mathematical Statistics* | population, sample, r.v., estimators, mixtures, markov chain | Retail Fashion - appropriate choice of distributions  |
| **7** | **Set Up the Environment**. **Reproducible Research I** | terminal, conda, python, vscode, jupyter, git & github, poetry, pytest | DevXP, productivity, reproducibility |
| 8 | Bernoulli, Binomial, Poisson, Simulation, DAGs as stories | numpy, pandas, pymc, matplotlib, arviz, graphviz | couples showing up to safari |
| 9 | Bayes Rule | DAGs, Conditioning, Marginalization, Priors | Football Spreads, Medical testing |
| 10 | Beta-Binomial Model | pymc, praw, priors, conjugate families | A/B testing |
| 11 | [Newsvendor Problem](https://youtu.be/QLsSPnwWS_M). Probability distributions. Stochastic Processes | model specification, inference, simulation, optimization, constraints | groceries and perisable goods, [price optimization](https://youtu.be/RScjq1mkDc0) |
| *12* | *The most dangerous equation* | estimators properties, CLT, LLN, sample size | U.S. small schools, reddit upvote share, hackernoon algo |
| *13* | *Estimator properties. Bias-Variance* | $\chi^2_k, N, F_{m, n}, t_k$ | What does a statistician want?  |
| 14 | [12 Steps of ML](https://medium.com/swlh/12-steps-to-applied-ai-2fdad7fdcdf3). [What is ML?](https://work.caltech.edu/lectures.html)  | PAC Learning, generalization, CRISP-DM | Project requirements simplification |
| 15 | [12 Steps of Statistics](https://youtu.be/R4ckbZCgmxQ). A/B Testing Scheme | Default Action, Stat. Hypotheses, confidence intervals, relevance | problem of p-value, [statistics workflow](https://www.youtube.com/playlist?list=PLRKtJ4IpxJpBxX2S9wXJUhB1_ha3ADFpF) |
| **16** | **Reproducible Research II**| automation, code, data, environment, documents, publishing | Replication crisis, Excel and the Dead Salmon |

:::

Why these exact topics, since other courses in 16 lectures teach you either statistical learning, probability, statistics, causality, deep learning? Of course, and they're excellent at teaching exactly that, so I refer to those amazing resources, books, courses when it comes to the tool/model and didactic examples.

Our goal in the first module is to have a solid general foundation for all of those topics and domains. In order to achieve that, we need to understand the business environment and challenges it faces, mathematically and conceptually get the main ideas from probability and statistics, and be prepared with our computational skills and tooling to tackle those challenges. Here are all of these concepts illustrated in the following mind-map:

::: {.column-page-inset-right}
```{mermaid}
%%| label: fig-mermaid
%%| fig-width: 9
%%| fig-cap: |
%%|   A mindmap to help you navigate the first module, with 3 main branhes and programming sprinkled everywhere: business, statistics, and methodology.

flowchart LR
  Prob(Probability) --> Boot(Inference) -- LLN --> Es(Estimators)  --> PS[[Stat process]]
  Prob -->  Cond(DAGs) -- Cond-g -->  Bayes(Bayes Rule)

  DSc(Decisions, AI) -- statistics  --> Prob(Probability)
  DSc -- business  --> VUCA(V.U.C.A.) --> App(Applications) --> R(Role in firms)


  R --> BAW[[Analyst's Workflow]]

  DSc -- method. --> A(Analytics) --- Caus(Causal Inference) --- ML(Learning from Data)
  ML --> PML[[ML Process]]

  App --> NV(Newsvendor) --> Proj(Projects Overview)

```

:::



## Module IIa: Bayesian Statistics


::: {.column-page-inset-right}

```{mermaid}
%%| label: fig-mermaid
%%| fig-width: 9
%%| fig-cap: |
%%|   We develop a composable toolbox, capable to tackle the challenges of nonlinearity, heterogeneity, low level of aggregation, discreteness, missing data and heteroskedasticity. By putting together the right pieces in a right way, we can improve predictions and decisions in most aspects of a business. 

flowchart TD
  DM(Decisions)
  DM --> BB(Beta-Bin*) --> GPo(Gamma-Pois*) --> NIG(IG-Norm) --> G(Groups)
  BB --> LR(Logistic Regr.)
  DM --> LM(Linear Regr.) --> GLM(GLMs)  --> Caus(Hierarchical Models) --> Misc[/Time Series/]
  LR --> GLM
  GPo --> GLM
  G <--> Caus

  LM --> P(Prior Choice) --> Reg(Regularization) --> VS(Variable Selection)
  LM --> MS(Model Selection) --> LOO(LOO-PIT)
  LM --> Rb(Robustness) --> NA(Missing Data) --> Surv(Survival and Censoring)
  LM -- nonlinear --> NL(Splines) --> BART(BART) --> GP(Gaussian Proc.)

  DM --> DA(Decision Theory) --> NV(Newsvendor pb.)
  DA --> CF(Classification)
```

:::


::: {.column-page-inset-right}

| Week  | Title                      | Keywords  | Case Studies |
|---------------|-----------------------|-------------------|---------------|
| S13 (21 Jan) | Bayesian inference | Priors, Bayes Factors, Credible intervals | Generative models, Model critique |
| W1 (L7) | Pricing Decisions. Bayesian Inference. Link functions | demand elasticity, fixed capacity, Bayes, revenue management, DAGs  | VIP Lounge at Festivals |
| W1 (L8) | Bayesian Workflow. Regression, what is GLM | prior, likelihood, stochastic optimization  | Flight price optimization (static) |
| S12 (21 Jan) | Likelihood, Regularization | Likelihood ratio, James-Stein, curse of dimensionality | Getting a taste of the Fisherian perspective |

:::


## Module IIb: A/B Testing and Causal Inference

::: {.column-page-inset-right}

| Week  | Title                      | Keywords  | Case Studies |
|---------------|-----------------------|-------------------|---------------|
| C9 (13 Jan) | A/B Testing pitfalls | A/A Tests, Selection bias, Confounders, Novelty | The need for rigorous experiments |
| C10 (13 Jan) | Error Types, Effect Size, p-values | tests for difference in proportions and means | Strategy for simulation |
| S9 (14 Jan) | Power, Sample Size calculations, Non-inferiority tests | relevance, minimal effect size | Neyman-Pearson, frequentism as action in the long-term |
| S10 (14 Jan) | Self-practice: Linear regression review | Hypothesis tests as linear models | Guidance with student projects |
| C11 (19 Jan) | Properties of Metrics for A/B Testing | Sensitivity, stability, directedness | Potential pitfalls |
| C12 (19 Jan) | Properties of Estimators | Bias-Variance, Fisher Information, Rao-Cramer | Relevance for ML, Deep Learning |
| C14 (20 Jan) | Bootstrap and Nonparametrics | resampling, ECDF |  Kolmogorov-Smirnov |
| S11 (21 Jan) | CLT, Choosing a test, Trading off errors, Increasing Power. What next? | Cohen's d, PPV, GLM, nonparametrics, multilevel models | Justifying $\alpha, 1 - \beta, \Delta, n$ by simulation |
| S14 (21 Jan) | Bootstrap, Multiple Testing | FDR, Bonferoni | Computer-age statistical inference |

:::


## Module III: ML and Deep Learning


::: {.column-page-inset-right}

```{mermaid}
%%| label: fig-mermaid
%%| fig-width: 9
%%| fig-cap: |
%%|   In this ML/DL module, we focus on practical, challenging use-cases and reliable, workhorse methods -- while keeping in mind the particularities of the domain and applications. 

flowchart LR

  DSc --> Unsup(Dim. Reduction) --> Clust(Clustering) --> MM(Mixtures) --> HDB(HDBScan)
  Unsup --> PCA --> CA --> UMAP

  DSc --> Cl(Classification) --> T(Tree-based Models) --> BG(Bagging) --> XG(Boosting)

  Cl --> Im(Imbalance) --> F(Fraud Detection)

  DSc(Decisions, Scale) --> Text[/NLP/] --> EM[Embeddings] --> Attn[Attention] --> ABSA[ABSA]
  DSc --> RS[/RecSys/] --> Mtr[Metrics] --> FM[Factorization Mach.] --> HM[Hybrid Models]
  DSc --> CV(DL: Vision) --> Conv[CNNs] --> AK[Approx. kNN]

  Conv --> HM

  DSc --> TS(Time Series) --> MTS[Metrics] --> XGB[ML Approaches] --> DL[DL Approaches]
  EM --> HM

```

:::


## Module IV: Full-Stack Data Apps


::: {.column-page-inset-right}

```{mermaid}
%%| label: fig-mermaid
%%| fig-width: 9
%%| fig-cap: |
%%|   We will need to learn a lot of engineering and new tools, so that we're able to collect, clean, explore, visualize data, train models, and build useful applications which improve outcomes for our clients.

flowchart LR
  DSc(Data Apps)  --> Data(Data Wrangling)  --> EDA(EDA) --> DV(Visualization)
  EDA --> LP(Literate Programming)
  
  DSc --> Repr[Reproducibility] --> DP[Data Pipelines]
  DSc --> M[Modeling] --> Pymc(PyMC) --> Tr[Torch]
  DSc --> MLP[ML Pipes] --> Srv(Serving Models)
  
  DV --> FS[/Full Stack Apps/]
  Tr --> FS
  Srv --> FS

  Repr --> LP
  DP --> FS
  FS --> Dep[Deploy]
```

:::



