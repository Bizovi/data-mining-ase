I assume you read the conceptual stuff on this website, understood its use-cases, and the big picture of what to learn. Now, the question is **where and how to start** practicing, while catching up on the maths and programming.

::: {.column-margin}
![Decision-Making, ML, and Causal Inference is hard. Practice the fundamentals with patience and care, develop competence. Then, a beautiful world will open up to you!](01_fundamentals/img/karate-kid.webp "Practice"){width="90%"}
:::


There are too many courses, tutorials, and datasets for practicing ML/Stats on the web: from didactic toy examples to industrial-scale machine learning. Since it is so hard to strike a balance between clarity, simplicity, and the use-case being interesting, realistic -- you fill find here a list of **carefully curated resources**. Most of them have have **code**, **data**, and explained **theory** or methodology in a freely available e-book.


There is little point in replicating well-executed examples from other authors, just for the sake of consistency of code and style. I will, however, migrate examples not available in our frameworks or language of choice. Other times we can benefit from a significant improvement over the original presentation or improving code quality.


::: {.callout-tip}
## Right level of granularity

Instead of recommending whole books, which in this field are huge: 500-900p of dense material -- in the listings below, you have them grouped by tool, use-case, in manageable-sized chunks. 

They are ordered and organized in a way which facilitates a more linear, gradual, composable development of skills and understanding. **Think of them as lego pieces.**
:::


I made sure to include interesting examples and archetypal applications for each statistical tool and theoretical topic, so you can immediately apply the concepts you read about or watched during the lectures. However, the responsibility to practice falls entirely on the learner -- I can just do my best to make your journey less frustrating and more efficient.

::: {.column-margin}
Often, you will have to combine various aspects of an use-case, model, method -- from different sources, taking the best from each author.
:::

## Prerequisites: Probability and Python

We start from the foundations of probability, however, I do not cover calculus, linear algebra, or other mathematical tools for data analysis. We do much more programming than mathematics, therefore, the ability to code in a programming language like Python or R is a must.

::: {.callout-warning}
## Need a crash course in probability and statistics?

If you understand and can explain the following ideas in a simple, yet rigorous way  -- you're ready for the journey. Otherwise, if it feels shaky, here are some readings:

- **Probability Triple** and **Random Variables** - a quasi formal introduction is written in [this chapter](https://course.economic-cybernetics.com/01_fundamentals/stat_foundations.html) of the course website. From my experience, not many students have this understanding after their probability theory classes.
- **Collectivity** ("physical" structure), **Statistical Population**, **Sample** - where defining the population is the contract for our experiment, and the sampling process is critical. It is a much more nuanced topic than it looks, explained well and in great detail [here](https://openintro-ims.netlify.app/data-design.html) and [here](https://crumplab.com/statistics/04-SamplesPopulations.html).
- **Parameter** (Estimand), **Estimator**, **Estimation/Statistic** - same resources as above [^4]
- Stories behnid distributions: $Bin(n, \theta), Pois(\lambda), Exp(\lambda), N(\mu, \sigma^2), \chi^2_k, t_k$. What about $Beta(a, b)$, $Gamma(k, \theta)$, and $Dirichlet(\bar{\theta})$ -- what are they good for?
  - Look at some [examples with simulations](https://mathstat.slu.edu/~speegle/_book/probchapter.html#simulationsprob) and [stories / applications with more math](https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view) from Joe Blitzstein.
:::

[^4]: Although the perspective I take is Bayesian, I will take some time to cover and re-contextualize the Neyman-Pearson and Fisherian "frequentism".

## Building Blocks of Bayesian Statistics

We start simple, by modeling a single random variable $Y$, choosing the appropriate distribution for each phenomenon, a prior for the parameters, doing simulations  -- then sampling from the posterior with `pymc`, `numpyro`, and `bambi`.[^1]

[^1]: The R equivalents would be `stan` and `brms`, `rstanarm`. `bambi` and `brms` are a high-level API for most common models.

Limiting? Yes, as in reality we care about the relationship between random variables. However, we can get a lot of insight from thoughtful modeling the data generating process, which will serve as building blocks in more complicated and realistic models.

::: {.callout-note}
## Bayes Rule. Update your beliefs, often!

Any introduction to the subject will work out, a few excellent ones being Chapter 1/2 of [BDA3](http://www.stat.columbia.edu/~gelman/book/), Chapter 1/2 of [Bayes Rules](https://www.bayesrulesbook.com/chapter-2.html), and Chapter 1/2 of [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/).[^2] If you prefer videos, enjoy the 3Blue1Brown [visual masterpiece](https://www.youtube.com/watch?v=HZGCoVF3YvM) on how to think like a Bayesian or [here](https://www.coursera.org/learn/statistical-inferences/lecture/R6nV5/bayesian-thinking).

- (BDA3, Ch1): **Football spreads**, that can be estimated from [data about matches](http://www.stat.columbia.edu/~gelman/book/data/football.asc). What is the probability that a team wins? Are experts right, on average?
  - If you're into betting and sports, can you replicate the analysis on other datasets? What are your options for data collection?
  - For brevity, I won't elaborate much from now on, how to take an use-case and example to its limit. **If you're passionate about a particular topic -- go for it!**
- (BDA3, Ch1): **Spelling correction**, based on [empirical frequencies](http://norvig.com/ngrams/) provided by Peter Norvig. As in the previous case-study, you will have to code it up and figure it out for yourself -- it is good for a warm-up, but challenging enough to keep you occupied.
- ([Probability 110](https://projects.iq.harvard.edu/stat110/home)): **Medical testing** for rare diseases, hypothetical example with code in my [course repository](https://github.com/Bizovi/decision-making/blob/main/playground/02_bayes.ipynb). We use the same idea to reason about how confident are we our code has no bugs.
  - If you remember the Covid-19 rapid tests and their confusion matrices printed on instructions, you could've applied the same idea!

For the simplest models, one approach of comparing different hypotheses, is Bayes Factors. However, these do not translate well in practice for more sophisticated, multilevel models. You can look in the following courses [here](https://www.coursera.org/learn/statistical-inferences/supplement/IPkZK/assignment-2-2-bayesian-statistics) and [here](https://www.coursera.org/learn/bayesian/home/week/3) for the theory and examples.
:::

[^2]: You will notice in the callouts that I point out where to read the theory -- for a conceptual understanding and to figure out the mathematical details

[Or maybe you're passionate about biology, where you could apply it for Mendelian genetics and think about the mystery of deadly genes persistence]{.aside}

We applied Bayes rule and got some insightful results in three totally different domains. However, we weren't doing neither statistics, nor inference -- but got into the right mindset. It is a good opportunity to brush off the shelves some computational and mathematical tools. Now it's time for full-luxury Bayes and the simplest cases of inference.


::: {.callout-note}
## Beta-Binomial Model. Estimating proportions

I know, I know, the coin-tossing -- simple, yet fundamental and found anywhere there is a binary outcome $Y_i \in \{0, 1\}$. There are many ways to estimate the success probability $\theta$, when we observe $k$ successes from $n$ trials.

- [In Bayes' Rules](https://www.bayesrulesbook.com/chapter-3.html) is a detailed exposition of the theory, with examples about Michelle's election support and Milgram's experiment.
- Share of biking traffic in different neighborhoods (BDA3, Ch2, Pr. 8)
- A/B Testing for proportions in [BMH](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC_current.ipynb). Just remember that experiment design is much more nuanced than performing such inference or a test.
- Political attitudes in 2008 Election [data](http://www.stat.columbia.edu/~gelman/book/data/) (BDA3, Ch2, Pr. 21)
- Proportion of female births, given prior information. (BDA3, Ch2)

There are many more applications, but the ones below require more research and work. They are open ended and you can take these topics very far.

- The debate on the hot hand phenomenon is not yet over. [Here](https://blogs.cornell.edu/info2040/2018/11/29/hothand/) are the [bayesians weighting in](https://statmodeling.stat.columbia.edu/2015/07/09/hey-guess-what-there-really-is-a-hot-hand/) and some [new research](https://www.thecut.com/2016/08/how-researchers-discovered-the-basketball-hot-hand.html).
- Basketball shot percentages and true shooting, brilliantly explained by [thinking basketball](https://youtube.com/playlist?list=PLtzZl14BrKjTJZdubjNEY5jU0fGOiy51x) in a playlist about NBA statistics.
- Important problem in ecology: estimating size of population based on samples (BDA3, Ch3, Pr. 6). The challenge is that in $Bin(\theta, N)$ both parameters are unknown. Here is an [old paper](https://pluto.huji.ac.il/~galelidan/52558/Material/Raftery.pdf).
- Confidence intervals and [lower bound](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC_current.ipynb) for reddit posts like/dislike ratio. Read more about simple ranking algorithms and the issues of sample size: [reddit](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html), [hacker news](https://news.ycombinator.com/item?id=1781013). It is a good opportunity to work with the reddit API in order to collect data about posts and comments.
:::

The next step is learning how to model count data, which will open up to us applications of a different flavor. It is not a coincidence that when learning linear regression, we will extend it to poisson and logistic regression.

[You can notice how the issues of sample size creep in, as well as how to properly model variation within and between groups. I recommend you look up again the CLT, in the next section]{.aside}

Note that **prior choice** and justification is an art and science: you will have to learn and practice how to articulate assumptions and encode your domain knowledge into the priors. There is no universal recipe, but there are some guiding principles.


::: {.callout-note}
## Poisson Distribution. Gamma-Poisson Model

Counts of independent events in a unit of (space/time/...), with a low probability. You can review the maths [here](https://www.bayesrulesbook.com/chapter-5.html#gamma-poisson-conjugate-family). Below is a list of applications you can practice on:

- Deaths by horses in Prussian Army. Here is the [historical data](https://rpubs.com/SmilodonCub/567089) and a [blog post](https://towardsdatascience.com/poisson-distribution-from-horse-kick-history-data-to-modern-analytic-5eb49e60fb5f) if you need a refresher on Poisson distribution.
- Asthma mortality (BDA3, Ch2)
- [Airplane fatal accidents](https://www.briancallander.com/posts/bda3/chapter_02_exercise_13.html) and passenger deaths
- Estimating WWII [production of German](https://en.wikipedia.org/wiki/German_tank_problem#Bayesian_analysis) tanks based on samples captured
- Comparing football teams and [goals in football matches](https://allendowney.github.io/ThinkBayes2/chap08.html)
- [Comparing birth rates](https://allendowney.github.io/ThinkBayes2/hospital_birth_rate.html) in two hospitals

Check out the link functions for more sophisticated models. Also, in the examples above, we estimate the groups separately (corresponding to no pooling) -- there are better ways. Also, you will see a poisson example of how to take into account the sample size 
:::

The next examples are a little detour, to appreciate the flexibility of the modeling approach we're taking. We're building upon previous models, by inferring which rate $\lambda_1$ or $\lambda_2$ is the most plausible at a given point in time. This way, we add complexity and realism to the model, by incorporating knowledge about the phenomenon we're interested in.

[Ideally, we would leverage models which work well with time-series, like Hidden Markov Models. There is also a large literature in mining subsequences in a time series.]{.aside}

::: {.callout-tip}
## Poisson changepoint detection

Estimating rates, modeling a structural shift/change is a relevant, challenging, and unsolved problem in many fields. The models below are too simplistic to be useful in practice, but they capture the essence of real dynamics: things change not only continuously, but also structurally.

- [Coal Mining disasters](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters), pymc
- [Text Messages](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC_current.ipynb), pymc
- [U.S. School Shootings](https://sidravi1.github.io/blog/2018/05/22/what-happened-in-2006), is there an increase in attempts and occurences?
:::

By now, you encountered many simple Bayesian models, so it's useful to contrast it with the Fisherian and Neyman-Pearson (frequentist) approaches. The motivation is simple: the likelihood approach is widely used in Machine Learning (and Statistical Learning) teaching and practice. A nuanced understanding of frequentist methods will improve the way you ask statistical questions, perform and design experiments.

[In the courses I teach, I dedicate quite a lot of time on how not to fall into the most common pitfalls when applying frequentist methods. It's an useful skill when critically reading the literature.]{.aside}

When we get to the topic of A/B testing and experiment design, we will unavoidably stumble upon a few fascinating philosophical questions in relation to the nature of evidence. A quick overview of the below resources will give you most background you need to understand the heck those philosophical positions are about.

::: {.callout-note}
## Bayesian vs Frequentist vs Fisherian Inference

- I like very much the following metaphor, explained [here](https://www.coursera.org/learn/statistical-inferences/lecture/qC3A1/frequentism-likelihoods-bayesian-statistics), of looking at these 3 approaches as a: Path of Action, Path of Devotion, Path of Knowledge.
- Watch this [lecture](https://www.youtube.com/watch?v=LYcu3LoGqKc) by Zoltan Dienes to get a sense of the orthodox, Neyman-Pearson approach: its power and limitations.
- Likelihood - either from previous books or [this interactive visualization](https://rpsychologist.com/likelihood/). You can use the [lecture and lab](https://www.coursera.org/learn/statistical-inferences/lecture/8yZDk/likelihoods) from TU Eindhoven courses mentioned below.
- Bayesian vs Frequentist inference - [chapter 2, 3, 4](https://hastie.su.domains/CASI_files/PDF/casi.pdf) in Computer Age Statistical Inference. This [lecture](https://www.youtube.com/watch?v=NHFfJEvzPIo) by Zoltan Dienes contrasts Bayes Factors vs classical methods in t-test situations.
- The best teaching of (mostly) frequentist statistics I know of is in these two courses by TU Eindhoven and it is well worth your time:
  - Improving your statistical questions, [coursera](https://www.coursera.org/learn/improving-statistical-questions)
  - Improving your statistical inferences, [coursera](https://www.coursera.org/learn/statistical-inferences)
:::


We already worked with multiple parameters, even touching upon the relationship between two variables: counts and time $Y_t$, but not really -- it's more helpful to think about that in terms of stochastic processes. Therefore, we need a new tool, which is a link function, a nonlinear transformation $g(x)$ which maps $X$ to the correct support of $Y$. I recommend to introduce this before jumping into linear regression (LR), in order not to have the (flawed) impression that the LR is the only game in the town.

::: {.callout-tip}
## Link functions. Golf case-study

The domain/geometry inspired, custom model is presented in [pymc](https://www.pymc.io/projects/examples/en/latest/case_studies/putting_workflow.html) version, [stan](https://mc-stan.org/users/documentation/case-studies/golf.html) by Andrew Gelman, and [stan](https://avehtari.github.io/ROS-Examples/Golf/golf.html) by Aki Vehtari. It is modeling the relationship between distance and the probability of put, which is nonlinear and the sigmoid function won't work well for this case.
:::

Of course, we cannot forget about the Normal/Gaussian distribution, which is so prevalent in nature and pops up everywhere in the statistical practice. It is the building block of many following models. Remember the key idea of the expectation of independent random variables and estimating the mean from samples. Also, keep in mind any time you're doing regression that it's all about the conditional expectation $\mathbb{E}_\theta[Y|X=x]$.

::: {.callout-note}
## InverseGamma-Normal

- You can find the theory and mathematical exposition in Bayes Rules, with a case-study of football concussions [study](https://www.bayesrulesbook.com/chapter-5.html#normal-normal-conjugate-family)
- Speed of light experiment (BDA3, Ch3), [data](http://www.stat.columbia.edu/~gelman/book/data/). You will find here and in any statistics textbook the cases of known and unknown variance, and how the $t_k$ test is derived from the first principles.
- As in the case of proportions, we can use the model above to model the difference between the means of two independent groups.

TODO: more examples from science, business, and human behavior
:::



## Groups and Partial Pooling

::: {.callout-note}
## Beta-Binomial for groups

:::

::: {.callout-note}
## Normal Approximation. Agresti-Coull confidence interval

:::

::: {.callout-tip}
## A/B Testing and Multi-Armed Bandits
:::

::: {.callout-tip}
## The most dangerous equation
- Central Limit Theorem (Kolmogorov), deMoivre - the most dangerous equation. Asymptotics. [the theorem and simulations here](https://mathstat.slu.edu/~speegle/_book/SimulationRV.html#centrallimittheorem)
- Estimator properties: Bias, Consistency, Efficiency - you can find an accessible explanation with R code in [openforecast](https://openforecast.org/sba/estimatesProperties.html)
:::

::: {.callout-note}
## Gamma-Poisson for groups

The models become more complicated as we attempt to estimate parameters for **each group** of $n$ observations:

- Kidney Cancer rates, with priors chosen in accordance to the sample size (BDA3). [An R visualization](https://robinryder.wordpress.com/2019/09/13/reproducing-the-kidney-cancer-example-from-bda/)
- [Guns and suicides](https://sidravi1.github.io/blog/2018/06/15/empirical-and-hierarchical-bayes), with ideas from empirical and hierarchical Bayes.
- Richard McElreath's example of Starbucks coffee-shops queue lengths
:::

::: {.callout-tip}
## Estimating Customer Lifetime Value

- Model: Combining Gamma-Poisson and Beta-Binomial, with parameters at customer level
- The math and [code in pymc3](https://sidravi1.github.io/blog/2018/07/08/fader-hardie-clv)

:::


::: {.callout-note}
## Hierarchical Beta-Binomial
 
- [baseball batting](https://bambinos.github.io/bambi/notebooks/hierarchical_binomial_bambi.html), from Eric Ma tutorial in pycon, and the equivalent [numpyro code](https://num.pyro.ai/en/stable/examples/baseball.html)
- Police shooting training, race, [bambi](https://bambinos.github.io/bambi/notebooks/shooter_crossed_random_ANOVA.html) -- full bayesian workflow
- Hierarchical baseball, efron, [pymc](https://www.pymc.io/projects/examples/en/latest/case_studies/hierarchical_partial_pooling.html), idea of partial pooling
- Rat tumors, [pymc](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-hierarchical-binomial-model.html), binomial
:::

## A/B Testing and Experiment Design


Once you have a good grip on those statistical fundamentals, which are like cogs in the machinery of statistics, you can apply them to hypothesis testing and experiment design.

I'm emphasizing again the understanding and letting go of mechanical application of procedures and conventions (p-values, $\alpha, \beta$, statistical tests). You have to be able to justify all the choices you make during the phase of experiment design, that is before running the experiment.

- Understanding the philosophy of falsification and how it applies to hypothesis testing. [Week2 of this course](https://www.coursera.org/learn/improving-statistical-questions/lecture/j6Duu/lecture-2-1-falsifying-predictions-in-theory) has a great 20 minute explanation.
- Before jumping into the hypothesis testing, we should carefully ask whether we need an experiment at all. [Here](https://www.coursera.org/learn/improving-statistical-questions/lecture/uHfmn/lecture-1-2-do-you-really-want-to-test-a-hypothesis) you can see the reasoning for testing and [an article by Cassie](https://towardsdatascience.com/whats-the-point-of-statistics-8163635da56c) for statistics.
- Remember the importance of those 12 steps of statistics, especially the [Default Action](https://towardsdatascience.com/the-most-important-idea-in-statistics-8c18d514ad1c). For a more classical exposition - check [this](https://statsthinking21.github.io/statsthinking21-core-site/hypothesis-testing.html) out.
- Confidence Intervals - first check out this [simulation](https://rpsychologist.com/d3/ci/).  Also [chapter 12](https://openintro-ims.netlify.app/foundations-bootstrapping.html), uses bootstrap to estimate those.
- p-values - [simulation](https://rpsychologist.com/pvalue/), [simulation of distirbutions under H0/Ha](https://rpsychologist.com/d3/pdist/)
- Type 1, 2 errors, Type 3 errors (solving the wrong problem), [chapter14](https://openintro-ims.netlify.app/decerr.html)
- A/B Testing scheme, [An end-to-end example](https://towardsdatascience.com/simple-and-complet-guide-to-a-b-testing-c34154d0ce5a), but be careful to follow the 12 steps we have and not to forget to define the Default Action.
- Experiment Design and Hypothesis testing pitfalls - [from HBR](https://hbr.org/2020/03/avoid-the-pitfalls-of-a-b-testing), [8 pitfalls](https://towardsdatascience.com/online-controlled-experiment-8-common-pitfalls-and-solutions-ea4488e5a82e), [A/A tests](https://towardsdatascience.com/an-a-b-test-loses-its-luster-if-a-a-tests-fail-2dd11fa6d241), [user interference](https://towardsdatascience.com/how-user-interference-may-mess-up-your-a-b-tests-f29abfcfccf8)
- Metric Design and [properties of good metrics](https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/stedii-properties-of-a-good-metric/) - [an example](https://towardsdatascience.com/a-guide-for-selecting-an-appropriate-metric-for-your-a-b-test-9068cccb7fb)
- Relevance and Significance - Read [this paper](https://stat.ethz.ch/~stahel/relevance/stahel-relevance2103.pdf) by Werner Stahel
- Non-Inferiority testing - [This visualization](https://rpsychologist.com/d3/equivalence/)
- Effect size, Power, Sample Size - [here](https://mathstat.slu.edu/~speegle/_book/HTCI.html), and [here](https://www.huber.embl.de/msmb/06-chap.html) and [here](https://statsthinking21.github.io/statsthinking21-core-site/ci-effect-size-power.html). [Cohen's d](https://rpsychologist.com/cohend/), [Power Analysis](https://rpsychologist.com/d3/nhst/)
- Philosophy of science: Popper and Latakos, in this [lecture](https://www.youtube.com/watch?v=cgvKG_3Ck7Y)



## Gaussian Linear Regression

::: {.callout-note}
## Examples. Introduction to the workflow

- [bambi](https://bambinos.github.io/bambi/notebooks/ESCS_multiple_regression.html), Eugene-Springfield community sample data: OCEAN as related to drugs -- no categorical variables, y distribution normal-ish with a quirk
- [pymc](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-1-educational-outcomes-for-hearing-impaired-children) - Educational outcomes for hearing-impaired children, a nice workflow
- [marriages and waffles](https://num.pyro.ai/en/stable/tutorials/bayesian_regression.html) -- numpyro, mcelreath, there is also a pymc version
- [Kentucky derby horse race](https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html) - frequentist version in R
- [pymc](https://www.pymc.io/projects/examples/en/latest/case_studies/moderation_analysis.html) moderation analysis: muscle mass and age
:::

::: {.callout-tip}
## Common statistical tests are linear models

- [pymc](https://www.pymc.io/projects/examples/en/latest/case_studies/BEST.html) - Krutsche fake data drug trial, t-test, [bambi](https://bambinos.github.io/bambi/notebooks/t-test.html). 
- [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/#1_the_simplicity_underlying_common_tests) and the [python port](https://www.georgeho.org/tests-as-linear/) 
:::

::: {.callout-tip}
## Splines and Nonlinear Transformations

- [Splines](https://bayesiancomputationbook.com/markdown/chp_05.html#id36) from Osvaldo, on Bike Ridership (UCI data, bike sharing)
- [Splines](https://www.pymc.io/projects/examples/en/latest/case_studies/spline.html), rethinking, cherry blossoms data
- Log-log, power laws
:::


::: {.callout-note}
## Model Critique and Evaluation

- Technical: prior and posterior checks [pymc docs](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html#posterior-predictive)
- Bootstrap - [chapter 10](https://hastie.su.domains/CASI_files/PDF/casi.pdf) or [page 249](https://hastie.su.domains/Papers/ESLII.pdf)

:::


::: {.callout-note}
## Model Comparison and Selection

- [pymc](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-model-selection.html). - Model selection with fake data and polynomials
- Bias-Variance tradeoff - For an intuitive explanation, watch [lecture 8, slides](https://work.caltech.edu/lectures.html). See how this [tradeoff needs an update](https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update) for the modern machine learning. 

:::

::: {.callout-warning}
## Bad posterior geometry. Reparametrization

- Technical: (mu, sigma) - Neal's Funnel [numpyro](https://num.pyro.ai/en/stable/examples/funnel.html), there is equivalent pymc
- More solutions in numpyro for bad [posterior geometry](https://num.pyro.ai/en/stable/tutorials/bad_posterior_geometry.html)
:::

::: {.callout-note}
## Regularization and Variable Selection

- [Spike and Slab kaggle](https://www.kaggle.com/code/melondonkey/bayesian-spike-and-slab-in-pymc3/notebook)
- [Horseshoe prior](https://austinrochford.com/posts/2021-05-29-horseshoe-pymc3.html) pymc3
- [Robust LR](https://bambinos.github.io/bambi/notebooks/t_regression.html), bambi, simulated data, also is in pymc
:::



::: {.callout-tip}
## Portfolio optimization a la Markowitz

- Technical: Wishart and Portfolios from [BMH](https://nbviewer.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Ch6_Priors_PyMC3.ipynb) -- can also do optimization ala markowitz, but with uncertainty from posteriors
:::

::: {.callout-warning}
## Modeling how Data goes Missing

- Missing data imputation, [pymc](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html), both for linear and hierarchical
- Discrete missing data imputation [numpyro](https://num.pyro.ai/en/stable/tutorials/discrete_imputation.html), with simulated data and causal graphs
- Pymcon - missing data [tutorial](https://gist.github.com/junpenglao/7c505c6c76f99c928a4e2c1161cff43a) in pymc3
- [Missing Data Imputation](http://stronginference.com/missing-data-imputation.html)
:::



## Generalized Linear Models

::: {.callout-note}
## Logistic Regression

- Beetles survival by concentration chemical, logistic regression, [bambi](https://bambinos.github.io/bambi/notebooks/alternative_links_binary.html), tries out different link functions, but example is very nice
- Logistic regression, vote intention by age, clinton/trump/.. [bambi](https://bambinos.github.io/bambi/notebooks/logistic_regression.html), ANES data, 1200 samples
- Influences on income bracket, model comparison, [bambi](https://bambinos.github.io/bambi/notebooks/model_comparison.html)
- Multinomial regression [iris](https://bambinos.github.io/bambi/notebooks/categorical_regression.html), via bambi
:::


::: {.callout-note}
## Poisson Regression

- [Number of laws](https://www.bayesrulesbook.com/chapter-12.html) regarding equality, which includes a discussion for the issue of overdispersion.
- Stop and frisk data, the [frequentist version](https://omarfsosa.github.io/poisson_regression_in_python)
- [Campus crime](https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html) and estimating household size in Philippines.
- [Alcohol and meds interaction](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-poisson-regression.html), with simulated data
:::

::: {.callout-note}
## Overdispersion. Negative Binomial. Zero-Inflation

- [Cockroaches and pest management](https://avehtari.github.io/ROS-Examples/Roaches/roaches.html), where Negative-Binomial, Poisson and Zero-Inflated NBD is investigated.
- [Fishing catches in a park](https://num.pyro.ai/en/stable/examples/zero_inflated_poisson.html), in numpyro
- [Students' absence](https://bambinos.github.io/bambi/notebooks/negative_binomial.html), UCLA data, application of negative binomial, written in bambi
:::

::: {.callout-note}
## Proportions. Compositional Data Analysis

[Dirichlet regression](https://joshuacook.netlify.app/post/dirichlet-regression-pymc/), pymc3 - fake proportions dataset, but take some real ones from compositional data analysis books

:::




## Hierarchical GLMs

::: {.callout-note}
## Hierarchical Gaussian Regression

- Radon: Primary code reference: [pymc](https://www.pymc.io/projects/examples/en/latest/case_studies/multilevel_modeling.html) - A Primer on Bayesian Methods for Multilevel Modeling
	- [bambinos](https://bambinos.github.io/bambi/notebooks/radon_example.html) a higher level API, models the log-radon
	- [Omar Sosa - Practical introduction to Bayesian hierarchical modeling](https://github.com/omarfsosa/tech-talk-hierarchical-models) with numpyro
	- [McStanPy](https://mc-stan.org/users/documentation/case-studies/radon_cmdstanpy_plotnine.html) implementation
- [Bayesian Multilevel Regression](https://num.pyro.ai/en/stable/tutorials/bayesian_hierarchical_linear_regression.html) numpyro, linear regression [OSIC Pulmonary Fibrosis Progression](https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression)
- BeyondMLR [stage anxiety music performers](https://bookdown.org/roback/bookdown-BeyondMLR/ch-multilevelintro.html#cs:music)
- Stack facial feedback hypothesis, re-analysis, ctx replication crisis, via [bambi](https://bambinos.github.io/bambi/notebooks/Strack_RRR_re_analysis.html) -- full workflow, with iteration
:::

::: {.callout-note}
## Longitudinal Data and Studies
- [pymc](https://www.pymc.io/projects/examples/en/latest/case_studies/longitudinal_models.html) longitudinal data with drinking teens, alcohol consumption per cohorts, in time
- [Sleepstudy and reaction times](https://bambinos.github.io/bambi/notebooks/sleepstudy.html) bambi, by subject
	- Same idea in [pig growth study](https://bambinos.github.io/bambi/notebooks/multi-level_regression.html)
- Beyond MLR [charter schools longitudinal](https://bookdown.org/roback/bookdown-BeyondMLR/ch-lon.html#cs:charter)
:::

::: {.callout-note}
## Hierarchical Logistic Regression

- Beyond MLR logistic, college basketball referee foul differential [here](https://bookdown.org/roback/bookdown-BeyondMLR/ch-GLMM.html#cs:refs)
- [Graduate Admissions](https://num.pyro.ai/en/stable/examples/ucbadmit.html), from McElreath,  UC Berkeley in Fall 1973 (numpyro)
	- point to pymc port of rethinking
- item-response nba fouls rasch (pymc) with nba data, [pymc](https://www.pymc.io/projects/examples/en/latest/case_studies/item_response_nba.html)
:::


::: {.callout-note}
## Hierarchical Poisson Regression

- [Airbnb number of reviews](https://www.bayesrulesbook.com/chapter-18.html#hierarchical-poisson-negative-binomial-regression)
- Estimating the strength of a [rugby team](https://www.pymc.io/projects/examples/en/latest/case_studies/rugby_analytics.html)
- [Paper investigating seat-belt use rates](https://onlinelibrary.wiley.com/doi/full/10.1002/sta4.544), with data probably taken from the department of transportation [crashes website](https://crashstats.nhtsa.dot.gov/#!/)
:::

::: {.callout-note}
## Models with 3 Levels

- Beyond MLR [3-level seed germination](https://bookdown.org/roback/bookdown-BeyondMLR/ch-3level.html#cs:seeds)
:::


## Bayesian Machine Learning

::: {.callout-note}
## Bayesian Additive Regression Trees

- [BART](https://bayesiancomputationbook.com/markdown/chp_07.html) from osvaldo, on bike shares
- [Podcast Episode](https://learnbayesstats.com/episode/80-bayesian-additive-regression-trees-sameer-deshpande/) and an [R package](https://github.com/skdeshpande91/flexBART in R)
- Nonparametric methods -  what is the benefit, name a few equivalents. Be able to explain a signed-rank transformation. [This](https://steverxd.github.io/Stat_tests/) is the simplest and the most accessible explanation I know of so far.

:::


::: {.callout-note}
## Gaussian Processes

- Gelman: [Birthdays](https://omarfsosa.github.io/speedy_gaussian_processes), hilbert space approximation [repo](https://github.com/omarfsosa/hsgp), also in [stan](https://avehtari.github.io/casestudies/Birthdays/birthdays.html)

:::


## Datasets for Machine Learning


## Bibliography

```yml
- title : Bayesian Data Analysis
  title_short: bda3
  type: book
  edition: 3
  author: Andrew Gelman
  year: 2013
  link: http://www.stat.columbia.edu/~gelman/book/
  lectures: https://avehtari.github.io/BDA_course_Aalto/Aalto2022.html

- title: Bayesian Methods for Hackers
  title_short: bmh
  type: book
  edition: 1
  author: Cameron Davidson
  year: 2015
  link: https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/
  github: https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers

- title: Statistical Rethinking
  title_short: rethinking
  type: book
  edition: 2
  author: Richard McElreath
  year: 2021
  link: https://xcelab.net/rm/statistical-rethinking/
  lectures: https://github.com/rmcelreath/stat_rethinking_2023

- title: The Most Dangerous Equation
  title_short: danger-eqn
  type: article
  author: Howard Wainer
  year: 2009
  link: http://assets.press.princeton.edu/chapters/s8863.pdf

- title : Introduction to Probability
  title_short: probability-blitzstein
  type: book
  edition: 2
  author: Joe Blitzstein
  year: 2019
  link: https://projects.iq.harvard.edu/stat110/home
  lectures: https://projects.iq.harvard.edu/stat110/youtube
```