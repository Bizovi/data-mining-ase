---
title: "Are you ready for an adventure?"
author:
  - name: Mihai Bizovi
    id: mb
    degrees: 
    - "VP of Decision Science"
format:
  html:
    toc-location: left
lang: en
---


There is one question I lose sleep over: "How should data science be taught?" The vision and roadmap outlined on this website emerges out of:

::: {.column-margin}
As the author of the course (book) -- all views and mistakes are entirely my own and do not represent any organization. 
:::


- my industry experience as a data scientist, ML engineer, then leader 
- teaching to creative, motivated, but confused students
- my frustrations about the shortcomings of statistical education

I aspire to contribute to the understanding of this complex landscape and teach people how to navigate it, how to develop **valuable skills**, and become more **effective at problem-solving**. To achieve this, we need to master "[the fundamentals]{.underline}."

I believe that improving **decision-making** is the reason why we practice "AI".^[We will have to clarify all the loaded terms like AI, Cybernetics, Data Science, Analytics, ML, and Data Mining] The danger when a program lacks unified vision and a coordinated roadmap, is that we miss the interdisciplinary, big picture. Local optimization, global valley. We forget the why.

The commitment needed to get started in the field is so high, that a student is guaranteed to get lost in details, be overwhelmed by all those 800 page hardcore textbooks. On the other hand, seemingly pragmatic, cookbook and bootcamp-style approaches miss depth, nuance, key theoretical ideas, and methodological aspects. 


## Decision-Making for the Brave and True {.unnumbered}

This is an introductory, free, and open-source course which bridges the gap between theory and practice,^[There is a similar gap between the "math world" of elegant models / abstractions and messy "real world" of decisions in businesses
] cultivating the skills and understanding necessary to bring value to organisations by **improving decision-making**. It is an attempt to find a golden mean of both worlds -- so that you choose your own path intentionally. Here's the metaphor:

-   Illuminating theoretical ideas (contemplating in the library)
-   Practicing battle-tested technologies (engineering in the trenches).


::: {.callout-tip}
## Think of youself as a business person with superpowers

This is the **best advice** I ever got as a novice data scientist. It is easy to lose track of this idea, especially if inamored with the technicalities of statistics and machine learning.
:::

This is the **course I wish I had** when starting my journey in data science, which would prepare me for the realities of industry, often very different from the academic world. In my teaching, I achieve this by putting business decisions, understanding the domain, and working software at the forefront of each (statistical) tool we learn. 

::: {.callout-note}
## Who should read this book?

*Anyone lost, confused, stuck or overwhelmed by Data Science and Machine Learning complexities, who wants to see the big picture, a path forward, and the possibilities.*

If you stumbled upon this website, you're probably a student in Business Analytics, or know me personally -- well, because I shamelessly promoted it.

Maybe, you're an **engineer** getting curious about ML or an **analyst** with a knack for the business, looking to improve your workflow and expand the quantitative toolbox. Maybe you're a **product manager** or an **entrepreneur** who wants to infuse AI into your startup.

In my opinion, junior data scientists and ML practitioners a few years into their journey will benefit the most from the re-contextualization of fundamentals that I'm doing here, which could enable them to take another **leap in career**.
:::


The course is challenging due to its breadth across disciplines and depth (starting from fundamentals).^[Of course, you can just listen, read and extract valuable insights, but the course ultimately fails without deep self-study.] In service of understanding, it becomes conceptual at times, but I hope you bear with me until you see the benefits of those abstractions. Keep in mind that:

- It is NOT a theoretical course, nor a replacement for probability, mathematical statistics, and other prerequisites. There are two possibilities:
  - You already had those courses and will look at them with a fresh perspective 
  - Otherwise, treat it as learning roadmap of the most relevant ideas
- At the same time, this is NOT a bootcamp, nor a replacement for learning programming in Python/R/SQL for data science
  - Therefore, it is not enough by itself to land you a job
  - For that, there are many wonderful resources for practice and study

Think of it as a skeleton, a **conceptual frame** which ties together everything you have learned so far and can be built upon as you progress in your career and studies. You will probably go back to the same idea years later, with greater wisdom and skill -- to unlock its real power. I think that we should embrace the fact that learning is not linear. 

::: {.column-margin}
This course stands on the shoulders of giants, and I can only aspire to get to the level of clarity and rigor provided by [Hastie/Tibshirani](https://www.statlearning.com/) or [Andrew Ng](https://www.andrewng.org/courses/) when it comes to ML, and [Richard McElreath](https://xcelab.net/rm/statistical-rethinking/) or [Andrew Gelman](http://www.stat.columbia.edu/~gelman/book/) on Bayesian Statistics / Causal Inference.
:::


::: {.callout-warning}
## Overwhelming amount of content. What is best for learning?

Given the over-abundance of learning resources, it is too much to sift through a hundred pages of Coursera in the data science section, books, and tutorials to find optimal ones. There is a lot of great content, but it can be hard to decide where to start and our time is very limited. Even worse, poor quality resources can do more damage than good. 

The roadmap provided here should help you navigate the landscape of data science and find the shortest path towards developing your skills and better decisions in your firm. Pick a topic you're passionate about, for example, generalized linear models and look in the study guide for tips on where to read and how to practice.
:::

I have to clarify what I mean by contemplating in the library and  engineering in the trenches. In contemplation, we try to see how fundamental theoretical ideas translate into real-world stories and solutions to practical problems. We also focus on understanding the problem space in various domains, our clients' needs and challenges -- which enables us to ask the right questions, formulate the problem correctly, then pick the right model and methodology for the task. Last, but not least, we'll gain insights into pitfalls and common mistakes to avoid.

::: {.column-margin}
During the course, I present stories and toy problems which motivate probability distributions, LLN/CLT, Bayes' rule, sampling, measurement, etc. This is a way to highlight the **practical relevance** of otherwise dry theoretical ideas
:::

On the other hand, engineering in the trenches is a skill. The only way to become better at modeling and data analysis is hands-on practice. It is not sufficient to know about the modeling workflow and methodology. In practice, we have to develop pragmatic solutions and test, implement our ideas in code. If we are to increase our chances of success, we'll also have to follow best practices of reproducible research, automation, and model operationalization.

## Why should you care?

You might've heard that data scientist is the sexiest job of 21st century, that AI is going to take over repetitive jobs, Deep Reinforcement Learning models are beating people at Go, Dota, Chess, solving almost-impossible protein-folding problems. The world is awash in the newest ChatGPT and Midjourney frenzy, with new developments every month and week. 

::: {.column-margin}
![](img/dont.jpg){width="80%"}
:::

::: {.callout-important}
## How do I keep up?

You don't, at least if you want to have a balanced life. That's why I choose to focus on fundamentals which stood the **test of time**, which are anyways a prerequisite before you dive into understanding the technicalities of those cutting-edge models and systems.

You will be surprised **how far can you go with simple, even linear models**. In the end, it is not a competition, nor am I against deep learning: we learn to solve a very different class of problems that businesses encounter.
:::

But what does it actually mean, if we step outside the hype and buzzwords, use a plain language, and apply these ideas in down-to-earth, day-to-day problems and challenges in businesses? It can be a function of decision-making support or the system/product itself, like in the case of Uber, Amazon, Netflix, Spotify, Google and many others. 

Even if you are not a data scientist, you will work with them in one form or another *(Quant, Data Analyst, Business Analyst, ML Engineer, Data/BI Engineer, Decision-Maker, Domain Expert)*. Therefore, you have to understand their language, what are they doing, how to ask and make sure they solve the right problem for you.

In university years, you've probably been tortured by (or enjoyed) linear algebra, mathematical analysis, probability and statistics, operations research, differential equations, mathematical economics and cybernetics, algorithms and data structures, databases, object-oriented programming, econometrics and so on.

::: {.callout-tip}
## Why study all of this?

We live in a volatile, uncertain, complex and ambiguous world,[^index-2] but we still have to make decisions. Those decisions will bring better outcomes if they are informed by understanding the causal processes, driven by evidence and robust predictions. For a more in-depth explanation of the essence of each subject/domain, read [here](01_fundamentals/prerequisites.qmd).

I want you to take away ONE thing, that is "AI" and Data Science in Businesses boils down to: **Decision-Making under Uncertainty at Scale**
:::

[^index-2]: [VUCA: a mental model to better understand the world](https://www.vuca-world.org/where-does-the-term-vuca-come-from/)


> When somebody asks what have you learned in this book and course, I suggest two metaphors: one of simplification and another of seeing relations


::: {layout="[45, -2, 55]"}
![We see Pollock's **messy reality**, which is the data and observations. We want to get to Picasso's bare bones **essence**, for clarity and better decision-making](01_fundamentals/img/logo.jpeg "Reality"){.preview-image width="90%"}

![This is a **big picture** course, which re-contextualizes everything you have learned before, but didn't see how it fits together or how can it be implemented in practice to bring value to organisations, that is: be useful](01_fundamentals/img/elephant-blind.jpeg){width="90%"}
:::


## What will you learn?

I am tempted to say "act rationally" to improve business outcomes *(top line, bottom line, customer satisfaction, efficiency)*. However, what you will learn is much more nuanced than that -- hence, the short answer below which will take a while to unpack.

::: {.callout-important}
## Short version

$$Question \longrightarrow Modeling \longrightarrow Insight \longrightarrow Action \longrightarrow Outcome $$

Necessary, but insufficient fundamentals in **business economics** and **statistical modeling**:

- in order to ask the right questions
- so that we can build custom statistical models 
- which bring insight into consequences of actions and interventions
- so that we're informed which actions that should be taken
- such that a firm can achieve its tactical and strategic objectives

:::


My attempt in organizing the teaching is to split the course into self-contained modules, which can be mixed and matched for diverse audiences and needs. The book will lag years behind my teaching, but the good news is that you can safely use the carefully curated lists of resources and suggested practices.


::: {.callout-tip}
## Go to "Study Guide" pages for practice!

Conceptual understanding by itself is not enough. So, I curated a [list of resources](references.qmd) to practice on interesting case-studies, datasets, which directly apply the models, tools, and methodologies presented. These are written by experts in the field, are usually well thought, easy to follow, reproducible, and highlight important aspects of a problem and model.

Also, keep an eye on the course [github repo](https://github.com/Bizovi/decision-making), in which we'll do some exciting projects (full stack data apps) and investigate common problems/challenges with a fresh perspective.
:::


## Prerequisites and background


I think this course will bring most value to masters' students, professionals, and students in their last year of BSc -- precisely because of presumed exposure or competence in the following subjects:

::: {.column-margin}
![Decision-Making, ML, and Causal Inference is hard. Practice the fundamentals with patience and care, develop competence. Then, a beautiful world will open up to you!](01_fundamentals/img/karate-kid.webp "Practice"){width="90%"}
:::


- For **Linear Algebra** and **Calculus** -- only exposure is needed, but competence and mathematical maturity will help a lot. If you need a refresher, a crash course will do (for example, Grant Sanderson's [*"Essence of LinAlg"*](https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&si=3pvYphbdL56dnYVH))
- For **Probability Theory** -- competence is needed, even though we start from the very beginning (review of combinatorics). I suggest you read along and practice with a more comprehensive resource, like Joe Blitzstein's ["Probability 110"](https://projects.iq.harvard.edu/stat110/home)
- For **Mathematical Statistics** the story is the same as for Probability. You will need at least to be familiar with hypothesis testing and regression. This course can catalyze the process. My favorite resource so far is [Huber's "Modern Statistics for Modern Biology"](https://www.huber.embl.de/msmb/02-chap.html).
- **Python or R programming** for Data Science is **mandatory**, including data wrangling, visualization, scientific computing, databases. I recommend two free books:
  - ["R for Data Science"](https://r4ds.had.co.nz/) by Hadley Wickham 
  - ["Python for Data Analysis"](https://wesmckinney.com/book/) by Wes McKinney.
  - Of course, the more experience you have in one or both, the better
- This is completely optional, but if you have to deal with financial statements at your job or have an interest in finance, I strongly suggest you check out A. Damodaran's [crash course](https://youtube.com/playlist?list=PLUkh9m2BorqmKaLrNBjKtFDhpdFdi8f7C&si=UjEkr1_smeILJ8ZF) on accounting and corporate finance.

Besides the technical prerequisites, there is a list of concepts and ideas from research design, which can be helpful in practice, but are often not part of statistical curriculum. It is worth to invest a bit of time to get familiar with the following:

::: {.column-margin}
Taken in isolation, topics like measurement and sampling are very abstract. Therefore, they have to be integrated into the case-studies whenever they bring added value. This is not easy, so .. work in progress
:::

- **Measurement** is especially relevant and tricky in social science. Sometimes, we do not measure what we think we do (underlying construct). For measuring abstract things like happiness or personality, we need to dive into scales, instrumentation, operationalization, validity, and reliability. 
- **Sampling** isn't limited to simple random samples, as these rarely occur in practice. We need to be aware what tools exist so that we can generalize to the population of interest. For example, weighting and post-stratification.
- **Threats to validity**, including confounds, artifacts, and biases. On the one hand we use statistics as a guardrail against foolishness, on the other hand there a lot of the same problems that can compromise our study.
- **Reproducible research** and literate programming has become a de-facto standard in the data science world. We have amazing tools right now, but it's still not easy to achieve end-to-end reproducibility and automation.
- **Academic writing** is an important skill, even in businesses. Writing is both a way of thinking, problem-solving, and communication. In my opinion, the structure of a scientific paper is very helpful to crystalize your ideas.

Last, but not least, there is value in understanding the **scientific process**, in its broad sense. In section v., I adapt it to the context of data science in businesses and show how most processes and workflows like CRISP-DM,  Tuckey's EDA are a part of it.



## Module 1: Business School for Data Scientists

Remember the advice "think of yourself as a business person with superpowers"? It means that as a data scientist, it's helpful to think question, action, and outcome first -- and only then about models. In other words, we have to deeply understand our client's business domain and **problem space**.  The best way to achieve that, is experience and collaboration on the job. Therefore, my role is to tell as many stories from the trenches, with deep-dives in our case-studies, so that you know what to expect in practice.

::: {.column-margin}
Often, there is a disconnect between business economics, analytics classes and ML / stats / data science. We can't expect them to be coherent by defaut, but we can carefully design those synergies
:::

A good starting point is thinking about the purpose of a business from different perspectives.  Then, identify key decisions that firms have to make across many industries and domains. This exercise will show us how widespread are data science applications and use-cases, even in places we don't suspect. 

At this point it is important to clarify what we mean by AI, ML, analytics, data mining, statistics and when is it appropriate to choose one approach over another. You'll be surprised, but statistics is the hardest to define and often misunderstood, because of its inferential, experimental, and causal facets.

Next, we'll learn a few frameworks for understanding a firm's performance evolution, value chain, strategy and tie them all together in an analyst's workflow. These tools will help us ask good (statistical) questions and increase the chances that our modeling efforts result in actionable insights.  

::: {.column-margin} 
In time, I'll add many more case-studies from advertisement, promotion, merchandising, engagement, CRM, conversion rate optimization, demand planning, supply chain, etc.
:::

An essential aid in this pursuit of improving decision-making at scale, are different processes and **methodologies** for machine learning, experiment design, causal inference, and exploratory data analysis. It is important to mention that methodology is not a recipe, but way of structuring our work, a set of guiding principles and constraints over the space of all the things that we could do in analysis. Don't think of these constraints as limiting your freedom, but as helpful allies in effective problem problem solving.

> "What is methodology good for? Sounds like I could use that time to learn ML"

These methodolgical fundamentals are not "just theory", it is what will make or break projects in practice. There are so many pitfalls in ML and statistics that we cannot afford to do it ad-hoc.

::: {.callout-note}
## Newsvendor problem, Demand planning, Price optimization

If you worry that the business school is too conceptual -- don't despair, as we'll have a few hands-on, technical lectures -- involving math, modeling, and programming. 

- I use the newsvendor problem as a starting point in modeling for demand planning
- The pricing optimization as a starting point towards revenue management
:::



## Module 2: Probability and Statistics

I think that most students will benefit from re-framing the fundamentals of probability and statistics. A lot of mistakes in practice come from a fundamental misunderstanding of the nature of statistical inference. If we view statistics as **changing our minds and actions in the face of evidence** -- the good ol' t-test will shine in a new light. It will become clear why those models and procedures were invented in the first place.

::: {.column-margin}
Another benefit of revising these fundamentals is that we can develop an appreciation for the stories and ideas behind these methods. Also, it can be fun.
:::

We start with combinatorics and urn models, probability trees, distributions, conditioning, laws of large numbers, central limit theorem, Bayes' rule. For each one of these topics, I present the story, use-case, and hands-on simulations to show that even very simple tools can be useful.

Perhaps the most important lecture is also very abstract, where I formally define what is probability, a random variable, what is the key difference between probability and statistics. Understanding what is collectivity, statistical population, sample; estimands, estimators, estimates; and formally, what is a statistical model -- will give you the right terminology and set you on a good path towards more advanced models.

The module culminates in practical aspects of experiment design and A/B testing, sample size justification, **power analysis**, and a conversation about the causes and remedies to the replication crisis. After all this effort put into the study of fundamentals, you can go with ease and confidence into Bayesian Statistics, Machine Learning, and Causal Inference.


::: {.callout-note}
## The art of formulating a hypothesis

In statistics' classes, the problem is usually completely framed and students focus on computation / interpretation. Experiment design in practice is tricky and more complicated than it looks.
:::


My preferred sequence is to go in parallel with the "business school" and stats, since there is a great synergy. The prior focuses on the **process** (over content) of analytics, ML, statistics. The prior focuses on the **problem** and not so much on solutions.



## Module 3: Applied Bayesian Statistics

Once we have a confident grasp of the fundamentals, we continue on the path of **Applied Bayesian Statistics**. It is an extremely flexible and "composable" approach to building explainable models of increasing complexity and realism. 

::: {.column-margin}
![*(Source: [R. McElreath](https://youtu.be/KNPYUVmY3NM) -- Science before statistics)*](01_fundamentals/img/meme-causal.jpeg){width="90%"}
:::

Instead of applying an out-of-the-box model, we will build custom ones for each individual application in an iterative way. Choosing appropriate probability distributions and modeling assumptions will be critical in this process, along with model critique and validation.

::: {.callout-tip}
## Hierarchical Generalized Linear Models

Many experts in the field argue that this should be the default way we do regression analysis and that we need a very strong justification to show that a linear regression is the appropriate model.


To put it bluntly, this modeling approach will enable us to improve on most challenges in decision-making and inference that businesses face. 
:::


One notable difference between the Bayesian approach and the traditional way advanced econometrics is taught, is that we will focus on computation instead of proofs and heavy-duty mathematical statistics. We declare the model and the sampler does the job for us! If it explodes, we probably modeled it wrong.


### Notes about Causal Inference and ML

Did you get comfortable with building custom statistical models for inference and prediction? What about decisions with high stakes, where we sometimes want to do **randomized experiments**?


Often, A/B tests and randomised experiments are **unfeasible** or **unethical**. Also realize that we cannot reach a causal conclusion from observational data alone -- we can talk just about **associations**. We need a theory, which is our understanding of how the "world" works -- translated into a statistical model, plus data, which will give us new insight into the causal processes (the evidence).

This motivates a deep-dive into the field of Causal Inference -- a link between the theoretical models and observational data, where we sometimes can take advantage of certain "natural experiments". Causal inference requires deep thinking and understanding, which is truly challenging -- an art and science, in contrast with the "auto-magic", unreasonably effective pattern recognition of ML. 


::: {.callout-important}
## What if I care only about ML?

Even if you're interested only in machine learning, most practitioners will emphasize the importance of mastering regression (often starting with generalized linear models) and doing A/B tests resulting in evidence that our new model brings an improvement (or not). 

This is how we jump through various buckets, highlighting the golden thread linking them all: decisions and uncertainty. Moreover, the tools we learned in Bayesian Statistics are directly applicable in ML -- the lines between these two fields are indeed very blurry.
:::


::: {.column-margin}

![Navigate the complexities of the field and choose your path & adventure. *(Source: Generated with DALL-E)*](img/dalle_garden_forking_paths.png) 

:::

Often, we care not just about a single decision or developing better policies, but we have to make tons of little decisions at scale. This is when we switch to a predictive, Machine Learning perspective and walk through our workhorse models, which should serve us decades ahead in a wide range of applications: both predictive and exploratory.

The icing on the cake is miscellaneous topics dear to me and usually not covered in such a course: Demand Forecasting, Recommender Systems, and Natural Language Processing. All extremely useful in business contexts, but significant tweaks are needed to the models discussed before.


### Full-Stack Data Apps

I mentioned before that a key prerequisite is competence in Python/R, SQL, data wrangling, EDA, visualization, simulation, literate programming. There are many wonderful resources to learn and practice these skills and topics. During the labs we'll build from the ground up a tech stack for reproducible data analysis, model and data pipelines.

::: {.callout-tip}
## Project: 5-page applied research paper

After finishing the first three modules, a good chance to put the knowledge in practice, is to write a 5-page research paper on a question / problem you're passionate about.^[In the labs, I give detailed instructions on how a good project should look like, starting from structure and ending in presentation] We will see how easy is it to publish your article / paper / report with quarto and github pages.
:::

If you got to this point and want to continue, you will probably move onto Machine Learning and Causal Inference modules. There is, however, one huge problem when it comes to bringing added value in businesses -- [operationalizing models]{.underline}. 

I have been speaking at conferences about this topic for 6 years: from data, MLOps, product management, and organizational perspectives -- and it's still very relevant. Of course, the way we operationalize a model and build a system depends on the use-case and many factors -- however, the most general case to me is via a "Full Stack Data-Driven App" (with user interface, backend, database; data, training, and serving pipelines).



::: {.callout-tip}
## Build an impressive project for your portfolio

A full-stack data-driven app is your final project (for year two course) and something you can brag about in your portfolio and github profile. It sounds complicated, but we have the tools to make it easy for us. 

Don't worry about getting everything right, but focus on a problem and single area from the course you're passionate about: be it data visualization, ML, statistics or sheer engineering.

:::
