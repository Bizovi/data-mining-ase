<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Pragmatic Data Science – stat_foundations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../01_fundamentals/bolt.html" rel="next">
<link href="../01_fundamentals/background.html" rel="prev">
<link href="../img/favicon.jpeg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TNKQREJHQW"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-TNKQREJHQW', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean"
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Pragmatic Data Science</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Course Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-the-fundamenals" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">The fundamenals</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-the-fundamenals">    
        <li>
    <a class="dropdown-item" href="../01_fundamentals/background.html">
 <span class="dropdown-text">Data Science in Business Context</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../01_fundamentals/stat_foundations.html">
 <span class="dropdown-text">Statistical foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../01_fundamentals/bolt.html">
 <span class="dropdown-text">Case Study: Decisions at Bolt and LRB</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../01_fundamentals/adoreme.html">
 <span class="dropdown-text">Case Study: AI at AdoreMe (e-commerce)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../01_fundamentals/lab_01.html">
 <span class="dropdown-text">Lab: Recap of tidyverse and stats</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-causal-inference" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Causal Inference</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-causal-inference">    
        <li>
    <a class="dropdown-item" href="../02_causality/background.html">
 <span class="dropdown-text">Experiment Design and A/B Testing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../02_causality/power.html">
 <span class="dropdown-text">Advanced Topics in Experiment Design</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../02_causality/lab.html">
 <span class="dropdown-text">Lab: Deep Dive into Hypothesis testing</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-machine-learning" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Machine Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-machine-learning">    
        <li>
    <a class="dropdown-item" href="../03_ml/what_is_learning.html">
 <span class="dropdown-text">What is machine learning</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-engineering" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Engineering</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-engineering">    
        <li>
    <a class="dropdown-item" href="../04_engineering/happy_clients.html">
 <span class="dropdown-text">How can we keep our clients happy?</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/bizovi/data-mining-ase"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/bizovi-mihai-56982abb/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://blog.economic-cybernetics.com/"><i class="bi bi-journal-bookmark" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01_fundamentals/background.html" class="sidebar-item-text sidebar-link">Data Science in Business Context</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01_fundamentals/stat_foundations.html" class="sidebar-item-text sidebar-link active">Statistical foundations</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01_fundamentals/bolt.html" class="sidebar-item-text sidebar-link">Case Study: Decisions at Bolt and LRB</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01_fundamentals/adoreme.html" class="sidebar-item-text sidebar-link">Case Study: AI at AdoreMe (e-commerce)</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01_fundamentals/lab_01.html" class="sidebar-item-text sidebar-link">Lab: Recap of tidyverse and stats</a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#statistical-foundations" id="toc-statistical-foundations" class="nav-link active" data-scroll-target="#statistical-foundations">Statistical Foundations</a>
  <ul class="collapse">
  <li><a href="#the-garden-of-forking-paths" id="toc-the-garden-of-forking-paths" class="nav-link" data-scroll-target="#the-garden-of-forking-paths">The garden of forking paths</a></li>
  <li><a href="#the-probability-triple" id="toc-the-probability-triple" class="nav-link" data-scroll-target="#the-probability-triple">The probability triple</a>
  <ul class="collapse">
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables">Random Variables</a></li>
  <li><a href="#joint-conditional-and-marginal-distributions" id="toc-joint-conditional-and-marginal-distributions" class="nav-link" data-scroll-target="#joint-conditional-and-marginal-distributions">Joint, Conditional, and Marginal Distributions</a></li>
  <li><a href="#bayes-rule" id="toc-bayes-rule" class="nav-link" data-scroll-target="#bayes-rule">Bayes Rule</a></li>
  </ul></li>
  <li><a href="#mathematical-statistics-in-a-nutshell" id="toc-mathematical-statistics-in-a-nutshell" class="nav-link" data-scroll-target="#mathematical-statistics-in-a-nutshell">Mathematical Statistics in a nutshell</a>
  <ul class="collapse">
  <li><a href="#models-are-golemns-of-prague" id="toc-models-are-golemns-of-prague" class="nav-link" data-scroll-target="#models-are-golemns-of-prague">Models are Golemns of Prague</a></li>
  <li><a href="#model-specification-fisher-vs-bayes" id="toc-model-specification-fisher-vs-bayes" class="nav-link" data-scroll-target="#model-specification-fisher-vs-bayes">Model Specification: Fisher vs Bayes</a></li>
  <li><a href="#independence-and-exchangeability" id="toc-independence-and-exchangeability" class="nav-link" data-scroll-target="#independence-and-exchangeability">Independence and Exchangeability</a></li>
  <li><a href="#storytelling-with-dags" id="toc-storytelling-with-dags" class="nav-link" data-scroll-target="#storytelling-with-dags">Storytelling with DAGs</a></li>
  </ul></li>
  <li><a href="#what-does-a-statistician-want" id="toc-what-does-a-statistician-want" class="nav-link" data-scroll-target="#what-does-a-statistician-want">What does a statistician want?</a>
  <ul class="collapse">
  <li><a href="#estimator-properties" id="toc-estimator-properties" class="nav-link" data-scroll-target="#estimator-properties">Estimator Properties</a></li>
  <li><a href="#bias-variance-decomposition" id="toc-bias-variance-decomposition" class="nav-link" data-scroll-target="#bias-variance-decomposition">Bias-Variance Decomposition</a></li>
  <li><a href="#rao-cramer-inequality" id="toc-rao-cramer-inequality" class="nav-link" data-scroll-target="#rao-cramer-inequality">Rao-Cramer Inequality</a></li>
  </ul></li>
  <li><a href="#case-studies" id="toc-case-studies" class="nav-link" data-scroll-target="#case-studies">Case Studies</a>
  <ul class="collapse">
  <li><a href="#de-moivre-on-us-schooling" id="toc-de-moivre-on-us-schooling" class="nav-link" data-scroll-target="#de-moivre-on-us-schooling">De Moivre on US Schooling</a></li>
  <li><a href="#chess-example" id="toc-chess-example" class="nav-link" data-scroll-target="#chess-example">Chess example</a></li>
  <li><a href="#amazon-reviews-and-ratings" id="toc-amazon-reviews-and-ratings" class="nav-link" data-scroll-target="#amazon-reviews-and-ratings">Amazon reviews and ratings</a></li>
  </ul></li>
  <li><a href="#data-mining-and-crisp-dm-variants" id="toc-data-mining-and-crisp-dm-variants" class="nav-link" data-scroll-target="#data-mining-and-crisp-dm-variants">Data Mining and CRISP-DM variants</a>
  <ul class="collapse">
  <li><a href="#ml-is-a-geocentric-model-of-the-universe" id="toc-ml-is-a-geocentric-model-of-the-universe" class="nav-link" data-scroll-target="#ml-is-a-geocentric-model-of-the-universe">ML is a geocentric model of the universe</a></li>
  </ul></li>
  <li><a href="#statistical-process-and-science" id="toc-statistical-process-and-science" class="nav-link" data-scroll-target="#statistical-process-and-science">Statistical Process and Science</a>
  <ul class="collapse">
  <li><a href="#dead-salmon-a-warning-tale" id="toc-dead-salmon-a-warning-tale" class="nav-link" data-scroll-target="#dead-salmon-a-warning-tale">Dead salmon: a warning tale</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content page-columns page-full column-body" id="quarto-document-content"><p>[Mihai Bizovi | Head of Data Science @AdoreMe]</p>



<section id="statistical-foundations" class="level1 page-columns page-full">
<h1>Statistical Foundations</h1>
<p>In order to be a successful Data Scientist, one has to speak the language of probability and statistics. It is the foundation on which we can build towards more realistic and avanced models, with the purpose of improving <strong>decision-making under uncertainty</strong>. This foundation is a prerequisite for all three perspectives: analytics/mining, machine learning and causal inference.</p>
<blockquote class="blockquote">
<p>In order to build adequate models of economic and other complex phenomena, we have to take into account their inherent <em>stochastic nature</em>.</p>
</blockquote>
<div class="page-columns page-full"><p>Data is just the appearance, an external manifestation of some <code>latent processes</code> (seen as random mechanisms). Even though we won’t know the exact outcome for sure, we can model general regularities and relationships as a result of the large scale of phenomena. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Think of <strong>measurements</strong> taken from the Large Hadron Collider experiments: the volume, noise, and complexity of data. Ultimately, we want to make inferences about the <strong>underlying</strong>, physical processes which govern the behavior and interaction of particles. How do we know that inference is accurate and true? A necessary, but not sufficient condition is that the conditions of that experiment are repeatable – such that “on average”, a pattern emerges.</p></li></div></div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
A word of encouragement
</div>
</div>
<div class="callout-body-container callout-body">
<p>Reviewing the fundamentals of statistics doesn’t have to be boring! We can put “the classics” in context of modern statistics, big data challenges, and use simulation instead of heavy mathematics and proof-based approaches.</p>
<p>Moreover, theoretical ideas underlying statistical practice, which we often take for granted, deserve an explicit articulation. This will improve our awareness, understanding, and grasp of the field – such that we can become more effective practitioners.</p>
</div>
</div>
<div class="cell" data-fig-width="9">
<div class="cell-output-display">
<div id="fig-mermaid" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-1">flowchart TD
    Motiv[Why again?] --&gt; ProbT[Probability Triple] --&gt; SI[Uncertainty]
    SI --&gt; MS[Mathematical Statistics] --&gt; GP[Prague Golemn]

    Motiv --&gt; Estim[Estimators] --&gt; Prop[Desired Properties] --&gt; US[Example: US Schools] --&gt; Cond[Conditioning] --&gt; Marg[Marginalisation] --&gt; Ex[Exchangeability]

    Motiv --&gt; DM[Data Mining Process] --&gt; G[Prediction &amp; Geocentrism] --&gt; Sci[Scientific Process]

    Ex --&gt; Dead[Dead Salmon]
    GP --&gt; Dead
    Sci --&gt; Dead

    Dead --&gt; Mod[Modern Stats]
</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: In this chapter and set of lectures, I attempt to reformulate fundamental statistical ideas, concepts, and tools, in order to show their relevance in the day-to-day practice and decision-making. A secondary objective is to fill in the conceptual gaps left by the fact that it’s hard to make sense of it all the first time we’re encountering it in the classroom.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The plan for this lecture is the following: we start by constructing the probability triple, formalize sources of uncertainty, and investigate the building blocks of a statistical model. I then highlight in a few case-studies what can go wrong in the process of statistical inference and how difficult it is to choose the right model.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>How far you go back into the abstract foundations? For practical intents and purposes, you won’t need measure theory and proof-based mathematical analysis.</p>
<p>I find those interesting for their own sake and understanding the foundations of higher-level tools we use. However, I can’t argue it’s an efficient use of your time.</p>
</div></div><p>The most technical, heavy, and mathematical part is about estimators and their properties, but it is necessary both for hypothesis testing and machine learning. To make it more accessible and intuitive, we will use simulation and visualization during the labs to get an understanding and intuition on how estimators behave. We wrap up the lecture by looking at the statistical process in firms and put it in contrast with the process for ML, predictive systems. Last, but not least, there is one more cautionary tale about multiple testing – which will be our gateway into truly modern statistics.</p>
<div class="callout-tip callout callout-style-default callout-captioned page-columns page-full">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Complementary resources if you feel like starting over
</div>
</div>
<div class="callout-body-container callout-body page-columns page-full">
<ul>
<li>Realize that lots of common statistical tests are particular versions of linear models <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. It takes a few hours to go through the theory and the code in the referenced book.</li>
<li>Here’s the approach taken by <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, which integrates data analysis and simulation in learning of statistics.</li>
<li>Upgrade your statistical thinking for the 21st century challenges and be aware of the pitfalls and problems in the field. A good reference is <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, which takes multiple perspectives (frequentist, bayesian, causal inference), while going through the workhorse models and methods.</li>
<li>Be comfortable with exploring, wrangling, visualizing and analyzing data in R or/and Python, get familiar with reproducible research best practices. A good starting point is <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, which is a course in collaboration with the RStudio team.</li>
</ul>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Doogue - <a href="https://steverxd.github.io/Stat_tests/">Common statistical tests are linear models</a></p></li><li id="fn3"><p><sup>3</sup>&nbsp;Speegle, Clair - <a href="https://mathstat.slu.edu/~speegle/_book/preface.html">Probability, Statistics, and Data: A fresh approach using R</a></p></li><li id="fn4"><p><sup>4</sup>&nbsp;Poldrack - <a href="https://statsthinking21.github.io/statsthinking21-core-site/">Statistical Thinking for the 21st Century</a> has two companion books, one with R and another in Python</p></li><li id="fn5"><p><sup>5</sup>&nbsp;Bryan - <a href="https://stat545.com/">STAT 545</a> It is also notable for its focus on teaching using modern R packages, Git and GitHub, its extensive sharing of teaching materials openly online, and its strong emphasis on practical data cleaning, exploration, and visualization skills, rather than algorithms and theory.</p></li></div><p>There are a gazillion books, courses on statistics, which basically do/teach the same thing. For reference and your curiosity, I curated a few which stand out with the right balance of data, code, simulation, theoretical rigor and real-world applications:</p>
<ul>
<li>Cetinkaya-Runde, Hardin - <a href="https://openintro-ims.netlify.app/index.html">Introduction to Modern Statistics</a> goes through all the fundamentals in a clear, concrete, extensive way, with code!</li>
<li>Crump - <a href="https://www.crumplab.com/statistics/">Answering questions with data</a> – introductory statistics for Psychology Students has an interesting approach, focused on the challenges of Psychology</li>
<li>Thulin - <a href="http://www.modernstatisticswithr.com/">Modern Statistics with R</a> goes through the whole process, with R, with additional topics, normally not present in a statistics course</li>
<li>Holmes, Huber - <a href="https://www.huber.embl.de/msmb/index.html">Modern Statistics for Modern Biology</a> is for biologists, but we can see how central to the field are multidimensional methods, clustering and high-performance computing, working with big and messy data</li>
</ul>
</div>
</div>
<section id="the-garden-of-forking-paths" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-garden-of-forking-paths">The garden of forking paths</h2>
<div class="page-columns page-full"><p>As a warm-up, try working out through the following problem, sometimes called “The Birthday Paradox”.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> If outcome space, complementarity, <em>sampling with replacement</em>, <em>pigenhole principle</em> or the simulation code you’re seeing seem puzzling, you might benefit from a refresher on probability theory. If you haven’t seen this problem before, you will probably find the results surprising.</p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;First, an analytical solution on paper, then simulate it in R or Python to check your answer.</p></li></div></div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The Birthday Problem
</div>
</div>
<div class="callout-body-container callout-body">
<p>What is the probability at least two people have the same birthday, in a room of <span class="math inline">\(n=25\)</span> people? Assuming independence, for example no twins, every day is equally likely and ignoring the problem of 29th February. Can you generalize it to an arbitrary <span class="math inline">\(n\)</span>?</p>
<p>For most practical problems, simulation is an extremely important skill. We might start with it to quickly get an answer, however, for simpler problems, the process of working out the solution analytically sheds light on its structure and easily generalizes to other applications. This simulation indeed, converges to the true results, but it takes a lot of computation, so it might not work when the code needs to run really fast!</p>
<div class="cell" data-hash="stat_foundations_cache/html/unnamed-chunk-4_581a09b83394d0719ece659ea8d16001">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>nr_people <span class="ot">&lt;-</span> <span class="dv">25</span> </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>simulate_birthdays <span class="ot">&lt;-</span> <span class="cf">function</span>(nr_people, <span class="at">nr_sim =</span> <span class="dv">10000</span>) {</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    birthday_events <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="at">n =</span> nr_sim, {</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        birthdays <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">365</span>, <span class="at">size =</span> nr_people, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="fu">anyDuplicated</span>(birthdays) <span class="sc">&gt;</span> <span class="dv">0</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(birthday_events)  <span class="co"># this returns implicitly!</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>pr_same_bday <span class="ot">&lt;-</span> <span class="fu">simulate_birthdays</span>(nr_people)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>bday_match_size <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">90</span>, simulate_birthdays, <span class="at">nr_sim =</span> <span class="dv">10000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<!-- <details> <summary>Show the answer</summary> -->
<!-- </details> -->
<p>So, the probability that a room of 25 people has two people with the same birth date is <span class="math inline">\(\approx\)</span> 0.558. For 50 people, it’s around 0.9692</p>
<div class="cell" data-hash="stat_foundations_cache/html/unnamed-chunk-6_6fb70620685be6c0999b14947d82dd0f">
<details>
<summary>Show the visualization code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="co"># Dist' from plot to side of page</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">mgp =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">0.4</span>, <span class="dv">0</span>), <span class="co"># Dist' plot to label</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">las =</span> <span class="dv">1</span>, <span class="co"># Rotate y-axis text</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">tck =</span> <span class="sc">-</span>.<span class="dv">01</span>, <span class="co"># Reduce tick length</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">xaxs =</span> <span class="st">"i"</span>, <span class="at">yaxs =</span> <span class="st">"i"</span>) <span class="co"># Remove plot padding</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">90</span>, <span class="at">y =</span> bday_match_size, </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">5</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>, <span class="co"># Labels</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="co"># Don't plot the axes</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">frame.plot =</span> <span class="cn">FALSE</span>, <span class="co"># Remove the frame</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">80</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="co"># Limits</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">panel.first =</span> <span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.25</span>), <span class="at">col =</span> <span class="st">"grey80"</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="dv">49</span>, bday_match_size[<span class="dv">50</span>], <span class="dv">49</span>, <span class="dv">0</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>at <span class="ot">&lt;-</span> <span class="fu">pretty</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">90</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text =</span> at, <span class="at">at =</span> at,</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">"grey20"</span>, <span class="at">line =</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">0.9</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>at <span class="ot">=</span> <span class="fu">pretty</span>(bday_match_size)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">2</span>, <span class="at">text =</span> at, <span class="at">at =</span> at, <span class="at">col =</span> <span class="st">"grey20"</span>, <span class="at">line =</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div class="cell-output-display callout-margin-content">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="stat_foundations_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Probability of matching birthdays as a function of the number of people</figcaption><p></p>
</figure>
</div>
</div></div><p>How this counter-intuitive statistical “paradox” relates to satellite collisions, DNA evidence and other coincidences</p>
<p>The source of confusion within the Birthday Paradox is that the probability grows relative to the number of possible pairings of people, not just the group’s size. The number of pairings grows with respect to the square of the number of participants, such that a group of 23 people contains 253 (23 x 22 / 2) unique pairs of people.</p>
<p>In each of these pairings, there is a 364/365 chance of having different birthdays, but this needs to happen for every pair for there to be no matching birthdays across the entire group. Therefore the probability of two people having the same birthday in a group of 23 is:</p>
<p>Go to richard McElreath’s garden of forking paths next</p>
</section>
<section id="the-probability-triple" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-probability-triple">The probability triple</h2>
<div class="page-columns page-full"><p>In the <a href="../01_fundamentals/background.html#why-did-you-study-all-of-that">previous lecture</a>, I mentioned why did we study probability theory. However, there is one more useful metaphor: remember how important is logic in mathematics, computer science, and philosophy; it’s one of the prerequisites in each of those, an essential tool for reasoning. Then, probability theory is the <strong>logic of uncertainty</strong>, a formal language. <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;If you studied fuzzy set theory, you might have a case for it being the candidate, however, it fell out of favor in practice – so I would suggest to focus on probability and Bayesian reasoning.</p></li></div></div>
<p>Often, probability and mathematical statistics are bundled together, as they make perfect sense in sequence, but they have different objectives in mind. Probability theory is concerned with the construction of probability triple <span class="math inline">\((\Omega, \mathcal{F}, \mathbb{P})\)</span>.</p>
<p>That is the foundation for developing useful tools like random variables and probability/cumulative density functions. Then, extending those to joint, conditional probabilities and multidimensional cases, introducing the machinery to operate all of that: like expectation, variance, moment-generating functions, conditioning, marginalization, Bayes theorem. This leads to the laws of large numbers, properties of distributions and their transformations.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class=""><strong>We use all of those results</strong> to model relationships between observed and latent variables in business processes and phenomena. It’s a building block for answering the “simple” question: does X cause Y?</span></div></div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
“I haven’t used Poisson outside that probability class”
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you empathise with this statement, you’re probably aware that it’s important, but wonder if it didn’t come up in practice – what about the rest of the machinery I described? My answer is that it’s so, so ubiquitous, but we need to learn to “see” the opportunities to use this set of tools in decision-making.</p>
<p>Poisson distribution and process can be a good choice to model counts of <strong>events per unit of time</strong>, space, with a large number of “trials”, each with a small probability of success.</p>
<p><span class="math display">\[
P(X=k) = \frac{e^{−\lambda} \lambda^k}{k!}; \space k=0, 1, ...
\]</span></p>
<ul>
<li>Arrivals per hour: requests in a call center, arrivals at a restaurant, website visits. We can use it for capacity planning.</li>
<li>Bankrupcies filed per month, mechanical piece failures per week, engine shutdowns, work-related accidents. We can use these insights to assess risk and improve safety.</li>
<li>Forecasting slow-moving items in a retail store, e.g.&nbsp;for clothing. We’ll investigate the <strong>newsvendor problem</strong> in excruciating detail, where we have to purchase the inventories ahead of time.</li>
<li>A famous example is of L. Bortkiewicz: in Prussian army there were 0.70 deaths per one corps per one year caused by horse kicks. <em>(“Law of small numbers”)</em>.</li>
</ul>
<p>Just before you get all excited about these applications, keep in mind that every distribution has a story behind it, and a set of assumptions that have to be met.</p>
</div>
</div>
<p>Other tools are as prevalent and useful: Bayes rule, DAGs (directed acyclic graphs of random variables), the exponential family, laws of large numbers and the cental limit theorem. It holds both in applications and statistics itself.</p>
<p>In probability theory, we’re still in a mathematical world trying to capture the essence of the real world, but ultimately, we need statistical inference to estimate those parameters from data. Before we get into it in more detail: mathematically, computationally and in terms of business cases, we have to define that foundation – the probability triple.</p>
<div class="page-columns page-full"><p>To make the following more clear, let’s start with an experiment where we show a customer ten clothing items (a collection), and they have to pick the one they like best. If we repeated it with many customers, preferably representative for the population of interest,<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> a pattern would emerge. This is an example of a discrete Outcome Set (or universal set, of all possible results). Alternatively, think of pebbles in an urn, where each one represents an outcome.</p><div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;You might intuitively know what a population is, but there are surprisingly many pitfalls, so we’ll investigate that notion with lots of care.</p></li></div></div>
<p>As an academic aside, we have to thank Kolmogorov for putting probability into a rigorous, axiomatic framework based on set theory and making it open for mathematical enquiry with tools from mathematical analysis, which by that time were well-established. That is important, because we can’t work the same naive way with continuous measurements and phenomena, as we do with pebbles.</p>
<ol type="1">
<li>A <strong>random experiment</strong> (<span class="math inline">\(\mathscr{E}\)</span>) is a set of <em>conditions which are favorable for an event</em> in a given form. It is that real-world process of interest which we try to simplify, with the following properties:
<ul>
<li>Possible results and outcomes are known apriori and exhaustably. For example: a coin/dice toss, quantity sold, time in a subscription, a default on the credit, a choice between subscriptions.</li>
<li>It’s never known which of the results of <span class="math inline">\(\mathscr{E}\)</span> will manifest or appear before we run the experiment, the “experiment” amounting to randomly picking that clothing item or a pebble from the urn.</li>
<li>Despite that, <strong>there is a perceptible regularity</strong>, which can be eventually measured and quantified, that is, encoding the idea of a probabilistic “law” in the results. That regularity could be a result of the large scale of the phenomena, for example, many customers seeing a product on the shelf.</li>
<li>Repeatability of the conditions in which the experiment runs, like the comparability and perservation of context. This is optional in the Bayesian perspective, where we’re not thinking in long-run frequency terms. <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></li>
</ul></li>
<li><strong>Elementary event</strong> as an auxiliaty construction: one of the possible results of <span class="math inline">\(\mathscr{E}\)</span>, usually denoted by <span class="math inline">\(\omega_i \in \Omega\)</span>.</li>
<li><strong>Universal set</strong> <span class="math inline">\(\Omega = \{ \omega_1, \omega_2, \dots \}\)</span> is also called (Outcome/ State/ Selection space). It suggests the idea of complementarity and stochasticity: we don’t know which <span class="math inline">\(\omega_i\)</span> will manifest, thus is a key object for a further formalization of probability measures.</li>
<li>We care not only about <strong>an event</strong> <span class="math inline">\(A = \bigcup\limits_{i = 1}^n \omega_i\)</span> (which is an union of elementary events) and its realization, but also about other events in the Universal Set, because they might contribute with additional information (about the probability) of our event of interest – remember conditioning? This means that we we’re interested in “all” other events.</li>
<li>The <strong>event space</strong> <span class="math inline">\(\mathcal{F}\)</span> is a sigma-algebra, should be defined on sets of subsets of <span class="math inline">\(\Omega\)</span> and this is where measure theory shines. For technical reasons, we usually can’t define a probability measure on all sets of subsets. On an intuitive note, we define the probability measure on sigma-algebras because if those conditions did not hold, the measure wouldn’t make sense, unions of events would step out of the bounds of event space.</li>
<li><strong>Probability as an extension of the measure</strong>: chance of events realizing. Note that the perceptible regularity can be thought as the ability to assign a probability (number between 0 and 1) to elementary events: <span class="math inline">\(\mathbb{P}(\omega_i)\)</span>. This is why additivity properties are key, as we care about random events, not only elementary events.</li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn9"><div class="quarto-figure quarto-figure-center"><sup>9</sup>&nbsp;
<figure class="figure">
<p><img src="img/coin-flips-lr.png" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption"><em>Source: <a href="https://bookdown.org/kevin_davisross/probsim-book/interpretations.html">Ross</a></em> This is the view of probability as a long-run frequency of events occuring, for example, flipping a coin</figcaption><p></p>
</figure>
</div>
</li><div class="">
<p><strong>Def: Algebra and Sigma-Algebra</strong></p>
<p>A set of subsets <span class="math inline">\(\mathcal{F} \subset 2^\Omega\)</span> is an algebra (field) if the following holds:</p>
<ol type="1">
<li><span class="math inline">\(\Omega \in \mathcal{F}\)</span> and <span class="math inline">\(\varnothing \in \mathcal{F}\)</span></li>
<li>If <span class="math inline">\(A \in \mathcal{F}\)</span> then <span class="math inline">\(A^C \in \mathcal{F}\)</span> (closed under complements)</li>
<li>If <span class="math inline">\(A, B \in \mathcal{F}\)</span> then <span class="math inline">\(A \cup B \in \mathcal{F}\)</span> (closed under union). Note that 2 and 3 imply that it’s closed under countable intersection</li>
<li>The additional condition for sigma-algebra: <strong>sigma</strong> refers to countability. If <span class="math inline">\(\{ A_i \}_{i \ge 1} \in \mathcal{F}\)</span> then <span class="math inline">\(\bigcup\limits_{i \ge 1} A_i \in \mathcal{F}\)</span> (closed under <strong>countable union</strong>)</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/sigma-algebra.svg" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption"><em>Source: <a href="https://upload.wikimedia.org/wikipedia/commons/d/d3/Sigma_algebra_ellipse_de.svg">Wikimedia</a></em> A beautiful visual representation</figcaption><p></p>
</figure>
</div>
</div></div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Probability Measure
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose we have defined a <strong>measurable space</strong> <span class="math inline">\((\Omega, \mathcal{F})\)</span>, where <span class="math inline">\(\mathcal{F}\)</span> is a sigma-algebra. A <strong>probability measure</strong> is the function <span class="math inline">\(\mathbb{P}:\mathcal{F} \rightarrow [0, 1]\)</span> such that:</p>
<ol type="1">
<li><span class="math inline">\(\mathbb{P}(\Omega) = 1\)</span> </li>
<li>For countable sequences of mutually disjoint effects, i.e.&nbsp;<span class="math inline">\(\forall \{ A_i \}_{i \ge 1}\)</span> where <span class="math inline">\(A_i \bigcap\limits_{i \ne j} A_j = \varnothing\)</span>, the following holds <span class="math inline">\(\mathbb{P}(\bigcup\limits_{i \ge 1} A_i) = \sum\limits_{i \ge 1} \mathbb{P}(A_i)\)</span></li>
</ol>
</div>
</div>
<p>A <strong>probability triple</strong> <span class="math inline">\((\Omega, \mathcal{F}, \mathbb{P})\)</span> is the fundamental object the whole probability theory is constructed upon. Again, Kolmogorov took the informal, gambling-type probability and put it onto axiomatic foundations – which enabled future breakthroughs. Notice that this definition of probability is not the naive one, of number of successes over the total possible numbers an event could arise.</p>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Measure theory rabbit hole: Why not all subsets?
</div>
</div>
<div class="callout-body-container callout-body">
<p>The reasons for this are very technical, and the concept of a sigma-algebra is essential in resolving the resulting paradoxes. If you’re interested in these technical details, you can check out my relatively accessible introudction to <a href="https://blog.economic-cybernetics.com/post/2018-06-01-probabiliy/">measure theory and the Caratheodori extension theorem</a>.</p>

<p>Even though you can go a long way as a practitioner with standard tools in probability theory, deeply understanding its <strong>measure-theoretic</strong> foundations could open up a whole new world to the researcher. It’s easy to take the results from statistics and probability for granted, but it’s useful to be aware what hides beneath the surface.</p>
<p>Evans Lawrence gives the following example of a function which is neither discrete nor continuous, for which you flip a coin and if it comes heads, draw from an uniform distribution and in case of tails a unit mass at one. If <span class="math inline">\(\chi_{[0,1]}(x) = (e^{ix} - 1)/ix\)</span> is the characteristic function of the interval from zero to one, in a way you can formulate its density, but usually it’s not the case, nor is it very helpful to think about it in such terms.</p>
<p><span class="math display">\[\begin{equation}
    p(x) = w_1 \chi_{[0,1]}(x) +  w_2\delta_1(x)
\end{equation}\]</span></p>
<p>Even though you can visualize this in two dimensions as the uniform and a spike, or as a CDF with a discontinuity, this approach just breaks down in higher dimensions or more complicated combinations of functions.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div class="callout-margin-content">
<p>Jeffrey Rosenthal begins his book by a similar motivation, constructing the following random variable as a coin toss between a discrete <span class="math inline">\(X \sim Pois(\lambda)\)</span> and continuous <span class="math inline">\(Y \sim \mathcal{N}(0,1)\)</span> r.v.</p>
<p><span class="math display">\[\begin{equation}
    Z = \begin{cases}
    X, p = 0.5 \\
    Y, p = 0.5
    \end{cases}
\end{equation}\]</span></p>
<p>He then challenges the readers to come up with the expected value <span class="math inline">\(\mathbb{E}[Z^2]\)</span> and asks on what is it defined? It is indeed a hard question.</p>
</div></div><p>We stumble upon different interpretations of probability (frequentists vs bayesians), when trying to clarify that “perceptible regularity”, despite the mathematics of probability theory being exactly the same. These are not “theorems” to prove, but rather axioms and philosophies often taken in practice as the starting point.</p>
<div class="page-columns page-full"><p>The frequency theory defines the probability of an outcome as the limiting frequency with which that outcome appears in a long series of similar events. If our experiment or investigation is such that those relative frequencies converge, then we can prove the LLN and the CLT. Basically, that we should view probability and predictions based on historical data. <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn10"><p><sup>10</sup>&nbsp;<em>“To a frequentist, the probability of an event is intrinsic to that event’s nature and not something perceived, only discovered.”</em></p></li></div></div>
<p>Formally, we can represent this statement by Bernoulli’s theorem, which is a special case of Law of Large Numbers, where <span class="math inline">\(m_n\)</span> is the number of times an event <span class="math inline">\(A\)</span> occurs in <span class="math inline">\(n\)</span> trials.</p>
<p><span class="math display">\[
\lim_{n \to \infty} \frac{m_n}{n} = p
\]</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Frequency Convergence as Statistical Stability
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since we don’t have an infinite number of trials, the best we can do is to say that “experimental evidence strongly suggests statistical stability”. This applies really well in gambling (by design) and physics, but is it plausible in human behavior? I don’t think so, but it might be <strong>unreasonably effective</strong>, even if not true.</p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>So, which one do you recommend? It depends, <strong>learn both</strong>! But I would agree with <a href="https://xcelab.net/rm/statistical-rethinking/">Richard McElreath</a>’s meme: it’s the scientific process that has the last laugh</p>
<p><img src="img/meme-causal.jpeg" class="img-fluid" style="width:80.0%"></p>
</div></div><p>Bayesians view probability as the degree of confidence in a belief, an apriori guess or knowledge, that is, before seeing the data. Then, using inverse probability, you update your beliefs in the face of evidence and data. Often, you do have knowledge and can define your prior probabilities by a process of ellicitation – it should’t be too vague, neither dogmatic. Other Bayesians put in great effort to pick non-informative priors, as the data should quickly overwhelm it.</p>
<p>TODO: Ellicitation of probability -&gt; we’ll do more later with inference</p>
<section id="random-variables" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="random-variables">Random Variables</h3>
<p>Thank you for bearing with me through the theory you have probably seen before, but we’re not done. We’re still in the land of set theory, and it is very hard to operate that way in practice – so, we need a new concept which will allow us to use the tools of mathematical analysis in probability, in order to make it feasible for practical uses.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The breakthrough idea of a Random Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>We started from some phenomena of interest and a random experiment. The random variable is a necessary abstraction in order to mathematically define quantifiable characteristics of the objects. Meaning, we start working with numbers instead of some qualitative properties. Now, we’re in business!</p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Random Variable is not a variable, nor random
</div>
</div>
<div class="callout-body-container callout-body">
<p>A random variable is quantificator of elementary events, a function defined on the outcome space which maps the elementary events to the real number line. That mapping can’t be done in any way we wish, it has to perserve the informational structure of the sample space. That is one of the technical reasons for sigma-algebras we mentioned before and is related to the idea of <strong>measurability</strong>, meaning we can assign a meaningful “volume”.</p>
<p><span class="math display">\[\begin{align}
X(\omega):\Omega \rightarrow \mathbb{R} \\
s.t. ~~ \{\omega \in \Omega | X(\omega) \leq r, \forall r \in \mathbb{R} \} \in \mathcal{F}
\end{align}\]</span></p>
</div>
</div>
<p>Let’s figure out what the hell do we mean by that fine print condition, using the diagram below. The idea of conservation of the informational structure is actually equivalent to the one of measurablility. If this property doesn’t hold, it’s not possible to explicitly and uniquely refer to the sets (events) of interest.</p>
<p>The idea is that the preimage defined above <span class="math inline">\(X^{-1}((-\infty,r]) = E \in \mathcal{F}\)</span> on the following interval corresponds to an event E which should be in the event space <span class="math inline">\(\mathcal{F}\)</span>. Because the only thing that varies is the limit of the interval r, the randomness comes from it. Also, it automatically suggests the idea of the Cumulative Distribution Function, which is <span class="math inline">\(F_X(X \le r)\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>CDF, one of the most important constructs in probability is a direct consequence of the definition of the random variable:</p>
<p><span class="math display">\[
P(A ≤ r) = F_X(X ≤ r)
\]</span></p>
<p>In the practice of modeling, we often work with probability density functions, because it is more convenient in many cases. Then, in order to translate to probabilities, we would think in terms of areas under that curve. For sure, you remember the following duality between CFD and PDF:</p>
<p><span class="math display">\[
F'(x) = p(x)
\]</span></p>
</div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="img/random_variable.png" title="Random Variable" class="img-fluid figure-img" style="width:90.0%">
</figure>
<p></p><figcaption class="figure-caption">Idea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable. It is the construct that enables us to define the statistical population (some relevant aspect of it to us)!</figcaption><p></p>
</figure>
</div>
<!-- 
::: {.callout-note}
## An adversarial example. Not in F!

The example shouldn't be too complicated in order not to respect the definition of a random variable. Here's a a mapping of a strange dice $X(w_i)$ given by G. Ruxanda in his lectures:

$$
X: \{ 
    (w_1 \rightarrow 0), 
    (w_2 \rightarrow 0), 
    (w_3 \rightarrow 1), 
    (w_4 \rightarrow 0), 
    (w_5 \rightarrow 1), 
    (w_6 \rightarrow 1)
\}
$$

The best way to see the problem here, is to draw the mapping and pick 3 thresholds for $r$. Then see if we can recover events in $\mathcal{F}$ by $X^{-1}(w)$.

::: -->
<div class="page-columns page-full"><p>As a motivation of why do we have to understand all of this, when for most practical applications we can get by just fine with using the results and tools from probability, I will introduce two examples: one of compositional data analysis<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> and time series analysis. What I want to say, is that for more “exotic” applications, we might need to tweak that probability triple because of the nature of the problem, which has downstream consequences for all the statistical machinery we use in those applications.</p><div class="no-row-height column-margin column-container"><li id="fn11"><div class="quarto-figure quarto-figure-center"><sup>11</sup>&nbsp;
<figure class="figure">
<figure class="figure">
<img src="img/composition.png" title="Simplex" class="img-fluid figure-img" style="width:90.0%">
</figure>
<p></p><figcaption class="figure-caption"><em>Source: <a href="https://www.mdpi.com/1660-4601/17/7/2220">Dumuid</a></em>: Data in a Simplex, which is later translated to <span class="math inline">\(R^n\)</span> by a carefully constructed basis expansion</figcaption><p></p>
</figure>
</div>
</li></div></div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The curious case of Compositional Data Analysis
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sometimes, the data doesn’t “live” in our normal, intuitive, euclidian space <span class="math inline">\(\mathbb{R}^n\)</span>. There are cases when the object of our analysis are proportions or compositions: think of what a material is made of, the proportion of the demand for different sizes of a shoe or garment.</p>
<p>We don’t necessarily care about their absolute value, but about their relative proportion. If we blindly apply traditional methods, or even statistical summaries, we will quickly hit weird results and paradoxes. So, we have to tweak existing methods we have make sense for compositions.</p>
<p>Compositional data analysis solves those issues by defining a probability triple over the simplex (instead of <span class="math inline">\(\mathbb{R}^n\)</span>): <span class="math inline">\((\mathcal{S}^n, \mathcal{F}, \mathbb{P})\)</span>. This leads to a different definition of the event space <span class="math inline">\(\mathcal{F}\)</span>, which is also a sigma-algebra and a different definition of the probability measure <span class="math inline">\(\mathbb{P}\)</span>.</p>
</div>
</div>
<div class="page-columns page-full"><p>Remember our exaple of <a href="../01_fundamentals/background.html#implicit-learning-intuition-and-bias">pigeon superstition</a> in the context of learning? It is not surprising to me that measure theory becomes important in Learning Theory,<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> which is exactly those carefully formulated principles that will prevent our automated learners to become supersitious.</p><div class="no-row-height column-margin column-container"><li id="fn12"><p><sup>12</sup>&nbsp;Even though most courses from which I studied don’t mention it explicitly (Yaser Abu-Mostafa, Shai Ben-David, Reza Shadmehr), according to Mikio’s Brown <a href="https://www.quora.com/Is-Measure-Theory-relevant-to-Machine-Learning/answer/Mikio-L-Braun?srid=KONR">answer</a> it’s essential in the idea of <strong>uniform convergence</strong> and its bounds, where <em>“you consider the probability of a supremum over an infinite set of functions, but out of the box measure theory only allows for constructions with countably infinite index sets”</em>.</p></li></div></div>
<p>For the next example, you don’t have to understand what Gaussian Processes are or are used for. However, later in the course, we will discuss nonparametric methods for hypothesis testing. Their usefulness comes from the fact that we make less distributional assumptions about our population, therefore getting more robust results, in contrast with choosing a wrong model or distribution.</p>
<p>It’s not that these methods don’t have parameters, but the parametrization varies depending on how much data we have, which makes them very flexible in a wide variety of applications, where we just don’t know what is a reasonable distribution or parametric functional form for the relationship that we model.</p>
<div class="callout-tip callout callout-style-default callout-captioned page-columns page-full">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Nonparametrics and Gaussian Processes
</div>
</div>
<div class="callout-body-container callout-body page-columns page-full">
<!-- Stochastic Processes, Graphs and Trees -->
<div class="page-columns page-full"><p>If we’re thinking about a regression from the nonparametric perspective: that is, over a set of abstract functions: <span class="math inline">\(f(x) \in \mathscr{C}^2:X \rightarrow \mathbb{R}\)</span>, <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> we might want to know how a draw of samples from an infinite set of continuous differentiable functions might look like.</p><div class="no-row-height column-margin column-container"><li id="fn13"><div class="quarto-figure quarto-figure-center"><sup>13</sup>&nbsp;
<figure class="figure">
<figure class="figure">
<img src="img/gp-conditioned.png" title="GP Posterior" class="img-fluid figure-img" style="width:90.0%">
</figure>
<p></p><figcaption class="figure-caption"><em>Source: <a href="https://blog.economic-cybernetics.com/post/2018-06-01-probabiliy/">Bizovi</a></em>: A posterior distribution of the Gaussian Processes, when conditioned on data</figcaption><p></p>
</figure>
</div>
</li></div></div>
<p><span class="math display">\[
f(x) \sim GP(\mu(x); K(x,x'))
\]</span></p>
<p>The questions arises: how to define a PDF (probability density function) in this space? In my bachelor thesis, I got away with using Gaussian Processes, which are a very special class of stochastic processes. In this special case I could informally define an apriori distribution by defining the mean vector and Kernel (covariance function), then condition it on observed data with a Normal Likelihood.</p>
<p><span class="math display">\[
p(f(x) \, |\left \{ x\right \})=\frac{p(\left \{ x\right \}| \, f) \, \mathbf{p(f)}}{p(\left \{ x\right \})}
\]</span></p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>In the case of stochastic processes, we work with a sequence of random variables <span class="math inline">\(\{X_t, t \in T \}\)</span> and start asking questions:</p>
<ul>
<li>What kind of time dependency is there? (autocorrelation)</li>
<li>What is the long-run behavior?</li>
<li>Can we say something about extreme events?</li>
</ul>
</div></div><p>A lot of important applications in economics and finance are dynamic, so we have to work with time series very often. It gets worse when data is correlated not only in time, but also geographically – which is why the field of spatio-temporal data analysis is in such demand right now for policy-making.</p>
<p>Thus, a natural extension of this probability machinery we discussed so far is stochastic processes, underlying these dynamical systems. We can look at our time series as a manifestation, a particular instantiation of this latent process. Depending on which one we choose, we can model a wide range of phenomena.</p>
</section>
<section id="joint-conditional-and-marginal-distributions" class="level3">
<h3 class="anchored" data-anchor-id="joint-conditional-and-marginal-distributions">Joint, Conditional, and Marginal Distributions</h3>
<!-- 
[The reason we went on a theoretical detour to construct the probability triple, was simply to raise awareness of what is the theory that stands behind probability as we will practice it and what are some situations when handwaving could bite us]{.aside}

The point of this course is not to be a comprehensive investigation into probability theory topics like moment-generating functions, expectation, variance, etc. If I stirred your interest and renewed a passion for these topics, feel free to refer to the additional resources in the beginning of the lecture.

Starting point: unconditional probability degree of belief in a proposition in the absence of any other evidence -->
<!-- The most complete method of reasoning about sets of random variables is by having a joint probability distribution. A joint probability distribution, , assigns a probability value to all possible assignments or realizations of sets of random variables.  -->
<!-- 
In the diagram, there are four random variables: 1) Interest Rate , 2) Stock Market , 3) Oil Industry , and 4) Stock Price  (assume for an oil company). The arrows between the random variables tell a story of precedence and causal direction: interest rate influences the stock market which then, in combination with the state of the oil industry will determine the stock price (we will learn more about these arrows in the Graphical Models chapter). For simplicity and to gain intuition about joint distributions, assume that each of these four random variables is binary-valued, meaning they can each take two possible assignments:


Thus, our probability space has  values corresponding to the possible assignments to these four variables. So, a joint distribution must be able to assign probability to these 16 combinations. Here is one possible joint distribution:

Collectively, the above 16 probabilities represent the joint distribution  - meaning, you plug in values for all four random variables and it gives you a probability. For example,  yields a probability assignment of 12.15%.


Notation note:  . Each defines a function where you supply realizations  and  and the probability function will return . For example, let  outcome of a dice roll and  outcome of a coin flip. Hence, you can supply potential outcomes, like  which means  and the function output would be 
 
 (if you were to do the math). -->
<!-- Three additional critical concepts are: joint probability, which is basically the storytelling behind the data generating process,  conditioning (basically reducing uncertainty and adding information) and marginalization (esp of nuisance parameters -- explain what are those).  -->
<!-- Condit, ionarea ne aduce informat¸ie ˆın plus (scade incertitudinea asupra variabilei
aleatoare de interes), ¸si dup˘a cum afirm˘a J.Blitzstein, este esent,a statisticii. Pe de
alt˘a parte, condit¸ionarea ne permite s˘a factoriz˘am distribut¸ii multidimensionale,
dup˘a logica influent¸elor. Utilizarea acestei tehnici este central˘a ˆın econometrie
¸si analiz˘a bayesian˘a.
Condit, ionare P(x, y) = P(x)P(y|x)
P(y|x) = P(x, y)
P(x)
(2.3)
Marginalizarea, ˆın cazul discret prin ˆınsumare, ˆın cel continuu prin integrare,
ne permite s˘a separ˘am o distribut¸ie a unei submult¸imii de variabile aleatoare
dintr-o distribut¸ie comun˘a.
P(x) = X
y
P(x, y) -->
<!-- conditional probability degree of belief in a proposition given some evidence that has already been revealed -->
</section>
<section id="bayes-rule" class="level3">
<h3 class="anchored" data-anchor-id="bayes-rule">Bayes Rule</h3>
<!-- Given clouds in the morning,
what's the probability of rain in the afternoon?
• 80% of rainy afternoons start with cloudy
 mornings.
• 40% of days have cloudy mornings.
• 10% of days have rainy afternoons. -->
<!-- ### Interpretations of probability -->
</section>
</section>
<section id="mathematical-statistics-in-a-nutshell" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-statistics-in-a-nutshell">Mathematical Statistics in a nutshell</h2>
<!-- 
Colectivitatea reprezint˘a o mult, ime de entit˘at, i, obiecte sau forme reale, care au
anumite caracteristici de interes corelate. Aceste caracteristici variaz˘a ˆıntre indivizi,
obiecte, ˆın timp s, i spat, iu. De¸si acest fapt motiveaz˘a o abordare multidimensional˘a,
apar problemele dimensionalit˘at¸ii ¸si redundant¸ei. De asemenea, modelele construite
trebuie s˘a respecte principiul parsimonit˘at¸ii, deseori numit briciul lui Occam.
Pentru ca modelarea s˘a fie posibil˘a, avem nevoie de o abstractizare a valorilor
pe care le ia o caracteristic˘a la nivelul colectivit˘at, ii. ˆIn acest context este relevant
conceptul de variabil˘a aleatoare, care aduce o caracteristic˘a ˆın domeniul cantitativ s, i
generalizeaz˘a valorile ce pot fi luate de colectivitate. Aceast˘a mult¸ime a observat¸iilor
¸si este populat¸ia statistic˘a, care va fi suportul pentru distribut, ia de probabilitate.
Informat, ia pe care o avem niciodat˘a nu va fi complet˘a, din cauza zgomotului,
naturii stochastice a fenomenelor sau a disponibilit˘at¸ii datelor ˆın timp ¸si la nivelele
de granularitate necesare. Acest fapt ˆınseamn˘a c˘a trebuie estimate cantit˘at, ile
relevante s, i testarea semnificat, iei rezultatelor este extrem de important˘a. Validarea
modelelor ¸si testarea, dup˘a cum vom vedea, poate lua forme diferite ˆın dependent¸˘a
de tipul model˘arii. -->
<!-- 
Exist˘a trei moduri principale de a privi es,antionul, ˆıns˘a pentru modelare, cea
mai important˘a interpretare este a es,antionului ca model de select, ie: o list˘a a
vectorilor variabilelor aleatoare pentru fiecare obiect din es,antion {X1, X2, ..., XT }.
Acest mecanism teoretic generalizeaz˘a observat, iile posibile din es,antion. Xi
sunt replici identice dup˘a definit, ie (vin din aceias, i distribut, ie), iar dac˘a sunt
s, i independente observat, iile, metodele de estimare ca Verosimilitatea Maxim˘a
pot fi folosite. -->
<!-- 
Un parametru ˆın abordarea frecventist˘a e o constant˘a important˘a pentru cunoas,tere,
dar care niciodat˘a nu e cunoscut˘a cu exacitate. Abordarea bayesian˘a consider˘a
parametrii ca dristribut, ii de probabilitate, ceea ce duce la un alt tip de inferent¸˘a.
Estimatorul tT (X1, X2, ..., XT ) ≈ ˆθ este funct, ie pe modelul de select, ie {X1, X2, ..., XT },
care are ca scop aproximarea parametrului. Ace¸sti estimatori, motora¸se de estimare,
sunt selectat¸i dup˘a propriet˘at¸ile sale de nedeplasare ¸si eficient¸˘a. Deoarece este o
funct¸ie pe modelul de select¸ie, t este o variabil˘a aleatoare, iar dac˘a ¸stim care este
distribut¸ia ei, se pot crea teste de semnificat¸ie statistic˘a.
O remarc˘a important˘a este c˘a valorile estimate deseori corespund cu valoarea
maxim˘a posterioar˘a (MAP) ˆın analiza bayesian˘a, iar odat˘a ce oper˘am cu distribut, ia
posterioar˘a, putem extrage mult mai mult˘a informat, ie decˆat o estimat, ie punctual˘a s, i
intervalele de ˆıncredere, ci ˆınsus, i probabilitatea de a se afla ˆıntr-un anumit interval.
Estimat, ia const˘a ˆın evaluarea estimatorului cu o manifestare a modelului de
select, ie (datelor disponibile), obt¸inˆand valoarea empiric˘a a parametrului. ˆθ =
tT (·)|x1,...,xT -->
<!-- Modelarea unui fenomen cu ajutorul datelor multidimensionale, care au o distribut¸ie
comun˘a p(Y, X) ¸si g˘asirea distribut¸iei de probabilitate este dificil˘a f˘ar˘a condit¸ionare.
ˆIn contrast, condit¸ionarea pe variabilele independente ne aduce informat¸ii ˆın plus -->
<!-- 
despre variabila dependent˘a p(Y |X = x). ˆIn acest mod, ˆın econometrie se deriveaz˘a
estimatorul, ¸si legea de probabilitate urmat˘a.
Exemplificˆand pe un caz bidimensional, dac˘a X ia valori discrete, p(Y |X =
x1), ..., p(Y |X = xk) vor fi distribut¸ii unidimensionale. Aceast˘a interpretare este
important˘a, deoarece ne sugereaz˘a descompunerea efectelor ˆıntre factorii nesemnificativi ¸si cei semnificativi, funct¸iile de regresie ¸si skedastice. Acelea¸si concepte vor fi
reg˘asite sub o alt˘a form˘a ˆın cadrul ˆınv˘at¸˘arii statistice. Media condit¸ionat¸˘a E[Y |xi
]
ˆın cadrul fiec˘arei instant¸e a variabilei aleatoare X = xi va fi o constant˘a, nivelul
mediu Y cˆand X ia anumite valori. Media condit¸ionat˘a pe X E[Y |X] = f(x) este
numit˘a regresie ¸si variant¸a condit¸ionat˘a V ar(Y |X) = g(x) - funct¸ia skedastic˘a.
Rat¸ionamentul de mai sus, ˆımpreun˘a cu cele dou˘a teoreme introduse mai jos,
permit completarea cadrului teoretic pentru a ˆınt¸elege mai bine argumentele despre
signal ¸si zgomot ˆın ˆınv˘at¸area automat˘a. Teorema iterativit˘at¸ii mediilor ne spune
c˘a a¸steptarea mediilor condit¸ionate este ˆıns˘a¸si a¸steptarea necondit¸ionat¸˘a.
E[E[Y |X]] = EY (2.5)
O teorem˘a ¸si mai important˘a, cea a descompunerii variant¸ei are o interpretare
intuitiv˘a: variant¸a poate fi descompus˘a ˆın m˘asura factorilor semnificativi, adic˘a
signal, care ne arat˘a cum variaz˘a a¸steptarea de la xi
la xj ¸si zgomot sau factori
nesemnificativi, adic˘a media variat¸iilor condit¸ionate.
V ar(Y ) = V ar(E[Y |X]) + E[V ar(Y |X)] (2.6)
Desigur, acest caz particular poate fi generalizat la dimensiuni multiple, dar ¸si
exemplul simplu are o narativ˘a ce face evidente conceptele de R2
teoretic, funct¸ie
de regresie, verosimilitate ¸si ne permite o interpretare din punctul de vedere a
teoriei informat¸iei. -->
<!-- 
importance of the data generating process, see what Ernesto said, then the causal process -- all is parameters, known or unknown 

Uses everything probability has to give, but is concerned with inference, estimation, hypothesis testing, prove theorems and probabilistic properties of estimators.

- Collectivity (can be anything, trees in central park, pixels, people, etc, etc) -- i.e stuff in the real world
- Statistical Population (binding contract)
- Sample (a data collection process is involved here)
- Parameter (average tree height, some aspect or characteristic of interest, unknown number/constant (bayes, a distribution), which is at population level) -- of the DGP, to be estimated from our sample.
- Estimator: t_theta(X) -> theta
    - Confidence intervals: depend on the distribution of the estimator, since X is rv, t(X) is a random variable too -- and it is the job of statisticians to tell us what is that P(t(X)) and their properties: if biased, consistent, efficient
- Estimation/a Statistic: theta_hat (a way to summarise and synthetise data)

Draw a diagram between sample and population. 


![*Source: [Data Science, A first Introduction](https://datasciencebook.ca/)* Population vs Sample](img/population_vs_sample.png "Population vs Sample"){width="90%"}


The point of statistics: change our opinion about the action we have (or phenomenon we want to understand) to take in the face of evidence.

One big problem is if we got the model right for our use-case and phenomena. A wrong model choice leads to wrong conclusions. It should be informed as much by stats as by the theory and domain. Think of this in the spirit of the scientific method. -->
<section id="models-are-golemns-of-prague" class="level3">
<h3 class="anchored" data-anchor-id="models-are-golemns-of-prague">Models are Golemns of Prague</h3>
<!-- Data + Domain Assumptions + Statistical Assumptions -->
</section>
<section id="model-specification-fisher-vs-bayes" class="level3">
<h3 class="anchored" data-anchor-id="model-specification-fisher-vs-bayes">Model Specification: Fisher vs Bayes</h3>
</section>
<section id="independence-and-exchangeability" class="level3">
<h3 class="anchored" data-anchor-id="independence-and-exchangeability">Independence and Exchangeability</h3>
<!-- 
I find it useful to review **exchangeability** and De Finetti's theorem, before presenting the components of a Bayesian model, as it gives a sense of what is the underlying assumption of Hierarchical Models. 

A sequence of random variables is infinitely exchangeable if the joint probability for every set of $n$ variables is invariant under permutation of indices. De Finetti's theorem, states that a sequence of random variables $(y_1, y_2, ...)$, with $P(d\theta)$ -- a measure, is exchangeable if and only if:

\begin{equation}
    p(y_1, ..., y_n) = \int \prod_{i=1}^{n} p(y_i | \theta) P(d\theta)
\end{equation}


It follows that there exists a parameter $\theta$, a distribution on $\theta$ and a likelihood $p(y_i | \theta)$ such that the random variable sequence is independent. Covariates can be incorporated into the formulation and theorem applied again for a vector of parameters $\theta$, which resembles the hierarchical formulation introduced in section 3. Thus, there exists a:

- hyperparameter $\phi$
- a distribution over the hyperparameter $p(\phi | \mathbf{x})$
- likelihood $p(\theta | \phi, \mathbf{x})$ such that $\theta_i$ are independent when drawn from this distribution


These facts highlight the elegance of the hierarchical models, where hyperprior distribution is learned from data and can be marginalized in the case it's a nuisance parameter for a particular analysis. However, de Finetti's theorem doesn't give any prescription on how these priors should be chosen, only that they exist, so best practices in literature should be consulted and appropriateness given the problem at hand assessed.



\begin{equation}
    p(\theta | y) \propto \int d\phi ~ p(y | \theta) p(\theta | \phi) p(\phi) 
\end{equation}


Bayesian models are usually built and formulated in a modular way, which forces the modeler to make stochastic and structural assumptions explicit. Because of the fact that formulation captures conditional independencies, it is interesting to view the model as a graph, where nodes are random variables, observed quantities and (directed) edges influences and information flows. The process of Bayesian Inference satisfies at once a few requirements: the ability to update prior beliefs once new data is available, combining different sources of information, modeling the data generating process and quantifying uncertainty in estimates. Let $D = \{ (x_i, y_i) \}_{i=1}^n$ be the data, then the iterative model update works according to the Bayes Rule:

\begin{equation}
 \overbrace{p(\theta | X, y)}^\text{posterior} = \frac{\overbrace{p(\theta | X)}^\text{prior} \cdot \overbrace{p(y | \theta, X)}^\text{likelihood} }{ p(y | X) }
\end{equation}


The posterior can be used as a prior when new data is available and iteratively updating it. This consistency is useful when constructing priors using literature, previous experiments, ``online`` learning and doing meta-analyses. In a business setting, it is an important property in order to capture changes in the phenomena modeled, but avoiding the (possible) instability related to re-training complex machine learning models.

\begin{equation}
 p(\theta' | X', y') = \frac{p(\theta | X, y) \cdot p(y' | \theta')}{p(y'|\theta, X')}
\end{equation}

Since the model is generative, i.e. after building a full probabilistic model of the data, parameters and latent variables, it is useful not only to infer and summarize parameter distributions, but to generate data as a tool for checking assumptions made. In this regard, prior and posterior predictive distributions are most often used in literature. Sampling using the marginal distribution of the observed value, before fitting any data, gives a sense on what kind of patterns, in which range, with what spread can the priors and likelihood specify.

\begin{equation}
    p(y) = \int d\theta ~ p(\theta) p(y | \theta) 
\end{equation}


To make a prediction/inference regarding an unobserved or new value $\tilde{y}$ given the posterior distribution of parameters $p(\theta | y)$, they are marginalized. I will extensively use this technique throughout the next chapters and summarize this distributions with ``credible intervals``.

\begin{equation}
    p(\tilde{y} | y) = \int d\theta ~ \underbrace{p(\tilde{y} | \theta, y)}_\text{likelihood} \cdot  \underbrace{p(\theta | y)}_\text{posterior}
\end{equation}


Latent variables and parameters are treated the same in Bayesian Modeling and dealing with missing data is more natural than in Statistical Learning, because it is viewed as an unobserved value and will have a posterior predictive distribution, as a part of the sampling process. Moreover, if the data is Missing Not at Random, this mechanism can be modeled explicity, but needs more informative priors about the nature of missingness. \cite{best12} has an excellent overview of approaches regarding missing targets or covariates and summarize it with the following graphical models, where $m_i$ is an indicator variable in the case $y_i$ is missing.


The representation in figure above is a Probabilistic Graphical Model, which is a directed acyclic graph, where nodes are observed, unobserved variables and parameter, edges are logical or stochastic dependencies. An important way of thinking about causality was pioneered by Judea Pearl, which concerns itself with inference of the graph from data and factorization of joint distribution. Two ideas are relevant for this dissertation: Reichenbach's common cause principle, \cite{reichen62} and the idea of counterfactual formalized by Rubin.

- If $X$ and $Y$ are statistically dependent, there exists $Z$ causally influencing both.
- Given a ``chain`` $X \rightarrow Z \rightarrow Y$, X and Y are conditionally independent on Z: $(X \perp Y) ~|~ Z$

In Bayesian statistics conventional definitions of identifiability do not apply, so I'm more concerned with ``information contents`` given a model and particular dataset. If a posterior distribution of some parameter is flat over a large range, it is an indication that little was learned from data relative to prior information. Viewing indentifiability as the posterior being proper is not very useful in practice, as there isn't a need to even define a finite variance or mean.
Model critique, improvement and visualization \cite{gabry19} play a key role in the Bayesian workflow, since a large amount of time is spent summarizing models and finding appropriate prior and structural specifications. As A. Gelman mentioned, ``Bayes doesn't give up``, when conventional methods would refuse to estimate the parameters, which implies it is the modeler's responsibility to challenge the model and check assumptions, find ways of better parametrization. In the last decades, the focus has shifted from the mathematical analysis to computational methods as Hamiltonian Monte Carlo, INLA, Variational Inference, which enable a great deal of experimentation with model specifications. Next chapters follow this spirit, favoring intuition and understanding over analytical tractability and focus on mathematical aspects which are really important for building models with ``nice`` properties. -->
</section>
<section id="storytelling-with-dags" class="level3">
<h3 class="anchored" data-anchor-id="storytelling-with-dags">Storytelling with DAGs</h3>
</section>
</section>
<section id="what-does-a-statistician-want" class="level2">
<h2 class="anchored" data-anchor-id="what-does-a-statistician-want">What does a statistician want?</h2>
<!-- From their estimators -->
<section id="estimator-properties" class="level3">
<h3 class="anchored" data-anchor-id="estimator-properties">Estimator Properties</h3>
</section>
<section id="bias-variance-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="bias-variance-decomposition">Bias-Variance Decomposition</h3>
</section>
<section id="rao-cramer-inequality" class="level3">
<h3 class="anchored" data-anchor-id="rao-cramer-inequality">Rao-Cramer Inequality</h3>
</section>
</section>
<section id="case-studies" class="level2">
<h2 class="anchored" data-anchor-id="case-studies">Case Studies</h2>
<section id="de-moivre-on-us-schooling" class="level3">
<h3 class="anchored" data-anchor-id="de-moivre-on-us-schooling">De Moivre on US Schooling</h3>
<!-- 
Let's remember what we discusssed last time. So, the US gov, tried to find out what makes some schools better than others. They saw that in top schools, small ones were dominating (in terms of nr students, perf. being SAT ~ Norm()). 

They decided to split up big schools into smaller ones. Can you say if it was a good, bad idea and why did they think that?

Students hypothesis:
- more attention to students in smaller classrooms
- private schools, where families are more wealthy
- geographical location, i.e. small towns and villages
- motivation of the underdog, from small towns
- friendships, community and peer competition (easier in smaller schools)
- self-selection: what kind of students go into small schools -- we assume average IQ is the same, including the distribution. Assume random allocation. 
- Skewed (even if not, longer tails): meaning, asymmetric distributions. 
- Finally: variance and standard deviation

About the decision sanity: reality vs data. The conservative decision is to do nothing, as restructuring costs a lot, in money and social changes & consequences, it is risky -- that is, the default action, without seeing any data or evidence, we would do nothing. 

S: A good remark about having a diversity, specializations (professional schools) -- can't have a one-size-fits-it-all policy. But we ignore it, oversimplifying the problem.

All of that is true, but just for 10 minutes, we introduce a (false) dichotomy between small and large schools. The alternative is to split big school -- remember, that outside this box, or bad framing of the problem, there are many many potential interventions (professor education, smaller classrooms, fairer allocation of students, raising up the disadvantaged hood schools). 

The punchline: we have all these factors potentially contributing to the performance. Because of DeMoivre and LLN (mean/sqrt(sd)) -- in a simulation, we notice the following thing: (insert graphs).

> Small schools will dominate both the top and bottom, due to larger variance. This is what it means. 

So, we go back to this idea that statistical models, are golemns, they do exactly what they are told to, and this is dangerous. We have to understand deeply in what contexts and in which ways these little robots fail and give absurd and invalid results without us knowing.

We will go into more detail next week in A/B testing and hypothesis testing, but here we go: once we have the default action and the alternative action, the null hypothesis is (sidenote, we don't care about rote calculation -- R packages does it for us, but we care about experiment design and decisions, try to figure out the causal influences):  in which worlds will I take the default actions (worlds, meaning the values of the unknown parameter of interest, describing the behavior of the populations -- in this case quantifying the impact of school size on SAT scores). The alternative hypothesis, is exactly the opposite, all the other worlds (parameter values) when I will take the alternative action.

So, in this case we will collect data, used to estimate the parameter and its confidence intervels (inference, with the idea of generalizing from the sample to the population). Now, given those confidence intervals, we ask how surprising it is to see the data/estimation, if the null hypothesis was true.

Most stats courses jump over these important aspects. Their starting point is: H0, HA are given, model is given, you just compute. In practice, even for experienced statistician, for a new problem, it is very hard to define those 4 components -- as it depends on the domain knowledge and business objectives. Moreover, even the interpretation of p-values is extremely tricky to communicate. We will develop tools to deal with those challenges.  -->
</section>
<section id="chess-example" class="level3">
<h3 class="anchored" data-anchor-id="chess-example">Chess example</h3>
<!-- I played chess for 14 years, professionally, question about difference in genders and what kind of tournaments and policies should we have. Why are there no women rn in top 100 (Historically, it was judit polgar in the top 20) ?

> Use this chance to extract data and analyse. Same problem with marathon runners (from BS big data). 

Causes?
- Bias: inferring natural ability -- nonsense from IQ research
- Misoginism
- Education and lack of encouragement
- Lack of opportunities, community, support
- Historical trajectory -- lack of competition

A lot is explained by how many boys and girls start from a given cohort/age. Similar problem, but slightly different -- for any problem of rank top(n).

We will discuss next time a whole range of other pitfalls, including spurious correlation (nicolas cage and drowning), common cause (babies and storks), ommited variables, mediation, selection bias, reverse causality, even including predictors which shouldn't be there. We need a statistical reasoning for all of these potential pitfalls. 

> explore python cli app + scraping + arrow + duckdb, see the trajectories of players and so on (chess.com vs fide). For the trajectories, mention the idea of longitudinal data, survival models, growth models, mixed models -- because of the particularities of DGP and causal process 

> see the difference between ELO vs xbox true skill, how fast it converges to the true ability -- that is a model we can investigate.  -->
</section>
<section id="amazon-reviews-and-ratings" class="level3">
<h3 class="anchored" data-anchor-id="amazon-reviews-and-ratings">Amazon reviews and ratings</h3>
<!-- How do you choose 4.7 (300) vs 4.5 (2000) -- according to what bounds, agresti-coull, correction. What chance do you give those with a small sample size? What is the optimal strategy here?

- we can guide ourselfs with the quality and relevance of reviews
- is it representative? (also, outside the box)
- text summary

Q: What is the variance and distribution, it depends, also on our risk appetites (w.r.t.) volume or if is one time (e.g. buy shoes vs coffee shops -- repeated interactions).

Introduce the confusion matrix, which will be discussed at length in ML and hypothesis testing (types of error) -- the point is that sometimes the payoff for correct decison or a mistake is asymmetric. And that matters (fraud vs reco quality) -- so, that is context-dependent. 

> good applications for text mining, statistical tests -->
</section>
</section>
<section id="data-mining-and-crisp-dm-variants" class="level2">
<h2 class="anchored" data-anchor-id="data-mining-and-crisp-dm-variants">Data Mining and CRISP-DM variants</h2>
<section id="ml-is-a-geocentric-model-of-the-universe" class="level3">
<h3 class="anchored" data-anchor-id="ml-is-a-geocentric-model-of-the-universe">ML is a geocentric model of the universe</h3>
<!-- 
The good news is that sometimes, you just need a reliable prediction, as you're not intervening in the system causing a certain phenomena -- and by retraining ML models you can adapt to minor changes introduced by our interventions. Those ML models, they clearly didn't figure out a scientific explanation of the causal process, e.g. for demand forecasting in uber, and that just the prediction to be used in a larger ecosystem and environment for decision-making. 

Again, that is appropriate in low-stakes, large scale decisons. You might not care so much about the latent, causal process; but of course, you care that it generalizes to the population of interest (that is, a binding contract, a boundary in which your predictions are valid -- if you go outside that range, can easily get absurd predictions -- this is why this kind of system needs checks-and-balances, boxes to constrain the artificial stupidity). It is extraordinarily unlikely that these models translate to novel situations and environments without explicit transfer learning and careful adaptation.  -->
</section>
</section>
<section id="statistical-process-and-science" class="level2">
<h2 class="anchored" data-anchor-id="statistical-process-and-science">Statistical Process and Science</h2>
<section id="dead-salmon-a-warning-tale" class="level3">
<h3 class="anchored" data-anchor-id="dead-salmon-a-warning-tale">Dead salmon: a warning tale</h3>
<!-- Forward reference to power and multiple testing -->
<!-- Statistical Foundations: Reference, Revies --->


</section>
</section>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../01_fundamentals/background.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Data Science in Business Context</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../01_fundamentals/bolt.html" class="pagination-link">
        <span class="nav-page-text">Case Study: Decisions at Bolt and LRB</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">The course I wish I had when getting started. Built with ❤️ by Mihai Bizovi</div>   
  </div>
</footer>



</body></html>