---
title: "MLR3"
output: html_document
---

## Scop, Obiective

- actiuni pe baza modelului
- care e domeniul, scopul

```{r import_packages, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(mlr3)
library(mlr3viz)
library(mlr3verse)
library(mlr3learners)
library(mlr3tuning)

set.seed(423)
```


### Import, Split and Visualize Data

- ce date am
- tipul problemei, abordarea (clasificare, red. dim., clustering)
- asteptari despre rezultate

```{r load_data}
data(diabetes, package = "mclust")
```

```{r split_data}
diabetes_idx <- diabetes %>% mutate(idx = row_number())
df_train <- diabetes_idx %>% dplyr::sample_frac(size = 0.8)
df_test <- dplyr::anti_join(diabetes_idx, df_train, by = "idx")
```

```{r}
df_train <- df_train %>% select(-idx)
df_test <- df_test %>% select(-idx)

summary(df_train)
```

A data frame with the following variables:
- **class**: The type of diabete: Normal, Overt, and Chemical.
- **glucose**: Area under plasma glucose curve after a three hour oral glucose tolerance test (OGTT).
- **insulin**: Area under plasma insulin curve after a three hour oral glucose tolerance test (OGTT).
- **sspg**: Steady state plasma glucose.

```{r}
df_train %>%
  ggplot(aes(x = glucose, y = insulin, col = class)) + 
  geom_point() +
  theme_minimal()

df_train %>%
  ggplot(aes(x = glucose, y = sspg, col = class)) + 
  geom_point() +
  theme_minimal()

df_train %>%
  ggplot(aes(x = insulin, y = sspg, col = class)) + 
  geom_point() +
  geom_density2d(aes(alpha = 0.4)) + 
  theme_minimal()
```
```{r}
df_train %>% 
  tidyr::gather(key = "variable", value = "value", -class) %>%  # transform to long (melt)
  ggplot(aes(x = value, fill = class)) + 
  geom_histogram(alpha=0.5) + 
  facet_grid(cols = vars(class)) +
  theme_minimal()
```

```{r}
df_train %>% 
  filter(insulin <= 800) %>%
  group_by(class) %>%  # group by 
  summarise(n = n(), average_glucose = mean(glucose)) %>%  # aggregate
    ggplot(aes(x = class, y = n, fill = class)) + 
    geom_col() + 
    theme_minimal()
```
### Data preprocessing

- calculate the mean and s.d. of the training data for continuous variables
- store them in variables
- apply **same** transformation on test dataset

```{r normalize-data}
# Exercitiu - automatizare (pt lista de coloane)
mean_tr_glucose <- mean(df_train$glucose)
mean_tr_insulin <- mean(df_train$insulin)
mean_tr_sspg    <- mean(df_train$sspg)

sd_tr_glucose <- sd(df_train$glucose)
sd_tr_insulin <- sd(df_train$insulin)
sd_tr_sspg    <- sd(df_train$sspg)

df_train_norm <- df_train %>% mutate(
    glucose = (glucose - mean_tr_glucose) / sd_tr_glucose, 
    insulin = (insulin - mean_tr_insulin) / sd_tr_insulin, 
    sspg    = (sspg - mean_tr_sspg) / sd_tr_sspg,
  ) 

df_train_norm %>%
  ggplot(aes(x = insulin, y = sspg, col = class)) + 
  geom_point() +
  geom_density2d(aes(alpha = 0.4)) + 
  theme_minimal()
```

```{r apply-test-normalization}
df_test_norm <- df_test %>% mutate(
    glucose = (glucose - mean_tr_glucose) / sd_tr_glucose, 
    insulin = (insulin - mean_tr_insulin) / sd_tr_insulin, 
    sspg    = (sspg - mean_tr_sspg) / sd_tr_sspg,
  ) 
```


```{r define-task}
diabetes_task <- mlr3::TaskClassif$new("diabetes", backend = df_train_norm, target = "class")
diabetes_task$data(cols = c("glucose", "sspg")) %>% head()
```
```{r visualize-pairs}
autoplot(diabetes_task, type = "pairs") + theme_minimal()
```

- glmnet: Generalized linear models w/ regularization (elastic-net)
- kknn: Kernel KNN
- e1071: SVM
- ranger: random forest
- xgboost: extreme gradient boosting


```{r define-learner}
# mlr_learners$mget(mlr_learners$keys("^classif"))
lrn_kknn <- lrn("classif.kknn")
lrn_kknn$param_set$values = list(k = 3, kernel = "optimal")
```

```{r train-model}
trained_kknn <- lrn_kknn$train(diabetes_task)
fitted_values <- trained_kknn$predict(diabetes_task)
fitted_values$confusion
```

```{r}
lrn_multinomial <- lrn("classif.multinom")
learners <- list(lrn_kknn, lrn_multinomial)
grid <- benchmark_grid(
  tasks = diabetes_task, 
  learners = learners,
  resamplings = rsmp("cv", folds = 10)
)
bmr = benchmark(grid)
```

```{r}
# performances <- bmr$aggregate(measures)
# performances
```

```{r}
trained_multinomial <- lrn_multinomial$train(diabetes_task)
```
Comparare cu un baseline simplu!

```{r}
pred_multi <- trained_multinomial$predict_newdata(df_test_norm)
pred_multi$confusion
```
```{r}
# pred_multi$score(measures)
```

```{r}
test_pred <- trained_kknn$predict_newdata(df_test_norm)
# test_pred$score(measures)
```

### Investigating the final model(s), on test data
```{r}
final_results <- df_test_norm %>% mutate(pred = test_pred$response) %>%
  mutate(mistake = class != pred) 

final_results %>%
  ggplot(aes(x = insulin, y = sspg, col = class, shape = mistake)) + 
  geom_point() +
  geom_point(data = final_results %>% filter(mistake == TRUE), size=3) +
  theme_minimal()
```
```{r}
final_results %>% 
  filter(class != pred)
```





```{r}
test_pred$confusion
```

```{r}
autoplot(test_pred)
```


```{r define-metrics}
measures <- msrs(c("classif.acc"))  # "classif.auc"
fitted_values$score(measures)

# for predicting on new data:
# trained_kknn$predict_newdata(df_test_norm)
```
```{r}
autoplot(fitted_values) + theme_minimal()
```
```{r}
resampling_scheme <- rsmp("cv", folds = 10)
resampling_results <- resample(
  diabetes_task,  # task (Data, target, type of problem)
  lrn_kknn,       # model + hyperparameters + configs+ 
  resampling_scheme,  # cross-validation
  store_models = TRUE # keep the models in memory/object
)
```

```{r}
resampling_results$aggregate(measures)
resampling_results$resampling$test_set(10)
resampling_results$learners[[1]]
resampling_results$predictions()[[2]]
```

### Tuning the hyperparameters
```{r configure-tuning}
lrn_kknn2 <- lrn("classif.kknn")
search_space <- ps(
  k = p_int(lower = 3, upper = 11),
  kernel = p_fct(c("gaussian", "rectangular", "optimal"))
)
terminate_condition <- trm("none")
cv10 <- rsmp("cv", folds = 10)
cv10$instantiate(diabetes_task)

tuner_instance <- TuningInstanceSingleCrit$new(
  task         = diabetes_task,
  learner      = lrn_kknn2,
  resampling   = cv10,
  measure      = msr("classif.acc"),
  search_space = search_space,
  terminator   = terminate_condition,
)
tuner <- tnr("grid_search")
tuner$optimize(tuner_instance)e
```

```{r}
as.data.table(tuner_instance$archive)  %>% 
  ggplot(aes(x = k, y = classif.acc, color = kernel)) + 
  geom_point() + 
  geom_line() + 
  theme_minimal()
```

```{r nested-resampling}
auto_tuning_grid <- AutoTuner$new(
  learner     = lrn_kknn2,
  resampling   = rsmp("cv", folds = 5),
  measure      = msr("classif.acc"),
  terminator   = trm("none"),
  tuner        = tnr("grid_search"),
  search_space = search_space,
)

resampling_results_auto <- resample(diabetes_task, auto_tuning_grid, cv10, store_models = TRUE)
```

```{r}
extract_inner_tuning_archives(resampling_results_auto) %>%
  as.data.table() %>%
  ggplot(aes(x = k, y = classif.acc, color = kernel)) + 
  geom_point() + 
  theme_minimal()
```

Concluzii: 
Performanta vs baseleline (benchmarking). Chestii interesante in date: relatii, legaturi, corelatii, clustere - cu ce ar fi utile. 








