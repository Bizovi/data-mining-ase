[
  {
    "objectID": "index.html#welcome-to-the-course",
    "href": "index.html#welcome-to-the-course",
    "title": "Pragmatic Data Science",
    "section": "Welcome to the course!",
    "text": "Welcome to the course!\nThis is an introductory, big picture graduate course which bridges the gap between theory and practice, cultivating the skills and understanding necessary to bring value to organisations by improving decision-making. It is an attempt to find a golden mean of both worlds:\n\nIlluminating theoretical ideas (contemplating in the library)\nPracticing battle-tested technologies (engineering in the trenches).\n\n\n\n\n\n\n\nWho should read this book?\n\n\n\n\nAnyone lost, confused, stuck or overwhelmed by Data Science and Machine Learning complexities, who wants to see the big picture and the possibilities\n\nIf you stumbled upon this e-book, you’re probably a student in Business Analytics at Bucharest’ Academy of Economic Sciences – well, because I shamelessly promoted it.\nMaybe, you’re an engineer getting curious about ML or an analyst with a knack for the business, looking to improve your workflow and expand the quantitative toolbox. Maybe you’re a product manager or an entrepreneur who wants to infuse AI into your startup.\n\n\nThis is the course I wish I had when starting my journey in data science, which would prepare me for the realities of the industry, often very different from the academic world. At the same time, this is NOT a bootcamp: it is not enough by itself to land you a job, which requires lots of practice and extra study.\nTo the service of understanding, it becomes quite abstract and conceptual at times, but I hope you bear with me until you see the benefits of those abstractions. Think of it as a skeleton, a conceptual frame1 which ties together everything you have learned so far and can be built upon as you progress in your carreer and studies.1 This course stands on the “shoulders of giants”, and I can only aspire to get to the level of clarity and rigor provided by Hastie/Tibshirani or Andrew Ng. However, there is too much content out there, and the roadmap provided here should help you navigate it and find the shortest path towards better decisions in your firm."
  },
  {
    "objectID": "index.html#why-should-you-care",
    "href": "index.html#why-should-you-care",
    "title": "Pragmatic Data Science",
    "section": "Why should you care?",
    "text": "Why should you care?\nYou might’ve heard that data scientist is the sexiest job of 21st century, that AI is going to take over repetitive jobs, Deep Reinforcement Learning models are beating people at Dota and Chess, solving almost-impossible protein-folding problems. But what does it actually mean, if we step outside the hype and buzzwords, use a plain language, and apply these ideas in a more down-to-earth, day-to-day problems and challenges in businesses?\nOver the past 3 years, you’ve probably been tortured by (or enjoyed) linear algebra, mathematical analysis, probability and statistics, operations research, differential equations, mathematical economics and cybernetics, algorithms and data structures, databases, object-oriented programming, econometrics and so on.\n\n\n\n\n\n\nWhy study all of this?\n\n\n\nWe live in a volatile, uncertain, complex and ambiguous world,2 but we still have to make decisions. Those decisions will bring better outcomes if they are informed by understanding the causal processes, driven by evidence and robust predictions.2 VUCA: a mental model to better understand the world\nI want you to take away ONE thing, that is AI and Data Science in Businesses boils down to: Decision-Making under Uncertainty at Scale\n\n\nIt can be a function of decision-making support or the system/product itself, like in the case of Uber, Amazon, Netflix, Spotify, Google and many others. Even if you are not a data scientist, you will work with them in one form or another (Quant, Data Analyst, Business Analyst, ML Engineer, Data/BI Engineer, Decision-Maker, Domain Expert). Therefore, you have to understand their language, what are they doing, how to ask and make sure they solve the right problem for you.\n\nWhen somebody asks you what have you learned in this book and course, I suggest two metaphors:3 one of simplification and another of seeing relations3 Due to my recent philosophical readings, some of the language here has a very specific meaning, which is totally outside the scope. For the curious, I will write a few blog posts outlining some philosophical arguments which struck a chord in me and which I found tremenously useful in day-to-day life.\n\n\n\n\n\n\n\n\n\nWe see Pollock’s messy reality, which is the data and observations. We want to get to Picasso’s bare bones essence, for better and clearer decision-making\n\n\n\n\n \n\n\n\n\n\nThis is a big picture course, which re-contextualizes everything you have learned before, but didn’t see how it all fits together or can it be implemented in practice to bring value to organisations, that is: be useful"
  },
  {
    "objectID": "index.html#cybernetics-done-right",
    "href": "index.html#cybernetics-done-right",
    "title": "Pragmatic Data Science",
    "section": "Cybernetics Done Right",
    "text": "Cybernetics Done Right\n\n\n\n\n\n\nWarning\n\n\n\nThe idea behind Cybernetics curriculum is great: an interdisciplinary approach to solving complex economic problems, that is what we know today as weak/specialised AI4.4 Or data science, if you wish – at the intersection of computer science, domain expertise (economics), statistics and mathematics. We will dive in more detail and nuance in our first lectures.\nUnfortunately, the execution is extremely difficult to pull off without a consistent vision, expertise and an account of recent developments. It is even more difficult to put those 80’s models in practice, especially in a data-driven age.\n\n\nLet’s go back to the burning question: what I wish I had in a course? It is hard to teach these topics in a way which is both theoretically sound and can be immediately applied.\nAfter recognizing that there is no silver bullet, my conclusion is that following the principles outlined below consistently, dramatically increases the chances of preparing a new generation ready to tackle the messy, ill-defined problems we encounter.\n\nProvide motivation for why something is important (a field, theory, model, method, technology)\nDiscuss practical applications and challenges the firms face, from an insider’s perspective:\n\nApplications are based on realistic or real data, which can be messy, hard to access, biased and incomplete\nCoding up the solutions and software implementations, with the real-world challenges of deploying models and improving decision making\n\nDevelop a conceptual undestanding, an intuition about the problem and the “tool” we think is appropriate in tackling it:\n\nPresent the tool theoretically rigorous and sound, but only where it matters\nLeverage previous mathematical, statistical, and domain knowledge\nFor the mathematically inclined, add some elements of abstract math to understand the underlying foundations of these methods and models\n\nUse simulations as a safe playground 5 that we control, to get a feel for the behavior of models and algorithms.\n\nImplement those in code using best practices, as an engineering exercise\nWith lots of visualization, especially interactive ones\n\n\n5 I often find myself truly understanding something, only after I code it up and understand the “mechanics” (of a model) well, then try to think of how I would apply it in practice, in different contexts. This hints to the idea that we need some complementary background and tools, some of them right from the beginning of higher education."
  },
  {
    "objectID": "index.html#your-unorthodox-guide",
    "href": "index.html#your-unorthodox-guide",
    "title": "Pragmatic Data Science",
    "section": "Your Unorthodox Guide",
    "text": "Your Unorthodox Guide\nThink of me as an industry expert,6 not an academic pushing you to do homework. Think of the course as a summer school or workshop, rather than required credits and another exam to pass. Here are a few relevant facts about me:6 \n\n\n\n\nSee one of my conference talks at BigDataWeek, intended for a mixed tech/business audience: Pragmatic AI in Google Cloud Platform\n\n\n\n\nGraduate of Cybernetics (2017) and Quantitative Economics (2019)\n\nDid some research in Complex Systems, Agent-Based Modeling, Systems’ Dynamics, and Heterodox Economics, which was lots of fun\nMy serious work was at the intersection of Bayesian Statistics and Machine Learning (thesis, dissertation)\n\nHead of Data Science 7 at AdoreMe Tech, with the firm since 2016\n\nLingerie e-commerce in the U.S., with a PAYG, subscription and try-at-home business models\n$250m revenue in 2022, acquired by Victoria’s Secret\n70 tech people in Bucharest, over 300 employees\nMy work involves AI Strategy, Product Management, ML Systems Design\n\nSome typical applications of AI in an e-commerce I contributed to:\n\nDemand Planning and Inventory Optimization systems to prevent lost sales and excess inventory\nRecommender Systems to help users find relevant, personalized items and bundles\nMarketing optimizations in Acquisition/Advertisement, Engagement, CRM, Merchandising, Pricing and Promotion\n\nHobbies:\n\nReading about “Philosophy as a Way of Life” & Cognitive Science\nPainting, Hiking, Jazz/Classical/Metal\nPlayed chess professionally, but that much stress isn’t worth it\n\n\n7 You can reach to me on Linkedin here, although the state of the platform drifted a long time ago from the professionals’ network to business influencers’ and vendors’ social media"
  },
  {
    "objectID": "index.html#course-roadmap",
    "href": "index.html#course-roadmap",
    "title": "Pragmatic Data Science",
    "section": "Course Roadmap",
    "text": "Course Roadmap\nWe start the course with a first lecture, in which we explore the context of data science in businesses, figure out what does AI mean, and where is it useful. Then, we review important statisical concepts and tools, exapting/repurposing them for our objectives, while remembering the hell we needed those in the first place.\nNext, we move on to A/B testing and towards a mindset of causal inference. Once you get a taste of decisions with high stakes, we switch to a predictive, Machine Learning perspective and walk through our workhorse models, which should serve us decades ahead in a wide range of applications: both predictive and exploratory.\nDid you get comfortable with ML? Good, because it’s time to acknowledge the limitations and get more powerful tools from causal inference, when A/B tests and randomised experiments are unfeasible or unethical, but we can leverage naturally-occuring experiments. This is truly challenging: it is an art and science, in contrast with the auto-magic pattern recognition of ML. It requires deep thinking and understanding.\nThe icing on the cake is miscellaneous topics dear to me and usually not covered in such a course: Demand Forecasting, Recommender Systems, and Natural Language Processing. All extremely useful in business contexts, but significant tweaks are needed to the models discussed before.\n\n\n\n\n\nflowchart TD\n  DSc(Decision-Making Under Uncertainty at Scale) -- understanding  --> Stat(Stat Fundamentals) \n  DSc -- business context  --> VUCA[V.U.C.A.] --> App(Applications) --> R[Role in firms] --> PML(ML Process) --> PS(Stat process)\n  Stat --> Prob(Probability) --> Es(Estimators) --> Boot(Bootstrap) --> AB(A/B Testing) -- optional --> ExpD[/Causality/]\n\n  DSc -- skills/labs  --> Eng(Engineering) --> Data(Data Wrangling) --> Repr[Reproducibility] --> Pipe(Pipelines) -- advanced --> FS[/Full Stack Apps/]\n  ML --> Class(Bayes Classifiers) --> GLM(GLMs) --> T(Tree-based Ensembles) -- advanced --> Caus(Hierarchical Models) --> Misc[/Time Series and Misc/]\n\n  DSc -- workhorse models  --> ML(Statistical Learning) --> Unsup(Dimensionality Reduction) --> Clust(Clustering) -- optional --> RS[/RecSys/] --> Text[/NLP Embeddings/]\n\n\n\n\n\nFigure 1: Don’t let the sheer diversity and breadth of topics intimidate you, as we’ll go step by step through each aspect, explaining the why and how. In one way or another, there is little you haven’t seen here, however the way we tie it all together IS challenging.\n\n\n\n\nIf you ask what is that engineering branch, you’re totally right! During the labs we’ll build from the ground up a tech stack for reproducible data analysis, model and data pipelines, culminating in a full-stack data app (with user interface, backend, database), which solves a real-world problem.\nThat is your final project for the course and something you can brag about in your portfolio and github profile. It sounds complicated, but we have the tools to make it easy for us, step-by-step. Don’t worry about getting everything right, but focus on a problem and single area from the course you’re passionate about: be it data visualization, ML, statistics or sheer engineering curiosity."
  },
  {
    "objectID": "index.html#schedule-and-admin",
    "href": "index.html#schedule-and-admin",
    "title": "Pragmatic Data Science",
    "section": "Schedule and Admin",
    "text": "Schedule and Admin\nThe final grade shouldn’t be a reason why you’re in this course, but if you need more structure, here’s the evaluation criteria below. Attendence is not mandatory, but highly beneficial for the professional development.\n\nLab 50%: for the full stack data app\n\nRisky/interesting/creative projects will be rewarded\nFocus on quality over quantity, one thing done well rather than trying to apply half of the models discussed in the course.\nPitch of at most 1 (one) page: what it does and why did you build it\n\nFinal Exam 50%: Focused on conceptual understanding:\n\nThere will be no problems to solve by hand.\nTry to be succint and clear\nThink for yourself, zero-tolerance policy for cheating\n\n\n\n\n\n\n\n\n\n\n\nCourse/Lab\nTitle\nKeywords\nComments\n\n\n\n\nC1 (09 Dec)\nData Science in Context\nAI, ML, Uncertainty, The Process\ndiscussion, use-cases\n\n\nC2 (09 Dec)\nFundamentals of Statistics\nProbability triples, Properties of Estimators\nlogistics, admin\n\n\nS1 (10 Dec)\nSet Up the Environment\nCommand Line, R, RStudio, Renv, Python, JupyterLab, Quarto, github, RStudio\na good setup is a prerequisite for painless labs\n\n\nS2 (10 Dec)\nTidyverse, warm-up\nggplot2, quarto, arrow, duckdb\npractice and learn some new packages"
  },
  {
    "objectID": "01_fundamentals/background.html",
    "href": "01_fundamentals/background.html",
    "title": "1  Data Science in Business Context",
    "section": "",
    "text": "You might’ve heard that data scientist is the sexiest job of 21st century, that AI is going to take over, Deep Reinforcement Learning models are beating people at Dota and Chess, solving almost-impossible protein-folding problems. But what does it actually mean, if we step outside the hype/buzzwords, use a plain language, and apply these ideas in a more down-to-earth, day-to-day problems and challenges in businesses?\nHow does this landscape of Data Science look like? What are the roles and jobs? What is the process for building smarter, data-driven software systems; drawing more reliable inferences and conclusions from data and theory? How does a day in data scientist’s life look like?\n\nFirst and foremost, AI is about Decision-Making, at Scale, under Uncertainty 1. You can make sense of the terminology and general confusion of terms, by reading M. I. Jordan’s brilliant article 2, which tell the history of “AI” and how this confusion arose. He also points out how many of the claims in the field, as of today are a stretch (i.e. the revolution hasn’t happened yet) 3.1 C. Kozyrkov - AI is decision-making at scale2 K. Pretz - Stop Calling Everything AI, Machine-Learning Pioneer Says3 M. Jordan - Artificial Intelligence: The Revolution Hasn’t Happened Yet\n\nWe have to recognize that data science is an umbrella term, with interdisciplinarity at its core. It draws inspiration, puts together multiple fields and continuously evolves. Despite that, we can clearly define what it does (see above), and how (depending on the perspective taken). For practical, pragmatic intents and purposes, we can distinguish 3 ways of thinking, which have to work harmoniously together, in order for a data science project to be successful:\n\nAnalytics and Data Mining - with the main goal of formulating better research questions (i.e. inspiration), find interesting and relevant stuff in massive datasets\nMachine Learning - learning from data, finding invariants/patterns, which generalize beyond the sample and training data, i.e. going from experience to expertise in an automated way\nStatistics, and especially causal inference - for making decisions with high stakes, and having greater confidence, rigor in the inferences and conclusions drawn\n\nOne has to cycle through these approaches, gain greater understanding, experience and skill in them, in order to use the appropriate tools in the right context. I recommend the following 4-part presentation 4 by Cassie Kozyrkov, so that you get a good idea of how “AI” fits into organization and decision-making process. I recommend following her and, basically reading everything she has written on medium 5.4 C.Kozyrkov - Making Friends with Machine Learning5 C. Kozyrkov (Chief Decision Scientist (Google?)) - https://kozyrkov.medium.com/\nPay close attention to the process of developing data-driven products 6 and what are the prerequisites for an AI project to be successful (or doomed from the very start). It is important not to skip the relevant steps, understand the roles of people involved: from decision-makers, to statisticians and data engineers. A good blueprint 7 for thinking about how to define and plan an AI project is given by Google’s PAIR (People and AI research group).6 C.Kozyrkov - 12 Steps to Applied AI7 People and AI Research (Google?) - Guidebook\nMake no mistake, the data science field is fascinating and the applications exciting, but as you well know from statistics, there are numerous pitfalls we can fall into. I think it is useful to demistify AI, data science, and get humble, down to earth about what it can and can’t do – the power, but also the limitations: - Just take a look at how many “AI” tools have been built to catch covid, and none helped 8 - One part of the problem is the mismatch between the real/business problem and objectives, and what models optimize for. Vincent Warmerdam brilliantly explains it in “The profession of solving the wrong problem”9 and “How to constrain artificial stupidity” 10.8 W.Heaven - Hundreds of AI tools have been built to catch covid. None of them helped.9 V. Warmerdam - The profession of solving the wrong problem10 V. Warmerdam - How to Constrain Artificial Stupidity\nAs a last, shameless plug, I’ll reference you to my conference talk 11, which shows how can AI add value to organizations, by taking a pragmatic approach to it. I introduce the idea of “Full Stack Data Apps”, which is critical in making the vision of decision-making under uncertainty at scale, a reality, or at least, give it a greater probability of succeeding (the definition of success being that it is used and useful).11 M. Bizovi - Pragmatic AI in Google Cloud Platform"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html",
    "href": "01_fundamentals/stat_foundations.html",
    "title": "2  Statistical Foundations",
    "section": "",
    "text": "In order to be a successful Data Scientist, one has to “speak the language” of probability and statistics; it is the basis on which we can build towards more realistic and avanced models, with the purpose of improving decision-making under uncertainty. This foundation is a prerequisite for all 3 perspectives: analytics/mining, machine learning and causal inference.\n\nA word of encouragement is that this doesn’t have to be boring! We can put “the classics” in context of modern statistics, big data challenges, and use simulation instead of heavy mathematics and proof-based approaches.\n\nTherefore, here’s a guide on how to refresh the statistical knowledge, in such a way, that statistics becomes fascinating, not boring and frustrating.\n\nRealize that lots of common statistical tests are particular versions of linear models 1. It takes a few hours to go through the theory and the code in the referenced book.\nUnderstand the following concepts:\n\nRandom Experiments, Contructing the probability triple, Random variables and CDFs\nCollectivity, Statistical Population, Samples and Sampling (as a process)\nParameter, Estimator, Estimation: properties of estimators (i.e. what does a statistician want)\nHere’s the approach taken by 2, which integrates data analysis and simulation in learning of statistics.\n\nUpgrade your statistical thinking for the 21st century challenges and be aware of the pitfalls and problems in the field. A good reference is 3, which takes multiple perspectives (frequentist, bayesian, causal inference), while going through the workhorse models and methods.\nBe comfortable with exploring, wrangling, visualizing and analyzing data in R or/and Python, get familiar with reproducible research best practices. A good starting point is 4, which is a course in collaboration with the RStudio team.\n\n1 Doogue - Common statistical tests are linear models2 Speegle, Clair - Probability, Statistics, and Data: A fresh approach using R3 Poldrack - Statistical Thinking for the 21st Century has two companion books, one with R and another in Python4 Bryan - STAT 545 It is also notable for its focus on teaching using modern R packages, Git and GitHub, its extensive sharing of teaching materials openly online, and its strong emphasis on practical data cleaning, exploration, and visualization skills, rather than algorithms and theory.There are a gazillion books, courses on statistics, which basically do/teach the same thing. For reference and your curiosity, I curated a few which stand out with the right balance of data, code, simulation, theoretical rigor and real-world applications: - 5 Cetinkaya-Runde goes through all the fundamentals in a clear, concrete, extensive way, with code! - 6 has an interesting approach, focused on the challenges of Psychology - 7 goes through the whole process, with R, with additional topics, normally not present in a statistics course - 8 is for biologists, but we can see how central to the field are multidimensional methods, clustering and high-performance computing, working with big and messy data5 Cetinkaya-Runde, Hardin - Introduction to Modern Statistics6 Crump - Answering questions with data – introductory statistics for Psychology Students7 Thulin - Modern Statistics with R8 Holmes, Huber - Modern Statistics for Modern Biology"
  },
  {
    "objectID": "01_fundamentals/lab_01.html",
    "href": "01_fundamentals/lab_01.html",
    "title": "3  Lab: tidyverse and ecosystem of packages",
    "section": "",
    "text": "RMarkdown and Quarto\ntidyverse and ggplot\narrow, duckdb and organizing the files\nrenv and reproducibility\n\n\n\n\n\n\nflowchart LR\n  A[qmd] --> B(Knitr)\n  A[qmd] --> C(Jupyter)\n  B(Knitr) --> D[md]\n  C(Jupyter) --> D[md]\n  D[md] --> E(pandoc)\n  E(pandoc) --> F(HTML)\n  E(pandoc) --> G(PDF)\n\n\n\n\n\nFigure 3.1: How Quarto orchestrates rendering of documents: start with a qmd file, use the Knitr or Jupyter engine to perform the computations and convert it to an md file, then use Pandoc to convert to various file formats including HTML, PDF, and Word."
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "4  Background",
    "section": "",
    "text": "You might’ve heard that data scientist is the sexiest job of 21st century, that AI is going to take over, Deep Reinforcement Learning models are beating people at Dota and Chess, solving almost-impossible protein-folding problems. But what does it actually mean, if we step outside the hype/buzzwords, use a plain language, and apply these ideas in a more down-to-earth, day-to-day problems and challenges in businesses?\nHow does this landscape of Data Science look like? What are the roles and jobs? What is the process for building smarter, data-driven software systems; drawing more reliable inferences and conclusions from data and theory? How does a day in data scientist’s life look like?\n\nFirst and foremost, AI is about Decision-Making, at Scale, under Uncertainty 1. You can make sense of the terminology and general confusion of terms, by reading M. I. Jordan’s brilliant article 2, which tell the history of “AI” and how this confusion arose. He also points out how many of the claims in the field, as of today are a stretch (i.e. the revolution hasn’t happened yet) 3.1 C. Kozyrkov - AI is decision-making at scale2 K. Pretz - Stop Calling Everything AI, Machine-Learning Pioneer Says3 M. Jordan - Artificial Intelligence: The Revolution Hasn’t Happened Yet\n\nWe have to recognize that data science is an umbrella term, with interdisciplinarity at its core. It draws inspiration, puts together multiple fields and continuously evolves. Despite that, we can clearly define what it does (see above), and how (depending on the perspective taken). For practical, pragmatic intents and purposes, we can distinguish 3 ways of thinking, which have to work harmoniously together, in order for a data science project to be successful:\n\nAnalytics and Data Mining - with the main goal of formulating better research questions (i.e. inspiration), find interesting and relevant stuff in massive datasets\nMachine Learning - learning from data, finding invariants/patterns, which generalize beyond the sample and training data, i.e. going from experience to expertise in an automated way\nStatistics, and especially causal inference - for making decisions with high stakes, and having greater confidence, rigor in the inferences and conclusions drawn\n\nOne has to cycle through these approaches, gain greater understanding, experience and skill in them, in order to use the appropriate tools in the right context. I recommend the following 4-part presentation 4 by Cassie Kozyrkov, so that you get a good idea of how “AI” fits into organization and decision-making process. I recommend following her and, basically reading everything she has written on medium 5.4 C.Kozyrkov - Making Friends with Machine Learning5 C. Kozyrkov (Chief Decision Scientist (Google?)) - https://kozyrkov.medium.com/\nPay close attention to the process of developing data-driven products 6 and what are the prerequisites for an AI project to be successful (or doomed from the very start). It is important not to skip the relevant steps, understand the roles of people involved: from decision-makers, to statisticians and data engineers. A good blueprint 7 for thinking about how to define and plan an AI project is given by Google’s PAIR (People and AI research group).6 C.Kozyrkov - 12 Steps to Applied AI7 People and AI Research (Google?) - Guidebook\nMake no mistake, the data science field is fascinating and the applications exciting, but as you well know from statistics, there are numerous pitfalls we can fall into. I think it is useful to demistify AI, data science, and get humble, down to earth about what it can and can’t do – the power, but also the limitations: - Just take a look at how many “AI” tools have been built to catch covid, and none helped 8 - One part of the problem is the mismatch between the real/business problem and objectives, and what models optimize for. Vincent Warmerdam brilliantly explains it in “The profession of solving the wrong problem”9 and “How to constrain artificial stupidity” 10.8 W.Heaven - Hundreds of AI tools have been built to catch covid. None of them helped.9 V. Warmerdam - The profession of solving the wrong problem10 V. Warmerdam - How to Constrain Artificial Stupidity\nAs a last, shameless plug, I’ll reference you to my conference talk 11, which shows how can AI add value to organizations, by taking a pragmatic approach to it. I introduce the idea of “Full Stack Data Apps”, which is critical in making the vision of decision-making under uncertainty at scale, a reality, or at least, give it a greater probability of succeeding (the definition of success being that it is used and useful).11 M. Bizovi - Pragmatic AI in Google Cloud Platform"
  },
  {
    "objectID": "lab_02.html",
    "href": "lab_02.html",
    "title": "5  Lab: tidyverse and ecosystem of packages",
    "section": "",
    "text": "RMarkdown and Quarto\ntidyverse and ggplot\narrow, duckdb and organizing the files\nrenv and reproducibility"
  }
]