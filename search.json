[
  {
    "objectID": "index.html#welcome-to-the-course",
    "href": "index.html#welcome-to-the-course",
    "title": "Pragmatic Data Science",
    "section": "Welcome to the course!",
    "text": "Welcome to the course!\nThis is an introductory, big picture graduate course which bridges the gap between theory and practice, cultivating the skills and understanding necessary to bring value to organisations by improving decision-making. It is an attempt to find a golden mean of both worlds:\n\nIlluminating theoretical ideas (contemplating in the library)\nPracticing battle-tested technologies (engineering in the trenches).\n\n\n\n\n\n\n\nWho should read this book?\n\n\n\n\nAnyone lost, confused, stuck or overwhelmed by Data Science and Machine Learning complexities, who wants to see the big picture and the possibilities\n\nIf you stumbled upon this e-book, you’re probably a student in Business Analytics at Bucharest’ Academy of Economic Sciences – well, because I shamelessly promoted it.\nMaybe, you’re an engineer getting curious about ML or an analyst with a knack for the business, looking to improve your workflow and expand the quantitative toolbox. Maybe you’re a product manager or an entrepreneur who wants to infuse AI into your startup.\n\n\nThis is the course I wish I had when starting my journey in data science, which would prepare me for the realities of the industry, often very different from the academic world. At the same time, this is NOT a bootcamp: it is not enough by itself to land you a job, which requires lots of practice and extra study.\nTo the service of understanding, it becomes quite abstract and conceptual at times, but I hope you bear with me until you see the benefits of those abstractions. Think of it as a skeleton, a conceptual frame1 which ties together everything you have learned so far and can be built upon as you progress in your carreer and studies.1 This course stands on the “shoulders of giants”, and I can only aspire to get to the level of clarity and rigor provided by Hastie/Tibshirani or Andrew Ng. However, there is too much content out there, and the roadmap provided here should help you navigate it and find the shortest path towards better decisions in your firm."
  },
  {
    "objectID": "index.html#why-should-you-care",
    "href": "index.html#why-should-you-care",
    "title": "Pragmatic Data Science",
    "section": "Why should you care?",
    "text": "Why should you care?\nYou might’ve heard that data scientist is the sexiest job of 21st century, that AI is going to take over repetitive jobs, Deep Reinforcement Learning models are beating people at Dota and Chess, solving almost-impossible protein-folding problems. But what does it actually mean, if we step outside the hype and buzzwords, use a plain language, and apply these ideas in a more down-to-earth, day-to-day problems and challenges in businesses?\nOver the past 3 years, you’ve probably been tortured by (or enjoyed) linear algebra, mathematical analysis, probability and statistics, operations research, differential equations, mathematical economics and cybernetics, algorithms and data structures, databases, object-oriented programming, econometrics and so on.\n\n\n\n\n\n\nWhy study all of this?\n\n\n\nWe live in a volatile, uncertain, complex and ambiguous world,2 but we still have to make decisions. Those decisions will bring better outcomes if they are informed by understanding the causal processes, driven by evidence and robust predictions. For a more complete explanation, read here.2 VUCA: a mental model to better understand the world\nI want you to take away ONE thing, that is AI and Data Science in Businesses boils down to: Decision-Making under Uncertainty at Scale\n\n\nIt can be a function of decision-making support or the system/product itself, like in the case of Uber, Amazon, Netflix, Spotify, Google and many others. Even if you are not a data scientist, you will work with them in one form or another (Quant, Data Analyst, Business Analyst, ML Engineer, Data/BI Engineer, Decision-Maker, Domain Expert). Therefore, you have to understand their language, what are they doing, how to ask and make sure they solve the right problem for you.\n\nWhen somebody asks you what have you learned in this book and course, I suggest two metaphors:3 one of simplification and another of seeing relations3 Due to my recent philosophical readings, some of the language here has a very specific meaning, which is totally outside the scope. For the curious, I will write a few blog posts outlining some philosophical arguments which struck a chord in me and which I found tremenously useful in day-to-day life.\n\n\n\n\n\n\n\n\n\nWe see Pollock’s messy reality, which is the data and observations. We want to get to Picasso’s bare bones essence, for better and clearer decision-making\n\n\n\n\n \n\n\n\n\n\nThis is a big picture course, which re-contextualizes everything you have learned before, but didn’t see how it all fits together or can it be implemented in practice to bring value to organisations, that is: be useful"
  },
  {
    "objectID": "index.html#cybernetics-done-right",
    "href": "index.html#cybernetics-done-right",
    "title": "Pragmatic Data Science",
    "section": "Cybernetics Done Right",
    "text": "Cybernetics Done Right\n\n\n\n\n\n\nWarning\n\n\n\nThe idea behind Cybernetics curriculum is great: an interdisciplinary approach to solving complex economic problems, that is what we know today as weak/specialised AI4.4 Or data science, if you wish – at the intersection of computer science, domain expertise (economics), statistics and mathematics. We will dive in more detail and nuance in our first lectures.\nUnfortunately, the execution is extremely difficult to pull off without a consistent vision, expertise and an account of recent developments. It is even more difficult to put those 80’s models in practice, especially in a data-driven age.\n\n\nLet’s go back to the burning question: what I wish I had in a course? It is hard to teach these topics in a way which is both theoretically sound and can be immediately applied.\nAfter recognizing that there is no silver bullet, my conclusion is that following the principles outlined below consistently, dramatically increases the chances of preparing a new generation ready to tackle the messy, ill-defined problems we encounter.\n\nProvide motivation for why something is important (a field, theory, model, method, technology)\nDiscuss practical applications and challenges the firms face, from an insider’s perspective:\n\nApplications are based on realistic or real data, which can be messy, hard to access, biased and incomplete\nCoding up the solutions and software implementations, with the real-world challenges of deploying models and improving decision making\n\nDevelop a conceptual undestanding, an intuition about the problem and the “tool” we think is appropriate in tackling it:\n\nPresent the tool theoretically rigorous and sound, but only where it matters\nLeverage previous mathematical, statistical, and domain knowledge\nFor the mathematically inclined, add some elements of abstract math to understand the underlying foundations of these methods and models\n\nUse simulations as a safe playground 5 that we control, to get a feel for the behavior of models and algorithms.\n\nImplement those in code using best practices, as an engineering exercise\nWith lots of visualization, especially interactive ones\n\n\n5 I often find myself truly understanding something, only after I code it up and understand the “mechanics” (of a model) well, then try to think of how I would apply it in practice, in different contexts. This hints to the idea that we need some complementary background and tools, some of them right from the beginning of higher education."
  },
  {
    "objectID": "index.html#your-unorthodox-guide",
    "href": "index.html#your-unorthodox-guide",
    "title": "Pragmatic Data Science",
    "section": "Your Unorthodox Guide",
    "text": "Your Unorthodox Guide\nThink of me as an industry expert,6 not an academic pushing you to do homework. Think of the course as a summer school or workshop, rather than required credits and another exam to pass.6 \n\n\n\n\nSee one of my conference talks at BigDataWeek, intended for a mixed tech/business audience: Pragmatic AI in Google Cloud Platform\n\n\n\nI’m a generalist – navigating uncertainty and complexity to improve decision-making at scale with data and domain-driven software. Here are a few relevant facts about me:\n\nGraduate of Cybernetics (2017) and Quantitative Economics (2019)\n\nDid some research in Complex Systems, Agent-Based Modeling, Systems’ Dynamics, and Heterodox Economics, which was lots of fun\nMy serious work was at the intersection of Bayesian Statistics and Machine Learning (thesis, dissertation)\n\nHead of Data Science 7 at AdoreMe Tech, with the firm since 2016\n\nLingerie e-commerce in the U.S., with a PAYG, subscription and try-at-home business models\n$250m revenue in 2022, acquired by Victoria’s Secret\n70 tech people in Bucharest, over 300 employees\nMy work involves AI Strategy, Product Management, ML Systems Design\n\nSome typical applications of AI in an e-commerce I contributed to:\n\nDemand Planning and Inventory Optimization systems to prevent lost sales and excess inventory\nRecommender Systems to help users find relevant, personalized items and bundles\nMarketing optimizations in Acquisition/Advertisement, Engagement, CRM, Merchandising, Pricing and Promotion\n\nHobbies:\n\nReading about “Philosophy as a Way of Life” & Cognitive Science\nPainting, Hiking, Jazz/Classical/Metal\nPlayed chess professionally, but that much stress isn’t worth it\n\n\n7 You can reach to me on Linkedin here, although the state of the platform drifted a long time ago from the professionals’ network to business influencers’ and vendors’ social media"
  },
  {
    "objectID": "index.html#course-roadmap",
    "href": "index.html#course-roadmap",
    "title": "Pragmatic Data Science",
    "section": "Course Roadmap",
    "text": "Course Roadmap\nWe start the course with a first lecture, in which we explore the context of data science in businesses, figure out what does AI mean, and where is it useful. Then, we review important statisical concepts and tools, exapting/repurposing them for our objectives, while remembering the hell we needed those in the first place.\nNext, we move on to A/B testing and towards a mindset of causal inference. Once you get a taste of decisions with high stakes, we switch to a predictive, Machine Learning perspective and walk through our workhorse models, which should serve us decades ahead in a wide range of applications: both predictive and exploratory.\nDid you get comfortable with ML? Good, because it’s time to acknowledge the limitations and get more powerful tools from causal inference, when A/B tests and randomised experiments are unfeasible or unethical, but we can leverage naturally-occuring experiments. This is truly challenging: it is an art and science, in contrast with the auto-magic pattern recognition of ML. It requires deep thinking and understanding.\nThe icing on the cake is miscellaneous topics dear to me and usually not covered in such a course: Demand Forecasting, Recommender Systems, and Natural Language Processing. All extremely useful in business contexts, but significant tweaks are needed to the models discussed before.\n\n\n\n\n\nflowchart TD\n  DSc(Decision-Making Under Uncertainty at Scale) -- understanding  --> Stat(Stat Fundamentals) \n  DSc -- business context  --> VUCA[V.U.C.A.] --> App(Applications) --> R[Role in firms] --> PML(ML Process) --> PS(Stat process)\n  Stat --> Prob(Probability) --> Es(Estimators) --> Boot(Bootstrap) --> AB(A/B Testing) -- optional --> ExpD[/Causality/]\n\n  DSc -- skills/labs  --> Eng(Engineering) --> Data(Data Wrangling) --> Repr[Reproducibility] --> Pipe(Pipelines) -- advanced --> FS[/Full Stack Apps/]\n  ML --> Class(Bayes Classifiers) --> GLM(GLMs) --> T(Tree-based Ensembles) -- advanced --> Caus(Hierarchical Models) --> Misc[/Time Series and Misc/]\n\n  DSc -- workhorse models  --> ML(Statistical Learning) --> Unsup(Dimensionality Reduction) --> Clust(Clustering) -- optional --> RS[/RecSys/] --> Text[/NLP Embeddings/]\n\n\n\n\n\nFigure 1: Don’t let the sheer diversity and breadth of topics intimidate you, as we’ll go step by step through each aspect, explaining the why and how. In one way or another, there is little you haven’t seen here, however the way we tie it all together IS challenging.\n\n\n\n\nIf you ask what is that engineering branch, you’re totally right! During the labs we’ll build from the ground up a tech stack for reproducible data analysis, model and data pipelines, culminating in a full-stack data app (with user interface, backend, database), which solves a real-world problem.\nThat is your final project for the course and something you can brag about in your portfolio and github profile. It sounds complicated, but we have the tools to make it easy for us, step-by-step. Don’t worry about getting everything right, but focus on a problem and single area from the course you’re passionate about: be it data visualization, ML, statistics or sheer engineering curiosity."
  },
  {
    "objectID": "index.html#schedule-and-admin",
    "href": "index.html#schedule-and-admin",
    "title": "Pragmatic Data Science",
    "section": "Schedule and Admin",
    "text": "Schedule and Admin\nThe final grade shouldn’t be a reason why you’re in this course, but if you need more structure, here’s the evaluation criteria below. Attendence is not mandatory, but highly beneficial for the professional development.\n\nLab 50%: for the full stack data app\n\nRisky/interesting/creative projects will be rewarded\nFocus on quality over quantity, one thing done well rather than trying to apply half of the models discussed in the course.\nPitch of at most 1 (one) page: what it does and why did you build it\n\nFinal Exam 50%: Focused on conceptual understanding:\n\nThere will be no problems to solve by hand.\nTry to be succint and clear\nThink for yourself, zero-tolerance policy for cheating\n\n\n\n\n\n\n\n\n\n\n\nCourse/Lab\nTitle\nKeywords\nComments\n\n\n\n\nC1 (09 Dec)\nData Science in Context\nAI, ML, Uncertainty, The Process\ndiscussion, use-cases\n\n\nC2 (09 Dec)\nFundamentals of Statistics\nProbability triples, Properties of Estimators\nlogistics, admin\n\n\nS1 (10 Dec)\nSet Up the Environment\nCommand Line, R, RStudio, Renv, Python, JupyterLab, Quarto, github, RStudio\na good setup is a prerequisite for painless labs\n\n\nS2 (10 Dec)\nTidyverse, warm-up\nggplot2, quarto, arrow, duckdb\npractice and learn some new packages"
  },
  {
    "objectID": "01_fundamentals/background.html#data-science-in-the-wild",
    "href": "01_fundamentals/background.html#data-science-in-the-wild",
    "title": "1  Data Science in Business Context",
    "section": "1.1 Data Science in the Wild",
    "text": "1.1 Data Science in the Wild\nAt this point, you might get tired of me emphasizing the decision-making aspect of data science as the main point of why it is important. It’s time we move from the general and abstact towards particular examples and applications in various industries. This will lead us to acknowledge how prevalent is AI (that we haven’t fully defined yet) in firms, services and technologies we use every day. 55 When reading this section, I want to get you into a mindset of the reverse engineer: step back and think deeply about products and services you use every day, put yourself in the shoes of the business making those decisions and building those systems. What were they thinking about? What challenges were they facing that were an appropriate use-case for ML, Statistics, and AI?\n\n\n\n\n\n\nDuring a lecture, I usually ask students\n\n\n\nCan you give some examples of businesses, sevices, technologies, problems, and domains which you suspect do have AI/ML algorithms and models behind the scenes?\nSee below some very good answers and argumentations provided by the students last year, then we’ll examine in more details one of them. Of course, there is always that one person, very passionate about sports or blockchain.\n\n\n\nDynamic pricing in Bolt and Uber, which takes into account the weather, especially if it rains, peak hours: balancing demand and supply. It is at the intersection of ML and economic theory, as they are a platform or marketplace. Prices also change with respect to competitors, so we see aspects of an oligopoly behavior. 6\n\nStock markets and trading bots: at the intersection of economics, finance and AI. I would add the “good old” and boring portfolio management and venture capital enterprises.\nManagement consulting: what market to enter, whether and how to build a new product (product development). Lots of use-cases in marketing and market research firms.\nMedicine Applications: developing new vaccines and drugs, aided by AI and designing clinical trials for novel treatments.\nBanks and insurance: risk management, predicting credit defaults on mortgages and business loans. Chatbots for customer support, for most frequent and simple questions.\nAutomotive: routine tasks like automated parking, the race towards self-driving, autonomous or semi-autonomous cars, safety warnings. Predictive maintainance is tackling a problem where they leverage predictions to replace risky parts before they go out of function.\nLiverpool F.C. won a title, and a key part of their success was leveraging AI and ML to discover new tactis on the field with the highest payoffs. 7\nNBA teams invested a lot in the data infrastructure and decision-making capabilities: LA Lakers found the best player at the moment for a particular position they were lacking and would play well along with the team. Rockets won the regular championship divison by going all in on the 3-point shot. Golden State Warriors simply revolutionised basketball with data, before everyone else was doing it – giving them a competitive edge. 8\n\n6 When it doesn’t work out – I’m pretty upset at their data scientists and domain experts. Here is where ethical issues creep up: jacking up prices, monopolies, drivers struggling to make a living wage.7 TODO: Reference article and maybe dataset for more details8 TODO: Reference to thinking basketball and other sports’ science resourcesIn all of the examples above, those businesses and systems do have to make decisions, under uncertainty from multiple sources, trying to solve complex problems at a large scale, which would be impossible to do manually even with an army of employees.\nI would like to add a few more examples, from an insider and practitioner’s perspective, which might not be as impressive and a bit routine, but no less important. Keep in mind, that if at a closer look, the service seems to do something relatively intelligent very fast, specialized AI might be involved behind the scenes.\n\nDemand Planning: How many SKUs (items) should I order for manufacturing, to satisfy the demand (that last item on the shelf, minimizing lost sales) and to minimize excess inventory.\nLogistics and Supply Chain: routing, distribution center operations and automations for order fulfillment, return management\nRecommender systems for music, videos, books, products, news in social media, services, platforms, and e-commerces like facebook, instagram, tiktok, youtube, spotify, amazon, emag. You can find recommendations in surprising places, like google maps.\nProgrammatic Advertisement: finding best placement for ads on various platforms, right now dominated by meta and google\n\n\n\n\n\n\n\nCheck out the Case Studies for a Deeper Understanding\n\n\n\nWe will explore some challenges and applications from this list in a series of case studies and labs. The idea is to improve our ability to identify opportunities and formulate problems from the point of view of an organisation, such that we can match those with the methods, models and algorithms discussed in the course.\nI’ll have to introduce a lot of new concepts when the language we developed so far will turn out to be insufficient to talk about and understand what’s happening inside these firms. Thus, each case study is an opportunity to play around with a novel idea. 99 If you have a pretty good idea about what is AI, Analytics, ML, Deep Learning, Big Data, Causal Inference and when to use one approach or another, feel free to skip the history and terminology and go straight to the case studies.\n\nIn the first deep-dive, we will look at Uber and Bolt, with publicly available information, trying to figure out what do their data scientists do.\nThen, we will look at the lingerie e-commerce where I work at, AdoreMe and a different set of problems we’re facing."
  },
  {
    "objectID": "01_fundamentals/background.html#what-is-ai-data-science-cybernetics",
    "href": "01_fundamentals/background.html#what-is-ai-data-science-cybernetics",
    "title": "1  Data Science in Business Context",
    "section": "1.2 What is AI, Data Science, Cybernetics?",
    "text": "1.2 What is AI, Data Science, Cybernetics?\nIt’s undeniable that there is a lot of excitement when it comes to AI, ML, and data science – to the point of calling it the sexiest job of 21st century. Data science is an umbrella term, with interdisciplinary at its core, drawing inspiration from multiple fields, sets of tools, practices, methods and it continuously evolves. 10 It is designed to help us tackle increasingly more complicated problems at a large scale. There is also a reasonable worry about ways in which these systems can go wrong or awry.10 There are many questions still unanswered: How does this landscape of Data Science look like? What are the roles and jobs? What is the process for building smarter, data-driven software systems; drawing more reliable inferences and conclusions from data and theory? How does a day in data scientist’s life look like?\nYou will often see a Venn diagram where data science sits at the intersection of mathematics – statistics, computer science – software engineering, and domain knowledge. I think this is not sufficient to characterise data science, therefore, will try to elaborate what it does, and how (which is as important).\n\n\n\n\n\n\nAI in a Nutshell\n\n\n\nFor all pragmatic intents and purposes, especially in businesses, AI is about Decision-Making under Uncertainty at Scale 11.11 C. Kozyrkov - AI is decision-making at scale\nOne important keyword here is uncertainty, as there is no point in building AI solutions based on complex models if we don’t have uncertainty. We have to be able to change our mind and actions in the face of evidence.\nOn the other hand, scale is the reason ML and Deep Learning is so powerful, because you can take lots of small decisions in an automated way, with little curation or guidance from humans. This is why many traditionally “paperwork” industried like legal and accounting embrace digitalisation and automation now.\n\n\nUltimately, why would I build a system which predicts demand for products in a direct-to-consumer ecommerce like Allbirds, Macy’s, or AdoreMe? Either in a marketplace like Emag or Amazon? Why would I try to find out the factors which contribute to a successful advertisement?\n\n\n\n\n\n\nHere are some answers from students\n\n\n\n\nIn order to allocate resources to the stuff which generates growth and profit. Avoid being scattered around (which I would call bad strategy), resulting in costs over targets and inefficiency.\nIn short, we attempt to allocate resources and efforts efficiently.\nWe can view information as a competitive advantage, anticipating and predicting so that we can plan and prepare, outperform competitors.\n\n\n\nWhen we talk about uncertainty, it’s important to recognize its sources: 12 one coming from incomplete information, that we always work with samples in one way or another. For example, even if at a certain point in time we might have real-time data, everything evolves and quickly becomes outdated. Everything is in a state of flux and change. Even in the current state, we don’t know for sure where we stand – sometimes, in economics, this problem is called nowcasting. When talking about the future, making a good prediction is one of the most difficult things.12 We will talk more formally about sources of uncertainty in the next lecture, while reviewing the fundamentals of statistics.\nFor example, who would’ve predicted the pandemic and all its implications on the supply chain and society? It’s important to note the difference between this kind of black swan events and the irreducible, fundamental uncertainty, which can’t be captured by any explanatory factors.\nIn a happy case, we can quantify and reduce it by conditioning the model, that is, joint distribution of random variables with a given structure, on data. That would result in inferences and evidence with respect to our hypothesis and model of the world.13 At the very least we can try to quantify how uncertain are we.13 We want to say something intelligent about the population, technically, to generalize. However, there is ambiguity, as objectives and the meaning/semantics of data fields are not always mathematically clear or without conflict.\nSo, we still have to make decisions. Those have to have a level of robustness and resilience to shocks, in the face of uncertainty. I would go even further, to suggest that we should aim for antifragility, meaning, the system improves after a shock or negative event, but that is very hard to implement and operationalize, therefore, it falls outside the scope of this course – to the realm of systems’ design.\n\n\n\n\n\n\nWhen you don’t need AI and Statistics\n\n\n\nAs a though experiment, itmagine we have an equation or program, with well-defined rules, which perfectly predicts the price on stock markets, or perfectly predicts how many items will a client buy and how she will respond if we change the price (an intervention). We won’t need machine learning, causal inference, or AI there.\nOf course, we don’t have that kind of program. It’s only somewhat true in cases when we have a well-tested theory, which stood the test of time and went through the scientific process to become the best theory with respect to all others. For example newtonian physics, relativity, quantum mechanics, evolution.\n\n\nHowever, when we talk about human behavior, we should resist the temptation and arrogance to say that we have a well-defined theory, be it normative or positive. Our preferences change, and we can “decide” in which direction they change or persist.\nRegardless of the business we work at or own, the place in the value chain, we’ll have to deal with human behavior: customers, employees, decision-makers, engineers. We need other kind of tools to infer perceptible regularities and patterns in their behavior. We will be forced in one way or another to learn from data and observation.\n\n\n\n\n\n\nA model is a simplified representation of reality\n\n\n\nWe need models to make sense of the world around us, because it is so complex and uncomprehensible if we are to represent it faithfully in a simulation. Therefore, we focus on relevant, interestig, essential aspects to us, we simplify by baking in domain knowledge, assumptions, and data into the models and algorithms.\nSo, we can collect data, apply algorithms to train models, in order to make inferences about some relevant quantities. That will help us in making evidence-based decisions which gets us closer to our objective in an efficient way.\n\n\n\n\n\n\n\n\nWeak AI is Domain-Specific\n\n\n\nBy now, you probably figured out that we’re not talking about General AI, trying to surpass human intelligence in general reasoning and problem-solving. Thus, we’re talking about weak or specialized AI, which depends very much on the domain.\nAI in an a fashion e-commerce, like AdoreMe, where we sell lingerie, will have a very different flavor from the tools and methods used in genomics, medicine, social science or psychology.\nDespite the fact that there are a lot of shared fundamentals, when it comes to the principles of building models, it is not straightforward to take something which works in one domain and apply it in another. Significant tweaks and adaptations are needed, which are dependent on the specificities of that domain.\nThe good news is that when these transdisciplinary groups of people work together and successfully adapt a method, it is often a breakthrough in the field borrowing the theory and technology.\n\n\n\n1.2.1 Cybernetics is what we call AI\nAt this point we have a working definition of Weak AI. At a first glance it might be hard to see what does it have in common with Cybernetics and its study of Complex Adaptive Systems.\nI’m not trying to equivocate those two, but argue that weak AI is how Cybernetics evolved and is mostly used in practice now. I will give a definition from P. Novikov, which I found tremendously useful, then explain it. Can you spot the parallels of “decision-making under uncertainty at scale” in this definition?\n\n\n\n\n\n\nA better definition of Cybernetics\n\n\n\nThe science of general regularities of control and information processing in animal, machine and human (socitey)\n\n\n\n\n\n\n\n\nUnpacking this dense/abstract definition\n\n\n\n\nControl means goal-directedness, the ability to reach the goals and objectives by taking action and stirring the system towards a trajectory. The objective can also be perserving the structural-functional organization of the system itself, an autopoesis.\nInformation Processing could be pattern recoginition, perception, how you understand and model the world, what inferences do you draw, what “data inputs” are used\nGeneral regularities means what is true and plausible of control and information processing across fields and a variety of complex systems, not only in particular cases.\nAnimal refers to applications in biology, machine – in engineering, and human – in our society and behavior.\n\nIn economic cybernetics, we’re concerned with economics, society and human behavior, rather than engineering, biology, or natural science applications.\n\n\nTo explain how Cybernetics evolved into Weak AI, there is a conglomeration of fields which went a bit out of fashion and favor: Game Theory, General Systems’ Theory, Agent-Based Modeling, Systems’ Dynamics, Complexity and Chaos, Evolutionary Algorithms. This stuff is fascinating and inspired many other breakthroughs, but it is extremely difficult to implement in practice.\nSo, we kind of settled on a more pragmatic set of tools, which is dominated pattern recognition and optimisation, in one form or another trying to learn from data (ML, DL, Causality) and act optimally (Dynamic Programming, Reinforcement Learning). .\nWait. What’s going on here? Am I saying that we did a bachelor’s degree in AI under the term of Economic Cybernetics? For me, personally, after having this epiphany – everything I studied makes so much more sense in retrospective.\n\n\n\n\n\n\nThe meaning of AI changed in the meanwhile\n\n\n\nYou can make sense of the terminology and general confusion of terms, by reading M. I. Jordan’s brilliant article 14, which tells the history of “AI” and how this confusion arose. He also points out how many of the claims in the field, as of today are a stretch (i.e. the revolution hasn’t happened yet) 15.14 K. Pretz - Stop Calling Everything AI, Machine-Learning Pioneer Says15 M. Jordan - Artificial Intelligence: The Revolution Hasn’t Happened Yet\nI highly encourage you to read the articles by M. Jordan, but until then, here are a few ways people understand AI:\n\nCybernetics and Weak AI, which we discussed before\nGeneral AI is a titanic project. It interweaves with Philosophy, Cognitive Science, in order to understand what makes us intelligent and conscious. On the other hand, trying to build general-purpose problem solving machines.\nSymbolic AI, is still relevant in a few niches, especially in automated proofs and logical reasoning.\nAugmentative AI, like VR, augmenting human capabilities, human-machine interactions\n\n\n\nIn practice, if you’re a data/business analyst, ML/data engineer, data scientist, statistician, product manager – Cybernetics is a way of thinking in systems and formulate problems well. When it comes to implementation, we mostly use data and the tools, models, methods discussed in this course."
  },
  {
    "objectID": "01_fundamentals/background.html#analytics-ml-or-statistics",
    "href": "01_fundamentals/background.html#analytics-ml-or-statistics",
    "title": "1  Data Science in Business Context",
    "section": "1.3 Analytics, ML or Statistics?",
    "text": "1.3 Analytics, ML or Statistics?\nFor practical, pragmatic intents and purposes, we can distinguish 3 ways of thinking, which have to work harmoniously together, in order for a data science project to be successful:\n\nAnalytics and Data Mining - with the main goal of formulating better research questions (i.e. inspiration), find interesting and relevant stuff in massive datasets\nMachine Learning - learning from data, finding invariants/patterns, which generalize beyond the sample and training data, i.e. going from experience to expertise in an automated way\nStatistics, and especially causal inference - for making decisions with high stakes, and having greater confidence, rigor in the inferences and conclusions drawn\n\n\n\n\n\n\nChoose your own adventure\n\n\nSo, xkcd cartoon explains it very well: the first question is - are you making decisionsthis? If no and are just curious, then we have lots of methods for data exploration and we look into analytics (inspiration!!). (ref: Cassie) If we do have to make decisions, and not many, and they are strategic and important, that means we need some rigorous statistics: if we do it at scale, e.g. us doing demand planning and inventory optimization for thousand and thousands of products, hundred of k of sizes - you cannot do it manually. On the other hand, it is not one or another, not a debate between ML and stats, you need both: recognize when is the appropriate time to use one or another – so, choose your own adventure.\nOne has to cycle through these approaches, gain greater understanding, experience and skill in them, in order to use the appropriate tools in the right context. I recommend the following 4-part presentation 16 by Cassie Kozyrkov, so that you get a good idea of how “AI” fits into organization and decision-making process. I recommend following her and, basically reading everything she has written on medium 17.16 C.Kozyrkov - Making Friends with Machine Learning17 C. Kozyrkov (Chief Decision Scientist (Google?)) - https://kozyrkov.medium.com/\nPay close attention to the process of developing data-driven products 18 and what are the prerequisites for an AI project to be successful (or doomed from the very start). It is important not to skip the relevant steps, understand the roles of people involved: from decision-makers, to statisticians and data engineers. A good blueprint 19 for thinking about how to define and plan an AI project is given by Google’s PAIR (People and AI research group).18 C.Kozyrkov - 12 Steps to Applied AI19 People and AI Research (Google?) - Guidebook\nMake no mistake, the data science field is fascinating and the applications exciting, but as you well know from statistics, there are numerous pitfalls we can fall into. I think it is useful to demistify AI, data science, and get humble, down to earth about what it can and can’t do – the power, but also the limitations: - Just take a look at how many “AI” tools have been built to catch covid, and none helped 20 - One part of the problem is the mismatch between the real/business problem and objectives, and what models optimize for. Vincent Warmerdam brilliantly explains it in “The profession of solving the wrong problem”21 and “How to constrain artificial stupidity” 22.20 W.Heaven - Hundreds of AI tools have been built to catch covid. None of them helped.21 V. Warmerdam - The profession of solving the wrong problem22 V. Warmerdam - How to Constrain Artificial Stupidity\n\n\n\n\n\nAnalyst’s workflow\n\n\n\n\n\n\n\nML workflow\n\n\ne.g. pandemics – decisions at the level of country - state - country - town: what is a good one? For that, I have to set objectives. Even right now, what is the real situation? We need to make an inference, a guess. It goes witout saying about the scale of it – it’s global, if we take into account people’s mobility and movement!\nSo the first, question is: is there some kind of value proposition for AI and do you have data? If yes and yes, we’re in business. (ref: Yaser) The important thing is to clarify what do I mean by Pragmatic: something which can be done by a small team, without huge investment in resources, maybe it’s not cutting edge, maybe not beat benchmarks and be gen-purpose, but it will make the life of the organization and the people working in it much better, and of course, driving outcomes/results.\nThe idea is to get an intuition when it is a good idea to use statistics, when a more powerful tool like ML is needed (but which is more dangerous in overfitting / backfiring) and what does Analytics means (you will or did have some subjects like BI, Data Analytics). What does analytics mean?\nThe point is to find inspiration and formulate hypotheses, find something interesting and eventually useful. So, you use data, to formulate hypothesis and ask better questions. Then in the decision-making processs, these can be communicated to statisticians and decision-makers, so that they have a clearer directions and promising candidates. This inspiration can be anything from exploratory data analysis, visualization, even to unsupervised ML methods, like dim red, clustering, etc, that is, data mining, a way to automate analytics, such that it finds patterns not readily visible from simple explorations and correlations.\nThis doesn’t mean that what we found (a difference between 2 groups of clients.). It doesn’t mean we found the causal process which makes some clients more profitable than others.\nThe same machinery which makes us intelligent and flexible, makes us prone to bullshit and self-deception, self-destructive behavior. Same with overfitting and drawing wrong inferences. We can’t say anything yet about causality, but it is a tremendously important role, so there will be always a place for a good analyst in a firm.\nIn statistics, first and foremost, we care to take decisions in a safer way, with more confidence (in a very specific sense, that the relationship, the found process is causal, i.e. is true outside or sample, that is, it generalizes) and less self deception, those are important, high-stake decisions. This is why we cannot do the mining and statistics on the same set of data. We will get next to the idea of splitting your damn data, such taht they are not contaminated by all the mining. This is why we have a process for experiment design and validating models, interpreting the parameters. If that stat model passes a rigorous critique, it has greater chance of finding a real/causal pattern.\nAt the firm level, the kind of decisions that are high-stakes, are strategic, e.g. pricing policies, entering markets, etc. Those are not low-level decisions, distribution channels mix, advertisement allocation spend, whether to deploy a new recommender system.\nIn contrast, in ML we have lower-cost of mistake decisions, and there are a lot of tiny decisions. Of course, we need lots of data and a clear objective to optimize for.\nI consider it a success if at the end of the course, when you get the problem, you can juggle between these 3 perspectives (stat, ml, analytics) and can figure out in each phase of the project, where do you stand, in what bucket. ML and Data Mining is powerful, but not appropriate everywhere, so, you have to know when not to use it, or when you don’t need the stats and just playing around.\nQ: did you have time series, data analysis, econometrics, statistics. Yes, all of them Somehow, when I graduated, I got the idea that it’s about data analysis, to find something interesting in data, to make a good prediction. Here I will try you to un-teach you that, mentioning that it is just the first step in this kind of problems (the exploratory, detective work, the pattern mining).\nWhy ML, since we put so much accent onto scientific rigor and trying to infer the causal processes? Sometimes – you don’t have a theory, e.g. in recommender systems, just too complicated. Then ML is a way to go from experience (data) to expertise (program/recipe) in an automated way, eg. which makes predictions: how do you do that, with an algorithm and certain model classes (e.g. trees, regressions)."
  },
  {
    "objectID": "01_fundamentals/background.html#why-did-you-study-all-of-that",
    "href": "01_fundamentals/background.html#why-did-you-study-all-of-that",
    "title": "1  Data Science in Business Context",
    "section": "1.4 Why did you study all of that?",
    "text": "1.4 Why did you study all of that?\nWhy did we have to go through all those excruciating months doing mathematical analysis, linear algebra, probability, statistics, econometrics, operations research, and lots of economics?\nIt was very frustrating for me, because it wasn’t clear how they fit together, what is the common thread, and more importantly, what part of the theory and particular concepts would be helpful in solving the kind of problems we discussed, and which ones are designed to enhance our academic understanding.\n\n\n\n\n\n\nSounds good – doesn’t work?\n\n\n\nAn important question is what works well in practice and why. On the other hand, what is intellectually fascinating, but not at all straightforward to apply. What is a minimal or most powerful set of the prerequisites that you need?\nLet’s draw a map, stop at each field and in a sentence explain why we learned it and how it contributes to AI, Data Science, and ML. We mentioned form the very beginning that it is an interdisciplinary field, but it is not just an union of those subjects – the inspirations and tools are quite carefully picked.\n\n\n\n\n\n\n\nflowchart TD\n  LA[Linear Algebra] --> OR[Operations Research]\n  MA[Mathematical Analysis] --> OR\n  MA --> SD[Systems Dynamics]\n  %% CS[CS Algorithms] --> OR\n  \n  PT[Probability] --> MS[Statistics] --> EC[Econometrics]\n  EC --> Caus[/Causal Inference\\]\n  EC --> TS[Time Series]\n\n %% subgraph 1\n  Caus --- DM[/Data Mining\\] --- ML[/Machine Learning\\] \n  ML --- Caus\n %% end\n\n  OR --> ML\n  MS --> ML\n  MA --> PT\n  SD --> Caus\n\n  Caus --- Econ[[Economics]]  \n  Econ --- GT[Game Theory]\n  Econ --- DT[Decision Theory]\n\n  style Caus fill:#f7f5bc\n  style ML fill:#f7f5bc\n  style DM fill:#f7f5bc\n\n  DM --- FSDA[/Full-Stack Apps\\]\n  FSDA --- DB[Databases/SQL]\n  FSDA --- OOP[OOP]\n  Econ --- TS\n\n  style FSDA fill:#f7f5bc\n\n\n\n\n\nFigure 1.2: Think of this as a stuctural organization of the fields and courses you studied before. Some are more useful in analytics, some in ML and some in making causal inferences, that is, based on data + theory.\n\n\n\n\n\nLinear Algebra is a language of data. The vast majority of models and training algorithms can be reduced to operations on matrices. Therefore, it is not a coincidence that is almost the only tool we have, in order to take these models and implement them in code, on a computer.\n\nMy perspective over linear algebra is ultimately computational and geometric, in the sense of the “space” the data points live in and the transformations of that space.\nUltimately, no matter the data type: image, video, text, voice, structured, panel – all can be represented as multidimensional arrays (or tensors, if you wish).\n\nMathematical Analysis is all about change, formalizing how a function behaves with respect to its arguments and parameters. It is an essential building block in optimization and deep learning (automated differentiation).\n\nI would argue that in order to understand any complex system, be it a firm, an economy, the climate or environnment, we have to model how it evolves in time.\nThis suggests the importance of differential equations and systems dynamics, modeling the feedback loops. All of this would be impossible to reason about without the mathematical analysis.\n\nProbability and Statistics is the language of uncertainty, the only instrument we have, that allows us to say something intelligent about how confident are we.\n\nWe will explore the role of statistics at lenght, but as a quote, think of it as a tool which enables us to change our mind and decisions in the face of evidence.\n\nEconometrics, in my personal opinion, tries to separate the signal for the noise and make inferences about the causal processes in economic decisions and phenomena. It specializes statistics in the domain of economics, by infusing economic theory – because you can’t derive a scientific theory from data alone.\n\nTime Series Analysis, senso largo, bridges the gap between Systems Dynamics (which takes a more theoretical perspective) with statistics and probability (stochastic processes). It adapts those tools to make inferences and predictions about phenomena which evolves in time, that are dynamic in their nature.\n\nI like the metaphor of Data Assimilation, which is actually an entire field trying to introduce the empirical dimension to differential equations.\n\nOperations Research is about optimization with constraints and efficiency. However, the problem is that often, we start from an Integer/Linear Programming problem formulation, and that is easy part – to apply an existing algorithm. The hard part is to reduce a messy real world problem at a large scale to that formulation, especially under uncertainty and nonlinearity.\nEconomics touches upon a wide range of aspects of our society and human behavior. In mathematically-oriented courses, you can think of it as optimization with constraints, the constraints being given by our positive or normative theory of economic decision-making.\n\nIn this course and in practice, we care more about business economics. It’s a very different beast from theoretical ideas in macroeconomics (ISLM, DSGE type of models) or microeconomic utilitarianism.\nBy business economics, we mean marketing, management, corporate finance, decision theory, supply chain, and logistics.\nWhat you have to know about marketing, especially if you are skeptical like me, is that it became innovative, mathematical, rigorous and data-driven. Look at any marketing journal: how much econometrics and ML models are in there.\nSo, if you hold the opinion that marketing and management are fields full of fluff – I advise you to rethink your positions. In the context of tech firms, you can’t bullshit your way through it.\nMoreover, when you combine marketing with behavioral economics and psychology, it introduces another layer of nuance and understanding over our decision-making.\nWhen we make decisions, we like to think of ourselfs as objective, but we have lots of biases and blind spots which prevent us to see the reality clearly. We often find patterns and regularities which are just noise, not causal. So, the usefulness of this kind of domain knowledge from economics about human behavior helps us to be wiser, that is, to prevent self-deception and self-sabotage towards achieving the goals and objectives.\n\n\n\n\n\n\n\n\nA word of encouragement\n\n\n\nNone of those courses were useless. Think of how can we take parts from each of those prerequisites, which are relevant in ML/DSc, so that we have more tools to solve problems of the complexity we encounter. To reiterate, data science is an umbrella term, borrowing from them all."
  },
  {
    "objectID": "01_fundamentals/background.html#implicit-learning-intuition-and-bias",
    "href": "01_fundamentals/background.html#implicit-learning-intuition-and-bias",
    "title": "1  Data Science in Business Context",
    "section": "1.5 Implicit Learning, Intuition and Bias",
    "text": "1.5 Implicit Learning, Intuition and Bias\nI would like to give a parallel here, of how people learn. We have a powerful capacity for implicit learning, meaning we can’t articulate or explain how we did it, like riding a bike, learning to read and write. There is a famous experiment: Researchers invented words from two languages, with 2 rules, between 3-8 characters, appropriate vowels and so on and they generated 2 sets of words.\nParticipants saw the list and they had to say where do they come from (matching) – it’s a good example of experiment design coming from cogsci research. They found that subjects differentiated them much better than chance. During the interviews, when asked to explain how they did it, it was either idk, or giving some rules, when implemented in a software, were indistinguishable from chance (i.e. those rules were NOT exactly how they were thinking), meaning they couldn’t really articulate what they did there.\nThis means that we have a capacity to find patterns and regularities in the real world, due to evolution building into us this powerful machinery of implicit learning.\nAnother example: Bait Shyness (ref: Ben-David), to the idea of priors being necessary for learning, when work well Pigeon Superstition - when doesn’t go well, picks up on noise, resulting in superstitios behavior. (insert images)\nWhat is the common thread among these 3 stories: when all goes well we call it intuition, business knack, when goes awry, it’s the bias or even worse cases bigot/prejudice: confirmation bias, recency, selection bias, misjudging horison – we attribute to phenomena causal explanation when it’s not. e.g. size of wedding to nr years (social status, community) in marriage, extroversion and attractiveness with competence, due to common cause and confounders. E.g. how religious are people vs years of life - Z: education numerous, numerous example. So, a part of wisdom is the ability to differentiate between corr/causal patterns.\nClose the paranthesis from cogsci, we have big data, domain knowledge and methodologies for hypothesis testing – meaning, formal tools to deal with it.\nWe’re not here in a behavioral economic class, but we will try to cultivate the kind of active open-mindedness which try to identify those biases in us and correct our behavior and decisions. We try to see clearly and update our beliefs, keeping the kind of scout mindset, and it’s easier in a formal field with all the statistical, mathematical tools. However, let’s keep in mind how easy researchers are getting fooled (not only by randomness).\nThe good news is that sometimes, you just need a reliable prediction, as you’re not intervening in the system causing a certain phenomena – and by retraining ML models you can adapt to minor changes introduced by our interventions. Those ML models, they clearly didn’t figure out a scientific explanation of the causal process, e.g. for demand forecasting in uber, and that just the prediction to be used in a larger ecosystem and environment for decision-making.\nAgain, that is appropriate in low-stakes, large scale decisons. You might not care so much about the latent, causal process; but of course, you care that it generalizes to the population of interest (that is, a binding contract, a boundary in which your predictions are valid – if you go outside that range, can easily get absurd predictions – this is why this kind of system needs checks-and-balances, boxes to constrain the artificial stupidity). It is extraordinarily unlikely that these models translate to novel situations and environments without explicit transfer learning and careful adaptation."
  },
  {
    "objectID": "01_fundamentals/background.html#calling-bullshit-in-the-age-of-big-data",
    "href": "01_fundamentals/background.html#calling-bullshit-in-the-age-of-big-data",
    "title": "1  Data Science in Business Context",
    "section": "1.6 Calling Bullshit in the age of Big Data",
    "text": "1.6 Calling Bullshit in the age of Big Data\nSmall data problems in big data – this is not the end of theory of stats. We have lots of entities with sparse information for each ones – all about aggregation level, 100m clients, but new ones, without much information or not so many purchases. At a certain level of granularity, the data becomes, discrete, noisy, heteroskedastic.\nEven in ML, e.g. in demand forecasting, we can’t fully escape the teory (which is our understanding of the world) – we need to provide it relevant data, factors which are related, possibly influencing that demand, like weather, holidays, road blockings, factors for demand and supply. We can’t just pour all this data into ML and expect the best: garbage in garbage out, it isn’t clear that feeding irrelevant data doesn’t break our model, st. it picks up on noise and spurious correlation, esp. in very powerful DL models – doesn’t help with better decisions.\nBullshit in the formal sense, introduced by Harry Frankfurt in his essay on Bullshit. A liar respects the truth, in BS you try to convince someone of something regardless of the truth, you distract their attention, providing relevance without truth.\nIn our age, it is more sophisticated than advertisement, tv trying you to buy something, manipulating, we can call it bs in the age of big data, transforming it into smth more quant, using jargon from stats, finance, economics – when explaining why interest rates rose, what happened in 2008 gfc, lots of numbers and charts, which is intimidating.\nI hope that following this course, you will look behind this veil and use first-principles to figure this sort of stuff out. For the ones interesting, check out the calling bullshit course. It’s all about how to lie with statistics, esp. in graphics and visualization.\nThere is no magic in AI, no silve bullet, these are concepts inspired from stats, optimization, algorithms. IF we asked the right questions and formulated the problem and objectives well (which should be consistent with business outcomes), to guide us towards an answer. e.g. AI for who enters the quarantine – what do yo optimize for? Infections, hospitalizations or deaths – large discussion here? So, a problem well framed is most of the answers + clean and relevant data – only then, we can claim an element of intelligence right there. Critical thinking becomes that much more important, when we have these powerful quantitative tools at out fingerprints, that is a part of data scientist’s job is to constrain artificial stupidity (more exactly, foolishness, because it does perfectly fine what you instructed it to do) and making sure we’re solving the right problem (sounds trivial, but ofter we solve the wrong problem, without being aware of it).\nDiscounting bias The way we care about immediate things, which are supersalient – when it goes well evolutionarily, smoking - when it doesn’t add up, all the possible deaths, we underestimate the risk. Connect it to probability theory (tree with paths – estimating probabilities).\n\n\n\n\n\n\nCausal vs Correlational Patterns"
  },
  {
    "objectID": "01_fundamentals/bolt.html",
    "href": "01_fundamentals/bolt.html",
    "title": "2  Case Study: Decisions at Bolt and Uber",
    "section": "",
    "text": "We can start by looking what data scientists from Bolt are saying, and then trying to figure ot the rest in more detail. 1 Why do we have so much information available from these big companies? Well, because we like to brag and there is no “secret sauce” algorithm.1 How we do data science at Bolt - Martin Lumiste\n\npricing decisions: what factors influence it\nmatching passengers and their routes/trips to cars\nfood delivery network: routing and optimization 2\nbalancing the demand and supply. Well, how then do you do testing if the feature influences and shifts both? 3\nreputation systems: ratings, canceling rides\nadvertisement and acquisition\nengagement and CRM: churn, inactivity\nfinance, fraud prevention, charging systems 4\nentering a new market\na/b testing app features\nethical considerations\n\n\n\n2 Food delivery - Understanding the problem we’re solving3 Tips and considerations for switchback test designs4 How to stop fraud with ML — best practices at Bolt"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#the-probability-triple",
    "href": "01_fundamentals/stat_foundations.html#the-probability-triple",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.1 The probability triple",
    "text": "3.1 The probability triple\nWhy? Language of uncertainty! Applications everywhere, the only way we can reason logically under uncertainty (fuzzy logic is out of favor, better study bayes).\nOften, probability and math stats are bundled together, as they make perfect sense in sequence, but they have different objectives in mind. Prob theory is concerned with the construction of the probability triple (O, F, P), which can get very rigorous and complicated, with measure theory. We define the probability measure, and it matters a lot in which space are we (R^n, simplex, f(R^n) and gauss processes, SP stochastic processes for time series analysis) – the curious case of compositional data analysis, in which the classical methods have to be tweaked in order to make sense. Explain how we define P on a simple later – but the main point to remember, that we construct it here.\nRandom Variable!\nThree additional critical concepts are: joint probability, which is basically the storytelling behind the data generating process, conditioning (basically reducing uncertainty and adding information) and marginalization (esp of nuisance parameters – explain what are those).\n\nRandom experiment:\n\npossible outcomes are known apriori and exhaustibly, e.g. coin toss, sales (Continuous)\nwe don’t know which of those will manifest\nthere is a perceptible regularity, which can be eventually measured and quantified\nconditions of the experiment are repeatable (bayes perspective - not necessary)\n\nElementary events {w1, w2}\nSelection space: complementarity of elementary events\nRandom Events as union of elementary events, but can’t do it in any way I want.\nEvent space: A and its complement(A) has to have a certain consistency, not always possible to have all possible combinations of elementary events – technical restrictions for continuous R. Why measure theory as a solid foundation, but irrelevant here.\nProbability measure: assigning a number to an event, between 0, 1; that is, it should represent that perceptible regularity, in frequentist terms - long-run frequency of occurences. n(A) / n -> prob; it also has to respect a minimal set of conditions.\n(O, F, P) triple\nRandom variable, most interesting ideas: not random, neither a variable, it’s a function X(w) -> R, such that it perserves the informational structure of F., i.e. X^-1(r) in F or <= r in R (not any definition of X is valid, can’t be arbitrary – we can come up with a counterexampe – basically inversable, can recover the right elementary events). It’s a quantification: of Events into numbers, essential for all the following machinery of statistics, ML and so on. It is basically the thing which enables us to define the statistical population (some relevant aspect of it to us)!\n\n\n\n\n\n\nIdea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#mathematical-statistics-in-a-nutshell",
    "href": "01_fundamentals/stat_foundations.html#mathematical-statistics-in-a-nutshell",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.2 Mathematical Statistics in a nutshell",
    "text": "3.2 Mathematical Statistics in a nutshell\nimportance of the data generating process, see what Ernesto said, then the causal process – all is parameters, known or unknown\nUses everything probability has to give, but is concerned with inference, estimation, hypothesis testing, prove theorems and probabilistic properties of estimators.\n\nCollectivity (can be anything, trees in central park, pixels, people, etc, etc) – i.e stuff in the real world\nStatistical Population (binding contract)\nSample (a data collection process is involved here)\nParameter (average tree height, some aspect or characteristic of interest, unknown number/constant (bayes, a distribution), which is at population level) – of the DGP, to be estimated from our sample.\nEstimator: t_theta(X) -> theta\n\nConfidence intervals: depend on the distribution of the estimator, since X is rv, t(X) is a random variable too – and it is the job of statisticians to tell us what is that P(t(X)) and their properties: if biased, consistent, efficient\n\nEstimation/a Statistic: theta_hat (a way to summarise and synthetise data)\n\nDraw a diagram between sample and population.\nThe point of statistics: change our opinion about the action we have (or phenomenon we want to understand) to take in the face of evidence.\nOne big problem is if we got the model right for our use-case and phenomena. A wrong model choice leads to wrong conclusions. It should be informed as much by stats as by the theory and domain. Think of this in the spirit of the scientific method."
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#sources-of-uncertainty",
    "href": "01_fundamentals/stat_foundations.html#sources-of-uncertainty",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.3 Sources of Uncertainty",
    "text": "3.3 Sources of Uncertainty"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#models-are-golemns-of-prague",
    "href": "01_fundamentals/stat_foundations.html#models-are-golemns-of-prague",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.4 Models are Golemns of Prague",
    "text": "3.4 Models are Golemns of Prague\nData + Domain Assumptions + Statistical Assumptions"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#what-does-a-statistician-want",
    "href": "01_fundamentals/stat_foundations.html#what-does-a-statistician-want",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.5 What does a statistician want?",
    "text": "3.5 What does a statistician want?\nFrom their estimators"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#case-study-de-moivre-on-us-schooling",
    "href": "01_fundamentals/stat_foundations.html#case-study-de-moivre-on-us-schooling",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.6 Case Study: De Moivre on US Schooling",
    "text": "3.6 Case Study: De Moivre on US Schooling\nLet’s remember what we discusssed last time. So, the US gov, tried to find out what makes some schools better than others. They saw that in top schools, small ones were dominating (in terms of nr students, perf. being SAT ~ Norm()).\nThey decided to split up big schools into smaller ones. Can you say if it was a good, bad idea and why did they think that?\nStudents hypothesis: - more attention to students in smaller classrooms - private schools, where families are more wealthy - geographical location, i.e. small towns and villages - motivation of the underdog, from small towns - friendships, community and peer competition (easier in smaller schools) - self-selection: what kind of students go into small schools – we assume average IQ is the same, including the distribution. Assume random allocation. - Skewed (even if not, longer tails): meaning, asymmetric distributions. - Finally: variance and standard deviation\nAbout the decision sanity: reality vs data. The conservative decision is to do nothing, as restructuring costs a lot, in money and social changes & consequences, it is risky – that is, the default action, without seeing any data or evidence, we would do nothing.\nS: A good remark about having a diversity, specializations (professional schools) – can’t have a one-size-fits-it-all policy. But we ignore it, oversimplifying the problem.\nAll of that is true, but just for 10 minutes, we introduce a (false) dichotomy between small and large schools. The alternative is to split big school – remember, that outside this box, or bad framing of the problem, there are many many potential interventions (professor education, smaller classrooms, fairer allocation of students, raising up the disadvantaged hood schools).\nThe punchline: we have all these factors potentially contributing to the performance. Because of DeMoivre and LLN (mean/sqrt(sd)) – in a simulation, we notice the following thing: (insert graphs).\n\nSmall schools will dominate both the top and bottom, due to larger variance. This is what it means.\n\nSo, we go back to this idea that statistical models, are golemns, they do exactly what they are told to, and this is dangerous. We have to understand deeply in what contexts and in which ways these little robots fail and give absurd and invalid results without us knowing.\nWe will go into more detail next week in A/B testing and hypothesis testing, but here we go: once we have the default action and the alternative action, the null hypothesis is (sidenote, we don’t care about rote calculation – R packages does it for us, but we care about experiment design and decisions, try to figure out the causal influences): in which worlds will I take the default actions (worlds, meaning the values of the unknown parameter of interest, describing the behavior of the populations – in this case quantifying the impact of school size on SAT scores). The alternative hypothesis, is exactly the opposite, all the other worlds (parameter values) when I will take the alternative action.\nSo, in this case we will collect data, used to estimate the parameter and its confidence intervels (inference, with the idea of generalizing from the sample to the population). Now, given those confidence intervals, we ask how surprising it is to see the data/estimation, if the null hypothesis was true.\nMost stats courses jump over these important aspects. Their starting point is: H0, HA are given, model is given, you just compute. In practice, even for experienced statistician, for a new problem, it is very hard to define those 4 components – as it depends on the domain knowledge and business objectives. Moreover, even the interpretation of p-values is extremely tricky to communicate. We will develop tools to deal with those challenges."
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#case-study-chess-example",
    "href": "01_fundamentals/stat_foundations.html#case-study-chess-example",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.7 Case Study: Chess example",
    "text": "3.7 Case Study: Chess example\nI played chess for 14 years, professionally, question about difference in genders and what kind of tournaments and policies should we have. Why are there no women rn in top 100 (Historically, it was judit polgar in the top 20) ?\n\nUse this chance to extract data and analyse. Same problem with marathon runners (from BS big data).\n\nCauses? - Bias: inferring natural ability – nonsense from IQ research - Misoginism - Education and lack of encouragement - Lack of opportunities, community, support - Historical trajectory – lack of competition\nA lot is explained by how many boys and girls start from a given cohort/age. Similar problem, but slightly different – for any problem of rank top(n).\nWe will discuss next time a whole range of other pitfalls, including spurious correlation (nicolas cage and drowning), common cause (babies and storks), ommited variables, mediation, selection bias, reverse causality, even including predictors which shouldn’t be there. We need a statistical reasoning for all of these potential pitfalls.\n\nexplore python cli app + scraping + arrow + duckdb, see the trajectories of players and so on (chess.com vs fide). For the trajectories, mention the idea of longitudinal data, survival models, growth models, mixed models – because of the particularities of DGP and causal process\n\n\nsee the difference between ELO vs xbox true skill, how fast it converges to the true ability – that is a model we can investigate."
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#case-study-amazon-reviews-and-ratings",
    "href": "01_fundamentals/stat_foundations.html#case-study-amazon-reviews-and-ratings",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.8 Case Study: Amazon reviews and ratings",
    "text": "3.8 Case Study: Amazon reviews and ratings\nHow do you choose 4.7 (300) vs 4.5 (2000) – according to what bounds, agresti-coull, correction. What chance do you give those with a small sample size? What is the optimal strategy here?\n\nwe can guide ourselfs with the quality and relevance of reviews\nis it representative? (also, outside the box)\ntext summary\n\nQ: What is the variance and distribution, it depends, also on our risk appetites (w.r.t.) volume or if is one time (e.g. buy shoes vs coffee shops – repeated interactions).\nIntroduce the confusion matrix, which will be discussed at length in ML and hypothesis testing (types of error) – the point is that sometimes the payoff for correct decison or a mistake is asymmetric. And that matters (fraud vs reco quality) – so, that is context-dependent.\n\ngood applications for text mining, statistical tests"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#data-mining-and-crisp-dm-variants",
    "href": "01_fundamentals/stat_foundations.html#data-mining-and-crisp-dm-variants",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.9 Data Mining and CRISP-DM variants",
    "text": "3.9 Data Mining and CRISP-DM variants"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#statistical-process-and-science",
    "href": "01_fundamentals/stat_foundations.html#statistical-process-and-science",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.10 Statistical Process and Science",
    "text": "3.10 Statistical Process and Science"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#byop-bring-your-own-problem",
    "href": "01_fundamentals/stat_foundations.html#byop-bring-your-own-problem",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.11 BYOP: Bring your own problem",
    "text": "3.11 BYOP: Bring your own problem\nDiscussion over the how/process"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#ml-is-a-geocentric-model-of-the-universe",
    "href": "01_fundamentals/stat_foundations.html#ml-is-a-geocentric-model-of-the-universe",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.12 ML is a geocentric model of the universe",
    "text": "3.12 ML is a geocentric model of the universe"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#dead-salmon-a-warning-tale",
    "href": "01_fundamentals/stat_foundations.html#dead-salmon-a-warning-tale",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.13 Dead salmon: a warning tale",
    "text": "3.13 Dead salmon: a warning tale\nForward reference to power and multiple testing"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#conditioning-marginalisation-and-exchangeability",
    "href": "01_fundamentals/stat_foundations.html#conditioning-marginalisation-and-exchangeability",
    "title": "3  Statistical Foundations: WIP",
    "section": "3.14 Conditioning, marginalisation and Exchangeability",
    "text": "3.14 Conditioning, marginalisation and Exchangeability"
  },
  {
    "objectID": "01_fundamentals/adoreme.html#wip",
    "href": "01_fundamentals/adoreme.html#wip",
    "title": "4  Case Study: AI at Adore Me",
    "section": "4.1 WIP",
    "text": "4.1 WIP\nAs i said before, e-commerce and we’re pretty tech-savy: have microservices everywhere and live in google-cloud; the second important component is the people: we’re small, self-organized teams and we communicate, collaborate a lot – in every problem we try to have a cross-functional teams. Our clients are not only internal decision-making and colleagues, but also the customers. On the other hand, we want to give as much power as possible to the developers. In this case, I will talk about the 5 people in the data science team. So, a small team, serving important internal clients and millions of customers.\nAnd, our two products/applications: We have a business model, called try at home – you receive a box of products, try them on keep what you like, return what you don’t, give feedback and eliminate this cognitive load of having a choice, a way to be surprised. This is all stuff at scale – therefore the AI value prop\n\n\n\n\n\nflowchart TD\n    B[Business Challenges] --> Org[Organisational Design]\n    B --> PM[Product Management]\n    B --> Tech[Tech Ecosystem]\n\n    Org --> CN[Creative Networkers] --> DM[Decision-Makers] \n    DM --> TT[Team Topologies] --> MMT[Management 3.0]\n\n    PM --> DSS[Strategic Alignment] --> WP[Wrong Problem] --> ES[Event Storming]\n    ES --> PAIR[People + AI] --> Proc[The full process]\n\n    Tech --> FSDA[Full Stack Apps] --> RCD[Restricted Computational Domain]\n    RCD --> Agile[Agile, XP, Anarchy] --> DX[Developer Experience]\n    DX --> Pr[Principles]\n\n    MMT --> CS[Case Studies]\n    Proc --> CS\n    Pr --> CS\n\n    CS --> CF[Clustering Customer Feedback] --> DP[New Products Forecasting] --> OS[Serving Recommendations]\n    OS --> DPE[Demand Planning] --> IO[Inventory Optimization]\n    IO --> CCs[Conclusions & Contributions]\n\n  style PM fill:#f7f5bc\n  style Tech fill:#f7f5bc\n  style Org fill:#f7f5bc\n  style CS fill:#f7f5bc\n\n\n\n\n\nFigure 4.1: What is new in here? People’s dimension, Product Management The idea of tech ecosystem and full stack data apps"
  },
  {
    "objectID": "01_fundamentals/adoreme.html#organizational-design",
    "href": "01_fundamentals/adoreme.html#organizational-design",
    "title": "4  Case Study: AI at Adore Me",
    "section": "4.2 Organizational design",
    "text": "4.2 Organizational design\n\npeople and teams\n\nCreative networkers: (ref: Jurgen) The same idea is in the creative networkers (not information workers) – it’s not an algorithm which is the key, but a collaboration of developers, data engineers, with data science and decision-makers, domain experts (since we talked about how important is it to change your mind when the evidence told you so). All these people have to be strategically aligned, so one piece of wisdom I want to show next is how software systems, models help you in this alignment. (ref: DSc Safari)\n\n\n\n\n\nTeams at adore me\n\n\nWhat kind of roles are out there and what do they do? Why are the generalists valuable? - Decision Maker - Analyst - Domain Expert - Stakeholder and Client - Statistician (Dsc and other generalists) - ML Engineer - Data Engineer - BI Engineer - ML Researcher - Decision Scientist - Product Manager - UX Designer - BE/FE Engineer - Software/Cloud Architect - DevOps - Project/Program Manager: execution! - C-Level people: strategy!"
  },
  {
    "objectID": "01_fundamentals/adoreme.html#product-management",
    "href": "01_fundamentals/adoreme.html#product-management",
    "title": "4  Case Study: AI at Adore Me",
    "section": "4.3 Product Management",
    "text": "4.3 Product Management\nData Science Strategy Safari"
  },
  {
    "objectID": "01_fundamentals/adoreme.html#tech-ecosystem",
    "href": "01_fundamentals/adoreme.html#tech-ecosystem",
    "title": "4  Case Study: AI at Adore Me",
    "section": "4.4 Tech ecosystem",
    "text": "4.4 Tech ecosystem\nPrinciples and philosophy: Simple made easy (ref: Hickey) How to manage all this chaos to make better decisions - Small and simple (software design principles.) So, why the heck am I talking about software design and architecture in a talk about AI, because you cannot do one w/o another. Even though AI has its particularities, we have to learn from years and years of experience of our fellow software engineeers, product people, devops. - It has to be scalable and flexible - We also don’t want to build a lot, we want to build the minimum amount of stuff which brings max value. - Also, when we talk about AI, I believe it is our job and responsibility to constrain artificial stupidity. AI by itself is nothing, without data, objectives you set to optimize: Message against the grain of the field. Once we talk pragmatism, we have to remember the limitations of the methods that we use.\nSo, we went through the first, conceptual part: What is AI , how do we use it in org, where does this network of people collaborating together to solve challenging problems comes into. Also we talked about decisions, the problems that are too big to do it manually, meticuously, so you need automation: especially where the classical programming doesn’t help – there will be too many rules to write down in code. We also talked about principles to organize everything together.\nNow, let’s move to the more technical and interesting side for us, engineers: the Data Apps.\nRmk: don’t forget about agile data science (DevFest and Google Cloud Days in 2019)\nEven though almost every talk is about models, for example, how can we build the latest neural network architecture, RNNs, CNNs, Transformers, best architecture to solve problems at google-scale. But most of us are not google: we don’t have the data, resources and problems. So, for me in the first place comes not the modeling, but the data and software.\nFrom two perspectives: enable people to do what wasn’t possible before. Second, to augment, to take away the boring stuff, excruciating excel work, if-elses and case scenarios. But, you want to leave them with decisions they take pride in, the strategy – want to inform them and let them choose and decice. Therefore, you need software for this, the way you modeled the business and decision-making process, to introduce it in software (ref: DDD).\nSo, where does AI come into play? Let’s see next, based on 5 case studies, going from classical algos, with little stats, to complex recommender systems! We will see what I mean by full stack.\nWe have this try-at-home service: you fill in a quiz, say what you like and don’t, rate a few products, then you receive a box full with products, try it at home, return what you don’t like, some customers find this very appealing. It’s a very profitable business model, but to get there, you need lots of work.\nHow do you schedule the boxes, for example, in a monthly membership. You have to account for lots of factors: was there a delay in delivery, does a customer prefer boxes more or less often? So, on the website, we want to show customers (on the adore me website) a reliable estimate when we will curate a box for them and when are they likely to receive it.\nWhat are the specifics of this problem? The algorithm is lightweight and trades off a few things: it tries to find the best schedule for each customer. It is scheduled, no need to update real-time or be message-driven (customers don’t check frantically when their next box will be placed). We have to push a lot of stuff to the “website”, it’s batch. Data Quality is super-important here – so we’ll have to discuss how do we build reliable data pipelines, in our case, in BQ, which is google’s serverless dwh/ columnar db which is very good for analytic purposes.\nThis is how the architecture looks like: we gather all the factors together, based on which we have to make our decision (100k decisions), when do we place that box. You put it together in SQL, since there isn’t any complicated feature engineering. We use a framework called dbt (explained next) – helps us keep that pipeline structured, documented.\nOn the other hand, we have a very lightweight algorithm, hosted on cloud run – get data, compute it on a machine (but cloud run is serverless): you define your container, script, inputs, outputs, calculate the date in which you will place the order - send to microservice, which takes care of displaying it to the customer and computing the delivery estimations. Pretty straightforward.\nThere is one problem: we have lots of customers, but a limited capacity in the distribution center (one more problem, of routing), we have to put the constraint on nr of orders (some might fail), and whether we respect the constraints of the distribution center. This amounts to rescheduling some customers and routing them to another one.\nWhat we saw here – is data-driven decision(s), combined with the domain knowledge. Therefore we need good pipelines so that this data is reliable. A simplified version of what we do is in the following diagram: we have lots of sources of data, in BQ which are coming from ms, we test if our external dependencies matches the expectation (right schema, freshness of data (2 days didn’t pass without data getting updated)) – if that happens, we get an alert on slack and we can fix it.\nThen we have a transformation: this huge DAG (Directed Acyclic Graph) – it is not only for scheduling, but for the whole try-at-home algo ecosystem – so you see lots of data sources, transformation, but easily explorable and you can visualize the whole graph in an application. Because everything is written in dbt (basically you write sql/models for transforming, spec sources – compiles to sql and sends job to bq). After that, we have to run some tests on data, to make sure it’s valid, no major changes are happening – data tests, if fail we don’t promote the final input for scheduling in production – receive an alert, and see whether the cause was something technical, like broken schema, new field or there is something in the distribution of data, which overwhelms our warehouse – then we can intervene, challenge and improve the logic.\nIn the end, we have a production artifact. Usually in a project we have many many sources and a few important artifacts, based on which we build models, stat calculations, algorithm, decisions and the AI stuff.\nThe first lesson is in the importance of data: if your algorithm doesn’t have much uncertainty – you can use a classical algorithm."
  },
  {
    "objectID": "01_fundamentals/adoreme.html#full-stack-data-apps",
    "href": "01_fundamentals/adoreme.html#full-stack-data-apps",
    "title": "4  Case Study: AI at Adore Me",
    "section": "4.5 Full-stack Data Apps",
    "text": "4.5 Full-stack Data Apps\nSo, we said that people are important, systems are important, DQ is paramount, and there is no one silver bullet – so we have to think of ecosystems, which cannot exist without a reliable IT infra, a place where to host your apps and code: you need clean and up to date data, non-ambiguous, and hwich is relevant and predictive for the things you wanna explain.\nIn our case that foundation is google cloud, dwh build by amazing DE team, BI ecosystem and microservices, – so, this underground, this infrastructure supports more use-cases and now we co one conceptual level above, and we start looking at how does the ecosystem develop. You have to identify important use-cases for AI, what can bring the most value? In our case was the demand planning, and at very beginning in marketing and acquisition. Our second pillar is the recommender systems.\nSo, it’s not THE magic algorithm, it’s an ecosystem of models, tools, data and people – think of it as a landscape, city, garden and you have to build it one brick at a time, piece by piece and it gets more and more complex.\nIf you think it through carefully, you can make every block small, composable, easy to modify, and to play well with others."
  },
  {
    "objectID": "01_fundamentals/adoreme.html#use-cases",
    "href": "01_fundamentals/adoreme.html#use-cases",
    "title": "4  Case Study: AI at Adore Me",
    "section": "4.6 Use-cases",
    "text": "4.6 Use-cases\n\n4.6.1 Clustering Customer Feedback\nLet’s no discuss an application suitable for NLP. We receive lots of customer feedback. Millions of pieces of text – so we want to improve our products: recommendations, shipment and general user experience on the website. People in our company, especially PMs, read it very carefully and try to put on the roadmap new features that improve the product. This reading is time consuming and wasteful, so, we thought to automate the clustering (not only classification) – discover new topics as they appear, bugs, stuff refering to same idea.\nMoreover, we don’t have the resources to go over them manually and label for training a ML model, 50k comments, for ex. So we have to do it in an unsupervised way, meaning we have no true labels, and we have to discover interesting clusters/topics in data.\nIn this image you can see a lot of comments, and we used the top2vec algorithm, which is a different story, modern way of doing topic modeling, based on huge pretrained models, of course coming from giants like google and fb, we just leverage it and adapt for out problems.\nSo, using this, we find similar comments, try to find which terms are characteristic for those topics: regarding shipping times, returns, personalization or product offering, bugs and problems. One interesting cluster was about people’s insecurities , and since one of our goals is to make them feel better in their own skin, it’s potentially useful information to improve personalization (at product pipelines.) - -it’s something that was found in data, we didn’t tell the model to look for it.\nSo, how does the architecture look like? We have a bunch of comments, we gathered them in BQ, we have a topic model, a script, logic - we use vertexAI pipelines. It’s not anymore the lightweight example of before, these models are heavy, too much to handle for a Cloud Run, little serverless machine with low memory and CPU. Why does we use vertex pipelines – it is a platform in which you can define your ML pipes, each component/step does something, e.g. first one does the text preprocessing, which is a surprisingly expensive operation, in terms of compute time.\nWe split into sentences, we cleaned up the words, lemmatized, identified negations (very tricky problem) – we try to cluster those with top2vec model (another component in the pipeline), in which you take a dataset as an input and you get a recipy/program, the binary thingy which you obtain as a result of training – use that model to make predictions on the new comments, and assign them on existing clusters.\nThis is the essence of ML, without going into much detail, the idea of going from experience (meaning data) to expertise (meaning a recipe, program), which can make predictions which generalize beyond the data that you have seen.\nSo we have this model, receive new data, run a job, store the prediction in BQ and show the results to POs and devs, the results in a dashboard, in a way which makes it easy to search, filter, find insights. So, we’re going slowly towards lots of decisions at scale, but we’re still not there – in the phase of exploration, trying to find interesting patterns in data, getting inspired (analytics).\n\n\n4.6.2 Launch Sales Curation (Demand Planning)\nOur second big challenge is demand planning. What does that mean? We have many products, with many different sizes, e.g. a bra from 16-28 sizes, 2 possible panties in the set => so the problem explodes in complexity (ordering and manufacturing the right quantity, at the right price, at the right time). But let’s take a little piece of it.\nAt the same time we don’t want to miss sales: every customer who wanted to buy a product – we want to make it available for them. Also, we don’t order too much, to avoid excess stocks/inventories – as we could use that money in other ways: to improve the DC, have a better marketing campaign and many others. However, in uncertain times, you might want to stockpile a little, e.g. it helped us when Covid hit, lockdown, demand for home apparel increased, so we had the necessary stock to smooth it out and adapt fast.\nIt’s a big, big challenge: to forecast the demand, quantify the uncertainty, forecast how much inventory will you have in 6-7 months, and how much to buy/manufactore. We can restrict it, to new product launches to show the idea of data-centricity:\nSo, we have an amazing team which designs the products, fits, they know the materials, manufacturers, trying to achieve the best quality and looks with price constraints at hand. Those new products, you don’t have history for them, have to look at similar products launched in the past, make certain assumptions, take into account the seasonality. So, if you’re forecasting the demand based on past launches, the DQ is paramount, you also need to know when did they launch – so here comes in the reality: the data is messy, not always consistent.\nWe had a model for forecasting the demand at launch, with satisfactory performance, we tested it, improved a lot in comparison with the gut feeling decisions (more optimistic). But we wanted to go further, have more accurate and interpretable forecasts, when we launch a new product. For that, we noticed that there were data inconsistencies. What we did – try to solve most of them automatically: what is the date we consider as launch (such that we get the best predictive performance). Sometimes it is before the declared date, as we do tests for a small percentage of customers, to see the appetite, test it. Other times it is launched later, due to delays.\nSo, using statistical criteria and business logic, we selected a range of the product launch sales, which can be used to forecast. However, we weren’t sure of some cases, but the good news is that we have domain experts, who knew exactly what was happening and could use their judgement for these edge-cases. For those exceptions, we built a little app in which they can either approve/reject some launch sales, or provide the right intervals, tweak based on their judgement.\nThis way, we solved a big problem: some DQ issues, which just couldn’t be automated. This, in turn, made our forecasting more accurate. What we used, a simple framework (Voila), in which you can take a jupyter notebook, and make it into an app. Shiny, dash, streamlit, anything works here.\nWe deployed it on APpEngine, and got user auth for free, without writing any code. It took very little time, not just for the forecasting of new products, but also for an auxiliary app that improve user’s life. You can think of this little app/module as that component in the “city”, that helps the main demand planning application with some particular issure: using data and lots of automation.\nLast remark here, if you heard lately Andrew Ng, he talks a lot about this data-centric approach to modeling, which I totally agree with. If you feed your model garbage data, you get nonsense answers\n\n\n4.6.3 Online model serving\nEverything which I described before is batch, not happening real-time, at high frequencies. But, what about, if you have to display recommendations on the website? What if the standard offerings, vendors of recommender systems aren’t flexible enough for your use-case and infrastructure? They don’t understand the particularities of lingerie, sizing and fit, colors, levels of sexiness of products, occasions. So, for purposes like this, we build custom algorithms and we have to serve them on the website.\nSometimes, the purpose is to gather more feedback, product ratings (as in nflx): “we think you would like this and that, pls confirm if it is true.” That helps us make better curations of boxes for the try-at-home. Other times, they choose something from a catalog and have the option to receive a few more products as a surprise, to try at home, return at no cost.\nOf course, every time you do online predictions, it has to be fast: has to take into account inventory constraints, the context of the user. So, how would this kind of architecture look like?\nWell, we talked about serving and modeling. On the one hand you have a serving API, in which we get a req. from user/ system doing the req. on behalf of the user, with the features that we can support. We load the data from redis (in gcp it is memorystore), run algo and return the prediction. We didn’t do it in go, c++, a cleverly engineered solution in python was enough. Also, we have a fallback from firestore.\nSo, let’s go full stack: you have the AM website, the backend of the whole ecommerce, and this little guy, let’s call him the stylist (ref: restricted computational domain, ddd), who can understand products, get customer preferences and rank those products. This little component is has to be fed with data. We connect the previous ideas about batch pipelines BQ, by uploading it to a feature store, or a database appropriate for serving features.\nSo, this is how we solved the difficult problem, and it was for the first time in which data science served recommendations on the website, and not just precomputed, but real-time. We used Cloud Run (think of lambda functions, on containers/kNative), in which you have more flexibility in how you difine the environment.\nLet’s see what we got out-of-the-box from GCP to respond to our challenges: we didn’t want cold start - we put at least one warm instance; more memory - no problem, simple config; fast database - matter of choosing appropriately. It went surprisingly well.\n\n\n4.6.4 Demand Planning and Vertical Slices\nThe last set of ideas i wanna go through is related to vertical slices. Usually we focus too much on the model: how to tweak it, squeeze the last bits of performance, but we rarely think about the ecosystem. So, let’s see what do I mean by the vertical slices.\nI showed you that application, in which we curated historical sales for forecasting new products, now let’s introduct the fact that we have to do the forecasting, and toghether, you have, at the bottom: - data pipelines - upload that data in the feature store, a place from which you can serve it to applications - then you have a model/serving api - and a classical application with frontend/backend, but with a more data-driven twist, with lots of data visualization, the one in which the final user spends most of their time.\nIf we focus on this well-defined task of forecasting new products, we get a vertical slice. But of course, in forecasting, we have to follow a complicated workflow: in our case demand forecast, split it, break down to the size level, look at how much inventory do we have, receive in the future, what would be the demand 12 mos in the future: then optimize – how much do I need to buy in order to avoid excess inventory and to satisfy the user demand. That would be an end-to-end thing. So, what am I suggesting, collaborate with other teams: technical, domain experts, decision-makers, and try to identify those vertical slices (in the context of a larger workflow), which is a mix of happy friends (application itself – and everything operational regarding that admin tool, or the website itself); which has a component of computational intelligence (a restricted computational domain with statistics, ML, optimization) – together with the stuff to train, retrain, validate, use it in production.\nThis is how the full picture would look like."
  },
  {
    "objectID": "01_fundamentals/adoreme.html#conclusions",
    "href": "01_fundamentals/adoreme.html#conclusions",
    "title": "4  Case Study: AI at Adore Me",
    "section": "4.7 Conclusions",
    "text": "4.7 Conclusions\nConclusions and Contributions: Agility, Scaling, Pragmatism In conclusion, what is the contribution of all of this? Is this talk philosophical? Well, yes, a little – a way of doing things, but what real impact did we have for the company and the team doing data science. I have to say that AI is hard, you have to know programming and CS, data, data pipes, math, to be able to build models which solve real challenges. First you have to build that AI, then constrain it, so it doesn’t go wild/stupid and break your business.\nBecause it is so hard, we don’t want to be burdened with managing infra, by inefficiency in communicating with stakeholders, slowed down by awkward integration between a dsc api/process and a tool owned by another team.\nSo, we want to be agile, flexible, build small components, communicate. This allows for easy scaling – just put more resources in vertex ai/ cloud run, with those serverless dbs, you don’t really have to do anything, scales well with large amounts of data.\nIt gives us a pragmatic way of getting stuff done. By getting stuff done, I mean what’s the smallest amount which I can build, which makes the life of my client easier (be it a colleague, decision-maker, customer.) – how can we improve it? Then, how can we iteratively, go into the direction of better and more complicated models, which better reflect the reality and make better predictions, forecasts, more interpretable statistical models.\nWe can get most of those nuisances out of the way by following a few easy principles, carefully taking into the consideration the domain in which we work. So, what should you take away from this talk? - Demistify what AI means. It is by no means magic and involves, hard, laborious work. We looked at some ways in which we can make that work, easier, more exciting (focus a larget percentage of time on the thing which we’re passionate about – that part of intelligence, statistics, maths, exciting modeling), and less time to waste."
  },
  {
    "objectID": "01_fundamentals/lab_01.html#reproducibility-crisis",
    "href": "01_fundamentals/lab_01.html#reproducibility-crisis",
    "title": "5  Lab: tidyverse and ecosystem of packages: WIP",
    "section": "5.1 Reproducibility crisis",
    "text": "5.1 Reproducibility crisis\nInsert excel meme"
  },
  {
    "objectID": "01_fundamentals/lab_01.html#setup-and-a-discussion-of-the-tech",
    "href": "01_fundamentals/lab_01.html#setup-and-a-discussion-of-the-tech",
    "title": "5  Lab: tidyverse and ecosystem of packages: WIP",
    "section": "5.2 Setup and a discussion of the tech",
    "text": "5.2 Setup and a discussion of the tech\n\nRMarkdown and Quarto\ntidyverse and ggplot\narrow, duckdb and organizing the files\nrenv and reproducibility\n\n\n\n\n\n\nflowchart LR\n  A[qmd] --> B(Knitr)\n  A[qmd] --> C(Jupyter)\n  B(Knitr) --> D[md]\n  C(Jupyter) --> D[md]\n  D[md] --> E(pandoc)\n  E(pandoc) --> F(HTML)\n  E(pandoc) --> G(PDF)\n\n\n\n\n\nFigure 5.1: How Quarto orchestrates rendering of documents: start with a qmd file, use the Knitr or Jupyter engine to perform the computations and convert it to an md file, then use Pandoc to convert to various file formats including HTML, PDF, and Word."
  },
  {
    "objectID": "01_fundamentals/lab_01.html#pipelines-dags-everywhere",
    "href": "01_fundamentals/lab_01.html#pipelines-dags-everywhere",
    "title": "5  Lab: tidyverse and ecosystem of packages: WIP",
    "section": "5.3 Pipelines: DAGs everywhere",
    "text": "5.3 Pipelines: DAGs everywhere"
  },
  {
    "objectID": "01_fundamentals/lab_01.html#practice-a-reproducible-analysis",
    "href": "01_fundamentals/lab_01.html#practice-a-reproducible-analysis",
    "title": "5  Lab: tidyverse and ecosystem of packages: WIP",
    "section": "5.4 Practice: A reproducible analysis",
    "text": "5.4 Practice: A reproducible analysis"
  },
  {
    "objectID": "01_fundamentals/lab_01.html#practice-our-first-data-apps",
    "href": "01_fundamentals/lab_01.html#practice-our-first-data-apps",
    "title": "5  Lab: tidyverse and ecosystem of packages: WIP",
    "section": "5.5 Practice: Our first data apps",
    "text": "5.5 Practice: Our first data apps"
  },
  {
    "objectID": "02_causality/background.html#ab-testing-scheme",
    "href": "02_causality/background.html#ab-testing-scheme",
    "title": "6  Background: WIP",
    "section": "6.1 A/B Testing Scheme",
    "text": "6.1 A/B Testing Scheme"
  },
  {
    "objectID": "02_causality/background.html#error-types",
    "href": "02_causality/background.html#error-types",
    "title": "6  Background: WIP",
    "section": "6.2 Error types",
    "text": "6.2 Error types"
  },
  {
    "objectID": "02_causality/background.html#metrics",
    "href": "02_causality/background.html#metrics",
    "title": "6  Background: WIP",
    "section": "6.3 Metrics",
    "text": "6.3 Metrics"
  },
  {
    "objectID": "02_causality/background.html#pitfalls",
    "href": "02_causality/background.html#pitfalls",
    "title": "6  Background: WIP",
    "section": "6.4 Pitfalls",
    "text": "6.4 Pitfalls"
  },
  {
    "objectID": "02_causality/background.html#most-tests-are-linear-models",
    "href": "02_causality/background.html#most-tests-are-linear-models",
    "title": "6  Background: WIP",
    "section": "6.5 Most tests are linear models",
    "text": "6.5 Most tests are linear models"
  },
  {
    "objectID": "02_causality/background.html#dead-salmon",
    "href": "02_causality/background.html#dead-salmon",
    "title": "6  Background: WIP",
    "section": "6.6 Dead Salmon",
    "text": "6.6 Dead Salmon"
  },
  {
    "objectID": "02_causality/background.html#bootstrap",
    "href": "02_causality/background.html#bootstrap",
    "title": "6  Background: WIP",
    "section": "6.7 Bootstrap",
    "text": "6.7 Bootstrap"
  },
  {
    "objectID": "02_causality/background.html#ecdf-not-your-grandmas-histogram",
    "href": "02_causality/background.html#ecdf-not-your-grandmas-histogram",
    "title": "6  Background: WIP",
    "section": "6.8 ECDF: Not your grandma’s histogram",
    "text": "6.8 ECDF: Not your grandma’s histogram"
  },
  {
    "objectID": "02_causality/background.html#using-graphs-to-reason-about-this-mess",
    "href": "02_causality/background.html#using-graphs-to-reason-about-this-mess",
    "title": "6  Background: WIP",
    "section": "6.9 Using graphs to reason about this mess",
    "text": "6.9 Using graphs to reason about this mess"
  },
  {
    "objectID": "02_causality/power.html#nonparametrics",
    "href": "02_causality/power.html#nonparametrics",
    "title": "7  Advanced topics in Hypothesis Testing: WIP",
    "section": "7.1 Nonparametrics",
    "text": "7.1 Nonparametrics"
  },
  {
    "objectID": "02_causality/power.html#bayesian-ab-testing",
    "href": "02_causality/power.html#bayesian-ab-testing",
    "title": "7  Advanced topics in Hypothesis Testing: WIP",
    "section": "7.2 Bayesian A/B testing",
    "text": "7.2 Bayesian A/B testing"
  },
  {
    "objectID": "02_causality/power.html#power-calculations",
    "href": "02_causality/power.html#power-calculations",
    "title": "7  Advanced topics in Hypothesis Testing: WIP",
    "section": "7.3 Power Calculations",
    "text": "7.3 Power Calculations"
  },
  {
    "objectID": "02_causality/power.html#multiple-testing-adjustments",
    "href": "02_causality/power.html#multiple-testing-adjustments",
    "title": "7  Advanced topics in Hypothesis Testing: WIP",
    "section": "7.4 Multiple testing adjustments",
    "text": "7.4 Multiple testing adjustments"
  },
  {
    "objectID": "02_causality/power.html#case-study-bayesian-covid-testing",
    "href": "02_causality/power.html#case-study-bayesian-covid-testing",
    "title": "7  Advanced topics in Hypothesis Testing: WIP",
    "section": "7.5 Case Study: Bayesian Covid testing",
    "text": "7.5 Case Study: Bayesian Covid testing\nMulti-armed bandits remark"
  }
]