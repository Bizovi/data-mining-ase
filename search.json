[
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "Mihai Bizovi | Head of Data Science @AdoreMe\n\n\nAt my day job, I’m a generalist – navigating uncertainty and complexity to improve decision-making at scale.\nAs a teacher, I aspire to contribute to the understanding of AI’s complex landscape; how to navigate it, develop valuable skills, and become more effective at problem-solving.\nAs a person, I aspire to a life of wisdom and meaning. Learning how to be present and deeply understanding what is friendship, caring, love, beauty, and spirituality.\n\n\n\n\nThese are some roles that are important to me: the craft of AI, teaching, and just being.\nOn another note, my interests in AI, art, cognitive science, and philosophy have an aspect of science, craft, worldview, and deep participation, engagement. Moreover, they are deeply interconnected and I would even say, synoptically integrated.\n\n\nIt is important to me that these conceptual frames fit into a coherent whole and contribute in a practical way towards a good life. When I started the blog in 2017, my work was at the center (metaphorically and literally), but now the golden thread is philosophy as a way of life.\nUnfortunately, I have none of the answers of how does Intelligence, Rationality, Wisdom contribute to meaning in life – at best, some plausible hypotheses\nI can’t emphasise enough the importance of different levels of “knowing”: propositional, procedural, perspectival, and participatory – as it is not enough to know the facts (or have beliefs), but to know how to do something, to have a perspective of the “world” and a sense of participation in whatever you’re engaged in. I mean that we’re agents in different arenas of life, and the sense of meaning comes from an attunement to those arenas. We participate in a course of something, which has impact on the environment, which changes us and how we view and relate to the world, self and others."
  },
  {
    "objectID": "about_me.html#long-story-short",
    "href": "about_me.html#long-story-short",
    "title": "Decision Science Course",
    "section": "Long Story Short",
    "text": "Long Story Short\n\nGraduate of Cybernetics (2017) and Quantitative Economics (2019)\n\nDid some research in Complex Systems, Agent-Based Modeling, Systems’ Dynamics, and Heterodox Economics, which was lots of fun\nMy serious work was at the intersection of Bayesian Statistics and Machine Learning (thesis, dissertation)\n\nTeaching Data Science (by which I mean Decision Science) at university and private courses. You can check out the key ideas in the course homepage.\nHead of Data Science at AdoreMe Tech, with the firm since 2016\n\nLingerie e-commerce in the U.S., with a PAYG, subscription and try-at-home business models\n$250m revenue in 2022, acquired by Victoria’s Secret\n70 tech people in Bucharest, over 300 employees\nMy work involves AI Strategy, Product Management, ML Systems Design 1\n\nSome typical applications of AI in an e-commerce I contributed to:\n\nDemand Planning and Inventory Optimization systems to prevent lost sales and excess inventory\nRecommender Systems to help users find relevant, personalized items and bundles\nMarketing optimizations in Acquisition/Advertisement, Engagement, CRM, Merchandising, Pricing and Promotion\n\nHobbies:\n\nReading about “Philosophy as a Way of Life” & Cognitive Science\nPainting, Hiking, Jazz/Classical/Metal\nPlayed chess professionally, but that much stress isn’t worth it\n\n\n1 \n\n\n\n\nSee one of my conference talks at BigDataWeek, intended for a mixed tech/business audience: Pragmatic AI in Google Cloud Platform\n\n\nUnsurprisingly, there will be lots of painting metaphors when it comes to simplicity, and cognitive science references when talking about ways in which we’re biased and foolish. Chess, of course, inspires analogies of competition, strategy, and tactics to its service."
  },
  {
    "objectID": "philosophy.html",
    "href": "philosophy.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "The hell is Cybernetics? Does it have something to do with robotics? Not really – it is studying general regularities of information processing and control1 in Complex Adaptive Systems. It is what was originally meant by AI, before symbolic, expert systems, and deep learning.1 Cybernetics: (gr, kybernetes) – to steer towards a desired outcome or trajectory. It highlights the aspects of agency, action, pattern recognition, and prediction.\nThe way I think of data science – is in the context of those complex adaptive systems. We will discuss it in more detail and nuance, as this perspective is powerful and rarely taught.\n\n\n\n\n\n\nSo, what’s the problem with Cybernetics education?\n\n\n\nThe idea behind a Cybernetics curriculum is great: an interdisciplinary approach to solving complex economic problems, that is what we know today as weak/specialised AI combined with quantitative economics.\nUnfortunately, the execution is extremely difficult to pull off without a consistent vision, expertise, and an account of recent developments. It is even more difficult to put those economic models from ’80s in practice, especially in a data-driven age.\nI am taking the first step to fix this, developing a vision to make it relevant for industry and businesses again. The easiest way is by smuggling ideas from systems’ thinking into the practice of data science.\n\n\nLet’s go back to the burning question: what I wish I had in a course? It is hard to teach these topics in a way which is both theoretically sound and can be immediately applied.\nAfter recognizing that there is no silver bullet, my conclusion is that following the principles outlined below consistently, dramatically increases the chances of preparing a new generation ready to tackle the messy, ill-defined problems we encounter.\n\nProvide motivation for why something is important (a field, theory, model, method, technology)\nDiscuss practical applications and challenges the firms face, from an insider’s perspective. Those are our case-studies.\n\nFirst, we try to deeply understand the problem and frame it in “our” language, that is, clarify and reduce ambiguity\nThen, we strip the problem to the simplest model, to illuminate some key aspect, like pricing and demand planning decisions. Then, and only then we bring back all the realism and complexities.\nApplications are based on realistic or real data, which can be messy, hard to access, biased and incomplete\nCoding up the solutions and software implementations, with the real-world challenges of deploying models and improving decision making\nImplement those in code using best practices, as an engineering exercise\nWith lots of visualization, especially interactive ones\n\nDevelop a conceptual undestanding, an intuition about the problem and the “tool” we think is appropriate in tackling it:\n\nPresent the tool theoretically rigorous and sound, but only where it matters. More geometry and graphs over equations, more simulations and stories over proofs.\nLeverage previous mathematical, statistical, and domain knowledge\nFor the mathematically inclined, add some elements of abstract math to understand the underlying foundations of these methods and models\n\nUse simulations as a safe playground 2 that we control, to get a feel for the behavior of models and algorithms.\n\nIt forces us to declare our assumptions about the problem\nBefore we commit to a costly real-life experiment or modeling project, it is essential to know that a model works in principle for our application: is able to recover the parameters, causal structure and generalize beyond the sample.\n\n\n2 I often find myself truly understanding something, only after I code it up and understand the mechanics of a model well, then try to think of how I would apply it in practice, in different contexts. This hints to the idea that we need complementary computational background and tools, right from the beginning of higher education.Speaking about the prerequisites, there are some tools we have to dust off the shelves and cultivate an appreciation of their importance: linear algebra, calculus, probability theory and mathematical statistics. Not least, being competent in a programming language like Python or R is essential, such that we can focus on what truly matters.\n\n\nThose prerequisites are placed in context of the business practice, with swift reviews and references to resources which should fill in all the gaps.\nAt the same time, we have to gradually get rid of the bad habits that were accumulated: analyses which can’t be reproduced, mechanically following a statistical procedure (because the flowchart of statistical tests said so, for example), jumping to a conclusion (as if there is only one, correct, textbook answer), rushing the learning process, handwaving to cover for a poorly articulated argument. On the other side of the spectrum, perfectionism doesn’t help either, as this field is inherently iterative and experimental.\nTherefore, we need to develop a set of processes and methodologies to iterate and improve effectively. Especially, we focus on the process of problem-solving: from formulation to modeling and operationalization."
  },
  {
    "objectID": "philosophy.html#on-interdisciplinarity",
    "href": "philosophy.html#on-interdisciplinarity",
    "title": "Decision Science Course",
    "section": "On Interdisciplinarity",
    "text": "On Interdisciplinarity\nThis is my second attempt at formulating a philosophy and principles for a challenging endeavor like this one. A moment of inspiration hit me and the following “map” resulted – which strikes a resemblence with an idea of R. Sapolsky in one of his lectures.\n\n\n\n\n\n\nThe danger of thinking in buckets\n\n\n\nHere’s the point, which you can find in the lecture notes.\n\nSapolsky starts out with: our brains think about stuff by creating boundaries – i.e. ‘buckets’ – around ideas.\nThese buckets can influence our memory, our language, and our ability to see the ‘big picture’.\nAn implication of our bucketing minds is that we are bad at differentiating facts that fall within the same category. Two shades of red are labelled ‘red’.\nA second larger implication is that when we focus on categories while talking about behavior, we lose out on the big picture.\nIt’s easy to see a single one of these categories as providing The Explanation. But they are merely various Behavior Buckets. They are all a part of the big picture explanation.\nIt is an easy trap to fall into. Flawed bucket thinking has been done by many of the most influential scientists in history!\n\n\n\nA major goal is to not fall for bucket thinking – we must resist the temptation to find “The Explanation” in one bucket. Much time will be spent traversing the various buckets and when we stop for a while, we practice and try to understand.\nRepeat until we’ve came full circle with a renowed appreciation, perspective, and ability. Sapolsky talks about evolution, neuroscience, molecular genetics, ethology, etc. We’re going trough different ones:\n\nProblem space (challenge land): understanding a domain where “we” have to make decisions, improve the relevant things for clients and stakeholders. There is much uncertainty there, questions about what will happen and how should we act. It’s the real world, seen as a Complex Adaptive System.\n\nBe it in firms, economics, and finance\nBe it in biology and life sciences\nBe it in engineering systems\nThis is where we get our data from and who we build software for!\n\nScience, especially cognitive science, which will give us insights about our intelligence, rationality, wisdom, foolishness, and biases. This is the place where we’ll get the process/method, learn how to observe, formulate scientific hypotheses, use theories, theoretical models to make predictions and perform experiments.\nIn probability, we reason about uncertainty in the real world, build narratives and tell stories with DAGs of random variables, which are prague golemns, little robots which generate data. In the happy case they match the theoretical models and result in plausible patterns. We’ll spend much time simulating phenomena, being the masters of these alternative multiverses. Not bad, aye?\nIn statistics, we change our minds and actions in the face of evidence. We learn the skills of exploratory data analysis, experiment design, and causal inference. Why build models? To make better decisions, of course.\nMachine Learning and Deep Learning, the younger tribes of statistics are the future: they learn from data and when things go well, make reliable and robust predictions, in order to optimize the heck out of any process. Think of them as shamans or oracles, who sometimes overfit, therefore are prone to acting foolishly.\nWe come back to the homeland of many of you: computer science and software engineering, the place where nowadays everything on this map becomes reality. We will learn how to build full-stack, data-driven software, good practices of the guild. While spending time here, an appreciation for the contribution of CS to all other places we already visited will become obvious.\nIt is all overseen by philosophy. Some things don’t change.\nAh! We forgot about mathematics. It is everywhere, but also stands proud on its own. An essential prerequisite for everything we do, however, it is hard to do rigorous mathematics in the setup we outlined, as it will take a decade. The good news is – we will be fine just with the starter pack!"
  },
  {
    "objectID": "philosophy.html#lessons-in-humility",
    "href": "philosophy.html#lessons-in-humility",
    "title": "Decision Science Course",
    "section": "10 Lessons in Humility",
    "text": "10 Lessons in Humility\nWhatever will be published here can’t be the be-all-end-all bootcamp or course. But here are a few of my beliefs, which may persuade you to take the long road to your own development in AI instead of searching for “the tutorial”:\n\nThere is no shortcut to deep understanding\n\nOf a domain, especially in an interdisciplinary setting\nWith communities engaged in an evolving dialogue\n\nThere is no shortcut to being skillful at something\nThe journey from novice to expert is not linear, however, the “interest compounds”\nThe journey need not be painful, but it can be seriously playful,a source of wonder and meaning\nWithout skin in the game, we can’t claim we truly get something\nWithout a vision which is flexible enough, but at the same time long-lived:\n\nIn the case of rigidity - there is a risk of being stuck, pursue obsessively, counterproductively the wrong thing\nIn the case of everything goes - there is a risk of wandering aimlessly and not finding a home\n\nFixating on beliefs and propositional knowing (the facts!) is counterproductive. Which should put into question all written above\nFixating on skills makes you lose the grasp of the big picture"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "Mihai Bizovi | Head of Data Science @AdoreMe\nThis website emerges out of teaching data science to students of various backgrounds and my practice in the industry as an engineer and leader.\nI aspire to contribute to the understanding of this complex landscape and teach people how to navigate it, how to develop valuable skills, and become more effective at problem-solving. To achieve this, we need to master “the fundamentals.”\nI believe that improving decision-making is the reason why we practice AI. We mostly forgot that as teachers. We’re also getting increasingly specialized, let’s say, in Deep Learning – which makes us miss the interdisciplinary, big picture."
  },
  {
    "objectID": "index.html#decision-making-for-the-brave-and-true",
    "href": "index.html#decision-making-for-the-brave-and-true",
    "title": "Decision Science Course",
    "section": "Decision-Making for the Brave and True",
    "text": "Decision-Making for the Brave and True\nThis is an introductory, big picture, free and open-source course which bridges the gap between theory and practice, cultivating the skills and understanding necessary to bring value to organisations by improving decision-making. It is an attempt to find a golden mean of both worlds:\n\nIlluminating theoretical ideas (contemplating in the library)\nPracticing battle-tested technologies (engineering in the trenches).\n\n\n\nThe best advice I ever got as a novice data scientist was: “Think of youself as a business person with superpowers”. I’m attempting to reconcile that spirit of simplicity and elegance with advances in statistical thinking and engineering.\n\n\n\n\n\n\nWho should read this book?\n\n\n\nAnyone lost, confused, stuck or overwhelmed by Data Science and Machine Learning complexities, who wants to see the big picture, a path forward, and the possibilities.\nIf you stumbled upon this website, you’re probably a student in Business Analytics, or know me personally – well, because I shamelessly promoted it.\nMaybe, you’re an engineer getting curious about ML or an analyst with a knack for the business, looking to improve your workflow and expand the quantitative toolbox. Maybe you’re a product manager or an entrepreneur who wants to infuse AI into your startup.\nIn my opinion, junior data scientists and ML practitioners a few years into their journey will benefit the most from the re-contextualization of fundamentals that I’m doing here, which could enable them to take another leap in career.\n\n\nThis is the course I wish I had when starting my journey in data science, which would prepare me for the realities of the industry, often very different from the academic world. In my teaching, I achieve this by putting business decisions, understanding the domain, and working software at the forefront of each (statistical) tool we learn.\nThe course is challenging due to its breath and depth. At the same time, this is NOT a bootcamp: it is not enough by itself to land you a job, which requires lots of practice and extra study. To the service of understanding, it becomes quite abstract and conceptual at times, but I hope you bear with me until you see the benefits of those abstractions.\nThink of it as a skeleton, a conceptual frame1 which ties together everything you have learned so far and can be built upon as you progress in your career and studies. You will probably go back to the same idea after years, with greater wisdom and skill – to unlock its real power.1 This course stands on the shoulders of giants, and I can only aspire to get to the level of clarity and rigor provided by Hastie/Tibshirani or Andrew Ng when it comes to ML, and Richard McElreath or Andrew Gelman on Bayesian Statistics / Causal Inference.\n\n\n\n\n\n\nOverwhelming amount of content. What is best for learning?\n\n\n\nGiven the over-abundance of learning resources, it is too much to sift through a hundred pages of Coursera in the data science section, books, and tutorials to find the optimal ones. Inevitably, there is so much bullshit and repetition.\nThe roadmap provided here should help you navigate it and find the shortest path towards developing your skills and better decisions in your firm. Pick a module you’re passionate about, for example, Bayesian Statistics and look in the resources page for a guide where to start reading and practicing."
  },
  {
    "objectID": "index.html#why-should-you-care",
    "href": "index.html#why-should-you-care",
    "title": "Decision Science Course",
    "section": "Why should you care?",
    "text": "Why should you care?\nYou might’ve heard that data scientist is the sexiest job of 21st century, that AI is going to take over repetitive jobs, Deep Reinforcement Learning models are beating people at Go, Dota, Chess, solving almost-impossible protein-folding problems. The world is awash in the newest ChatGPT and Midjourney frenzy, with new developments every month and week.\n\n\n\n\n\n\n\n\n\nHow do I keep up?\n\n\n\nYou don’t, at least if you want to have a balanced life. That’s why I choose to focus on fundamentals which stood the test of time, which are anyways a prerequisite before you dive into understanding the technicalities of those cutting-edge models and systems.\nYou will be surprised how far can you go with simple, even linear models. In the end, it is not a competition, nor am I against deep learning: we learn to solve a very different class of problems that businesses encounter.\n\n\nBut what does it actually mean, if we step outside the hype and buzzwords, use a plain language, and apply these ideas in a more down-to-earth, day-to-day problems and challenges in businesses?\nIn university years, you’ve probably been tortured by (or enjoyed) linear algebra, mathematical analysis, probability and statistics, operations research, differential equations, mathematical economics and cybernetics, algorithms and data structures, databases, object-oriented programming, econometrics and so on.\n\n\n\n\n\n\nWhy study all of this?\n\n\n\nWe live in a volatile, uncertain, complex and ambiguous world,2 but we still have to make decisions. Those decisions will bring better outcomes if they are informed by understanding the causal processes, driven by evidence and robust predictions. For a more in-depth explanation of the essence of each subject/domain, read here.2 VUCA: a mental model to better understand the world\nI want you to take away ONE thing, that is “AI” and Data Science in Businesses boils down to: Decision-Making under Uncertainty at Scale\n\n\nIt can be a function of decision-making support or the system/product itself, like in the case of Uber, Amazon, Netflix, Spotify, Google and many others. Even if you are not a data scientist, you will work with them in one form or another (Quant, Data Analyst, Business Analyst, ML Engineer, Data/BI Engineer, Decision-Maker, Domain Expert). Therefore, you have to understand their language, what are they doing, how to ask and make sure they solve the right problem for you.\n\nWhen somebody asks you what have you learned in this book and course, I suggest two metaphors:3 one of simplification and another of seeing relations3 Due to my philosophical readings, some of the language here has a very specific meaning, which is totally outside the scope. For the curious, I will write a few blog posts outlining some philosophical arguments which struck a chord in me and which I found tremenously useful in day-to-day life.\n\n\n\n\n\n\n\n\n\nWe see Pollock’s messy reality, which is the data and observations. We want to get to Picasso’s bare bones essence, for clarity and better decision-making\n\n\n\n\n \n\n\n\n\n\nThis is a big picture course, which re-contextualizes everything you have learned before, but didn’t see how it all fits together or can it be implemented in practice to bring value to organisations, that is: be useful"
  },
  {
    "objectID": "index.html#what-will-you-learn",
    "href": "index.html#what-will-you-learn",
    "title": "Decision Science Course",
    "section": "What will you learn?",
    "text": "What will you learn?\nI always start with the first module, in which we explore decisions in businesses, figure out what does AI mean, and where it adds value. If we view statistics as changing our minds and actions in the face of evidence – fundamental statistical concepts and tools will shine in a new light. It will become clear why those models and methodologies were invented in the first place.\n\nModule 1: Business Decisions and Probability Fundamentals\nModule 2a: Applied Bayesian Statistics\nModule 2b: Hypothesis Formulation and A/B Testing4\nModule 2c: Causal Inference with Directed Acyclic Graphs\nModule 3a: Probabilistic Machine Learning\nModule 3b: Deep Learning and Special Topics\nProjects: Software Engineering and Full-Stack Data Apps\n\n4 \n\n\nWe will see that formulating a hypothesis is an art in itself, and that experiment design in practice is fascinating and complicated, full of pitfalls and dangers. That’s not your stats 101! (Source: McElreath)\n\n\nThese fundamentals are not “just theory”, it is what will make or break our project in practice. We will achieve a lot with simple, elegant models; appreciate the importance of asking the right questions, persuasive communication, and storytelling with data. That’s why it’s essential to take every tool, and apply it in the context of an appropriate use-case/application – meaning, we’ll code a lot.\n\n\n\n\n\n\nGo to “Resources” page for practice!\n\n\n\nConceptual understanding by itself is not enough. So, I curated a list of resources to practice on interesting case-studies, datasets, which directly apply the models, tools, and methodologies presented. These are written by experts in the field, are usually well thought, easy to follow, reproducible, and highlight important aspects of a problem and model.\nAlso, keep an eye on the course github repo, in which we’ll do some exciting projects (full stack data apps) and investigate common problems/challenges with a fresh perspective.\n\n\nOnce we have a confident grasp of the fundamentals, we continue with understanding and applying Bayesian Statistics. It is an extremely flexible and composable approach to building explainable models of increasing complexity and realism. To put it bluntly, it will enable us to improve on most challenges in decision-making that businesses face.\nDid you get comfortable with building custom statistical models for inference and prediction? For decisions with high stakes, we often want to do controlled experiments. We’ll develop a rigorous methodology for designing and performing A/B tests, learn how to recognize and avoid pitfalls that it’s so easy to fall into.\nOften, A/B tests and randomised experiments are unfeasible or unethical. We also acknowledged the limitations of inferring an association5, so it’s time to get more powerful tools from causal inference. We can leverage natural experiments. This is truly challenging: it is an art and science, in contrast with the auto-magic pattern recognition of ML. It requires deep thinking and understanding.5 Realize that we cannot reach a causal conclusion from observational data alone. We need a theory, which is our understanding of how the “world” works – translated into a statistical model, plus data, which will give us new insight into the causal processes (the evidence).\n\n\n\n\n\n\nWhat if I care only about ML?\n\n\n\nEven if you’re interested only in machine learning, most practitioners will emphasize the importance of mastering regression (generalized linear models) and doing A/B tests to gather evidence that our new model brings an improvement.\nThis is how we jump through various buckets, highlighting the golden thread linking them all: decisions and uncertainty. Moreover, the tools we learned in Bayesian Statistics are directly applicable in ML – the lines between these two fields are indeed very blurry.\n\n\nOften, we care not just about a single decision or developing better policies, but we have to make tons of little decisions at scale. This is when we switch to a predictive, Machine Learning perspective and walk through our workhorse models, which should serve us decades ahead in a wide range of applications: both predictive and exploratory.\nDuring the labs we’ll build from the ground up a tech stack for reproducible data analysis, model and data pipelines, culminating in a full-stack data app (with user interface, backend, database), which solves a real-world problem.\n\n\n\n\n\n\nBuild an impressive project for your portfolio\n\n\n\nThat is your final project for the course6 and something you can brag about in your portfolio and github profile. It sounds complicated, but we have the tools to make it easy for us. Don’t worry about getting everything right, but focus on a problem and single area from the course you’re passionate about: be it data visualization, ML, statistics or sheer engineering curiosity.6 A project can be much simpler: a quarto blog to showcase your research, a python package and CLI application to train a model, a streamlit app to demo a model.\n\n\nThe icing on the cake is miscellaneous topics dear to me and usually not covered in such a course: Demand Forecasting, Recommender Systems, and Natural Language Processing. All extremely useful in business contexts, but significant tweaks are needed to the models discussed before."
  },
  {
    "objectID": "04_engineering/roadmap.html",
    "href": "04_engineering/roadmap.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "flowchart LR\n  DSc(Data Apps)  --> Data(Data Wrangling)  --> EDA(EDA) --> DV(Visualization)\n  EDA --> LP(Literate Programming)\n  \n  DSc --> Repr[Reproducibility] --> DP[Data Pipelines]\n  DSc --> M[Modeling] --> Pymc(PyMC) --> Tr[Torch]\n  DSc --> MLP[ML Pipes] --> Srv(Serving Models)\n  \n  DV --> FS[/Full Stack Apps/]\n  Tr --> FS\n  Srv --> FS\n\n  Repr --> LP\n  DP --> FS\n  FS --> Dep[Deploy]\n\n\n\n\n\nFigure 1: We will need to learn a lot of engineering and new tools, so that we’re able to collect, clean, explore, visualize data, train models, and build useful applications which improve outcomes for our clients."
  },
  {
    "objectID": "02_causality/roadmap.html",
    "href": "02_causality/roadmap.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "flowchart TD\n  DM(Decisions)\n  DM --> BB(Beta-Bin*) --> GPo(Gamma-Pois*) --> NIG(IG-Norm) --> G(Groups)\n  BB --> LR(Logistic Regr.)\n  DM --> LM(Linear Regr.) --> GLM(GLMs)  --> Caus(Hierarchical Models) --> Misc[/Time Series/]\n  LR --> GLM\n  GPo --> GLM\n  G <--> Caus\n\n  LM --> P(Prior Choice) --> Reg(Regularization) --> VS(Variable Selection)\n  LM --> MS(Model Selection) --> LOO(LOO-PIT)\n  LM --> Rb(Robustness) --> NA(Missing Data) --> Surv(Survival and Censoring)\n  LM -- nonlinear --> NL(Splines) --> BART(BART) --> GP(Gaussian Proc.)\n\n  DM --> DA(Decision Theory) --> NV(Newsvendor pb.)\n  DA --> CF(Classification)\n\n\n\n\n\nFigure 1: We develop a composable toolbox, capable to tackle the challenges of nonlinearity, heterogeneity, low level of aggregation, discreteness, missing data and heteroskedasticity. By putting together the right pieces in a right way, we can improve predictions and decisions in most aspects of a business."
  },
  {
    "objectID": "02_causality/ab_testing.html",
    "href": "02_causality/ab_testing.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "Once you have a good grip on those statistical fundamentals, which are like cogs in the machinery of statistics, you can apply them to hypothesis testing and experiment design.\nI’m emphasizing again the understanding and letting go of mechanical application of procedures and conventions (p-values, \\(\\alpha, \\beta\\), statistical tests). You have to be able to justify all the choices you make during the phase of experiment design, that is before running the experiment.\n\nUnderstanding the philosophy of falsification and how it applies to hypothesis testing. Week2 of this course has a great 20 minute explanation.\nBefore jumping into the hypothesis testing, we should carefully ask whether we need an experiment at all. Here you can see the reasoning for testing and an article by Cassie for statistics.\nRemember the importance of those 12 steps of statistics, especially the Default Action. For a more classical exposition - check this out.\nConfidence Intervals - first check out this simulation. Also chapter 12, uses bootstrap to estimate those.\np-values - simulation, simulation of distirbutions under H0/Ha\nType 1, 2 errors, Type 3 errors (solving the wrong problem), chapter14\nA/B Testing scheme, An end-to-end example, but be careful to follow the 12 steps we have and not to forget to define the Default Action.\nExperiment Design and Hypothesis testing pitfalls - from HBR, 8 pitfalls, A/A tests, user interference\nMetric Design and properties of good metrics - an example\nRelevance and Significance - Read this paper by Werner Stahel\nNon-Inferiority testing - This visualization\nEffect size, Power, Sample Size - here, and here and here. Cohen’s d, Power Analysis\nPhilosophy of science: Popper and Latakos, in this lecture"
  },
  {
    "objectID": "02_causality/roadmap_abtest.html",
    "href": "02_causality/roadmap_abtest.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "flowchart TD\n  H(Hypotheses) --> P(Phil. Science) --> GQ(Good Questions) --> Pre[Pre-Registration] --> Repl[Replications] --> B[Publication Bias] --> OS[Open Science] --> FSS[Falsification] --> Repr[Reproducibility] --> MA[Meta-Analysis] --> E[Ethics & Integrity]\n\n  GQ --> S[Simulation] --> Th[Theory Construction] --> NF[Null is False]\n  H --> A(Path of Action) --> ET(Error Types) --> PV[p-values] --> EC(Error Control)\n  EC --> PPV[PPV] --> Sign[Signficant Findings] --> ERJ[Justification]\n  A --> Eff[Effect Sizes] --> Ch[Cohen's d] --> Corr[Correlations] --> Min[Min Effect] --> TM[Telescope Method] --> RB[Resource-Based]\n  A --> Conf[Confidence Intervals] --> SS[Power Analysis] --> PCAn[p-curve analysis] --> CPc[Capture Percent.] --> CISD[confint std-dev] --> Anova[Anova Power]\n\n  NF --> RP[Risky Predictions] --> EQ[Equivalence Testing] --> IP[Interval Predictions]\n\n\n\n\n\n\nFigure 1: When discussing hypothesis testing and experiment design, we can’t avoid an overview of fequentist methods (path of action) and a discussion of the philosophy of science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture Agenda\nKeywords\nCase Studies\n\n\n\n\n1\nA/B Testing pitfalls\nA/A Tests, Selection bias, Confounders, Novelty\nThe need for rigorous experiments\n\n\n2\nError Types, Effect Size, p-values\ntests for difference in proportions and means\nStrategy for simulation\n\n\n3\nPower, Sample Size calculations, Non-inferiority tests\nrelevance, minimal effect size\nNeyman-Pearson, frequentism as action in the long-term\n\n\n4\nLinear regression review\nHypothesis tests as linear models\nGuidance with student projects\n\n\n5\nProperties of Metrics for A/B Testing\nSensitivity, stability, directedness\nPotential pitfalls\n\n\n6\nProperties of Estimators\nBias-Variance, Fisher Information, Rao-Cramer\nRelevance for ML, Deep Learning\n\n\n7\nBootstrap and Nonparametrics\nresampling, ECDF\nKolmogorov-Smirnov\n\n\n8\nCLT, Choosing a test, Trading off errors, Increasing Power\nCohen’s d, PPV, GLM, nonparametrics\nJustifying \\(\\alpha, 1 - \\beta, \\Delta, n\\) by simulation\n\n\n9\nBootstrap, Multiple Testing\nFDR, Bonferoni\nComputer-age statistical inference"
  },
  {
    "objectID": "03_ml/roadmap.html",
    "href": "03_ml/roadmap.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "flowchart LR\n\n  DSc --> Unsup(Dim. Reduction) --> Clust(Clustering) --> MM(Mixtures) --> HDB(HDBScan)\n  Unsup --> PCA --> CA --> UMAP\n\n  DSc --> Cl(Classification) --> T(Tree-based Models) --> BG(Bagging) --> XG(Boosting)\n\n  Cl --> Im(Imbalance) --> F(Fraud Detection)\n\n  DSc(Decisions, Scale) --> Text[/NLP/] --> EM[Embeddings] --> Attn[Attention] --> ABSA[ABSA]\n  DSc --> RS[/RecSys/] --> Mtr[Metrics] --> FM[Factorization Mach.] --> HM[Hybrid Models]\n  DSc --> CV(DL: Vision) --> Conv[CNNs] --> AK[Approx. kNN]\n\n  Conv --> HM\n\n  DSc --> TS(Time Series) --> MTS[Metrics] --> XGB[ML Approaches] --> DL[DL Approaches]\n  EM --> HM\n\n\n\n\n\n\nFigure 1: In this ML/DL module, we focus on practical, challenging use-cases and reliable, workhorse methods – while keeping in mind the particularities of the domain and applications."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "I assume you read the conceptual stuff on this website, understood its use-cases, and the big picture of what to learn. Now, the question is where and how to start practicing, while catching up on the maths and programming.\nThere are too many courses, tutorials, and datasets for practicing ML/Stats on the web: from didactic toy examples to industrial-scale machine learning. Since it is so hard to strike a balance between clarity, simplicity, and the use-case being interesting, realistic – you fill find here a list of carefully curated resources. Most of them have have code, data, and explained theory or methodology in a freely available e-book.\nThere is little point in replicating well-executed examples from other authors, just for the sake of consistency of code and style. I will, however, migrate examples not available in our frameworks or language of choice. Other times we can benefit from a significant improvement over the original presentation or improving code quality.\nI made sure to include interesting examples and archetypal applications for each statistical tool and theoretical topic, so you can immediately apply the concepts you read about or watched during the lectures. However, the responsibility to practice falls entirely on the learner – I can just do my best to make your journey less frustrating and more efficient."
  },
  {
    "objectID": "references.html#prerequisites-probability-and-python",
    "href": "references.html#prerequisites-probability-and-python",
    "title": "Decision Science Course",
    "section": "Prerequisites: Probability and Python",
    "text": "Prerequisites: Probability and Python\nWe start from the foundations of probability, however, I do not cover calculus, linear algebra, or other mathematical tools for data analysis. We do much more programming than mathematics, therefore, the ability to code in a programming language like Python or R is a must.\n\n\n\n\n\n\nNeed a crash course in probability and statistics?\n\n\n\nIf you understand and can explain the following ideas in a simple, yet rigorous way – you’re ready for the journey. Otherwise, if it feels shaky, here are some readings:\n\nProbability Triple and Random Variables - a quasi formal introduction is written in this chapter of the course website. From my experience, not many students have this understanding after their probability theory classes.\nCollectivity (“physical” structure), Statistical Population, Sample - where defining the population is the contract for our experiment, and the sampling process is critical. It is a much more nuanced topic than it looks, explained well and in great detail here and here.\nParameter (Estimand), Estimator, Estimation/Statistic - same resources as above 1\nStories behnid distributions: \\(Bin(n, \\theta), Pois(\\lambda), Exp(\\lambda), N(\\mu, \\sigma^2), \\chi^2_k, t_k\\). What about \\(Beta(a, b)\\), \\(Gamma(k, \\theta)\\), and \\(Dirichlet(\\bar{\\theta})\\) – what are they good for?\n\nLook at some examples with simulations and stories / applications with more math from Joe Blitzstein.\n\n\n1 Although the perspective I take is Bayesian, I will take some time to cover and re-contextualize the Neyman-Pearson and Fisherian “frequentism”."
  },
  {
    "objectID": "references.html#building-blocks-of-bayesian-statistics",
    "href": "references.html#building-blocks-of-bayesian-statistics",
    "title": "Decision Science Course",
    "section": "Building Blocks of Bayesian Statistics",
    "text": "Building Blocks of Bayesian Statistics\nWe start simple, by modeling a single random variable \\(Y\\), choosing the appropriate distribution for each phenomenon, a prior for the parameters, doing simulations – then sampling from the posterior with pymc, numpyro, and bambi.22 The R equivalents would be stan and brms, rstanarm. bambi and brms are a high-level API for most common models.\nLimiting? Yes, as in reality we care about the relationship between random variables. However, we can get a lot of insight from thoughtful modeling the data generating process, which will serve as building blocks in more complicated and realistic models.\n\n\n\n\n\n\nBayes Rule. Update your beliefs, often!\n\n\n\nAny introduction to the subject will work out, a few excellent ones being Chapter 1/2 of BDA3, Chapter 1/2 of Bayes Rules, and Chapter 1/2 of Statistical Rethinking.3 If you prefer videos, enjoy the 3Blue1Brown visual masterpiece on how to think like a Bayesian or here.3 You will notice in the callouts that I point out where to read the theory – for a conceptual understanding and to figure out the mathematical details\n\n(BDA3, Ch1): Football spreads, that can be estimated from data about matches. What is the probability that a team wins? Are experts right, on average?\n\nIf you’re into betting and sports, can you replicate the analysis on other datasets? What are your options for data collection?\nFor brevity, I won’t elaborate much from now on, how to take an use-case and example to its limit. If you’re passionate about a particular topic – go for it!\n\n(BDA3, Ch1): Spelling correction, based on empirical frequencies provided by Peter Norvig. As in the previous case-study, you will have to code it up and figure it out for yourself – it is good for a warm-up, but challenging enough to keep you occupied.\n(Probability 110): Medical testing for rare diseases, hypothetical example with code in my course repository. We use the same idea to reason about how confident are we our code has no bugs.\n\nIf you remember the Covid-19 rapid tests and their confusion matrices printed on instructions, you could’ve applied the same idea!\n\n\nFor the simplest models, one approach of comparing different hypotheses, is Bayes Factors. However, these do not translate well in practice for more sophisticated, multilevel models. You can look in the following courses here and here for the theory and examples.\n\n\nOr maybe you’re passionate about biology, where you could apply it for Mendelian genetics and think about the mystery of deadly genes persistence\nWe applied Bayes rule and got some insightful results in three totally different domains. However, we weren’t doing neither statistics, nor inference – but got into the right mindset. It is a good opportunity to brush off the shelves some computational and mathematical tools. Now it’s time for full-luxury Bayes and the simplest cases of inference.\n\n\n\n\n\n\nBeta-Binomial Model. Estimating proportions\n\n\n\nI know, I know, the coin-tossing – simple, yet fundamental and found anywhere there is a binary outcome \\(Y_i \\in \\{0, 1\\}\\). There are many ways to estimate the success probability \\(\\theta\\), when we observe \\(k\\) successes from \\(n\\) trials.\n\nIn Bayes’ Rules is a detailed exposition of the theory, with examples about Michelle’s election support and Milgram’s experiment.\nShare of biking traffic in different neighborhoods (BDA3, Ch2, Pr. 8)\nA/B Testing for proportions in BMH. Just remember that experiment design is much more nuanced than performing such inference or a test.\nPolitical attitudes in 2008 Election data (BDA3, Ch2, Pr. 21)\nProportion of female births, given prior information. (BDA3, Ch2)\n\nThere are many more applications, but the ones below require more research and work. They are open ended and you can take these topics very far.\n\nThe debate on the hot hand phenomenon is not yet over. Here are the bayesians weighting in and some new research.\nBasketball shot percentages and true shooting, brilliantly explained by thinking basketball in a playlist about NBA statistics.\nImportant problem in ecology: estimating size of population based on samples (BDA3, Ch3, Pr. 6). The challenge is that in \\(Bin(\\theta, N)\\) both parameters are unknown. Here is an old paper.\nConfidence intervals and lower bound for reddit posts like/dislike ratio. Read more about simple ranking algorithms and the issues of sample size: reddit, hacker news. It is a good opportunity to work with the reddit API in order to collect data about posts and comments.\n\n\n\nThe next step is learning how to model count data, which will open up to us applications of a different flavor. It is not a coincidence that when learning linear regression, we will extend it to poisson and logistic regression.\nYou can notice how the issues of sample size creep in, as well as how to properly model variation within and between groups. I recommend you look up again the CLT, in the next section\nNote that prior choice and justification is an art and science: you will have to learn and practice how to articulate assumptions and encode your domain knowledge into the priors. There is no universal recipe, but there are some guiding principles.\n\n\n\n\n\n\nPoisson Distribution. Gamma-Poisson Model\n\n\n\nCounts of independent events in a unit of (space/time/…), with a low probability. You can review the maths here. Below is a list of applications you can practice on:\n\nDeaths by horses in Prussian Army. Here is the historical data and a blog post if you need a refresher on Poisson distribution.\nAsthma mortality (BDA3, Ch2)\nAirplane fatal accidents and passenger deaths\nEstimating WWII production of German tanks based on samples captured\nComparing football teams and goals in football matches\nComparing birth rates in two hospitals\n\nCheck out the link functions for more sophisticated models. Also, in the examples above, we estimate the groups separately (corresponding to no pooling) – there are better ways. Also, you will see a poisson example of how to take into account the sample size\n\n\nThe next examples are a little detour, to appreciate the flexibility of the modeling approach we’re taking. We’re building upon previous models, by inferring which rate \\(\\lambda_1\\) or \\(\\lambda_2\\) is the most plausible at a given point in time. This way, we add complexity and realism to the model, by incorporating knowledge about the phenomenon we’re interested in.\nIdeally, we would leverage models which work well with time-series, like Hidden Markov Models. There is also a large literature in mining subsequences in a time series.\n\n\n\n\n\n\nPoisson changepoint detection\n\n\n\nEstimating rates, modeling a structural shift/change is a relevant, challenging, and unsolved problem in many fields. The models below are too simplistic to be useful in practice, but they capture the essence of real dynamics: things change not only continuously, but also structurally.\n\nCoal Mining disasters, pymc\nText Messages, pymc\nU.S. School Shootings, is there an increase in attempts and occurences?\n\n\n\nBy now, you encountered many simple Bayesian models, so it’s useful to contrast it with the Fisherian and Neyman-Pearson (frequentist) approaches. The motivation is simple: the likelihood approach is widely used in Machine Learning (and Statistical Learning) teaching and practice. A nuanced understanding of frequentist methods will improve the way you ask statistical questions, perform and design experiments.\nIn the courses I teach, I dedicate quite a lot of time on how not to fall into the most common pitfalls when applying frequentist methods. It’s an useful skill when critically reading the literature.\nWhen we get to the topic of A/B testing and experiment design, we will unavoidably stumble upon a few fascinating philosophical questions in relation to the nature of evidence. A quick overview of the below resources will give you most background you need to understand the heck those philosophical positions are about.\n\n\n\n\n\n\nBayesian vs Frequentist vs Fisherian Inference\n\n\n\n\nI like very much the following metaphor, explained here, of looking at these 3 approaches as a: Path of Action, Path of Devotion, Path of Knowledge.\nWatch this lecture by Zoltan Dienes to get a sense of the orthodox, Neyman-Pearson approach: its power and limitations.\nLikelihood - either from previous books or this interactive visualization. You can use the lecture and lab from TU Eindhoven courses mentioned below.\nBayesian vs Frequentist inference - chapter 2, 3, 4 in Computer Age Statistical Inference. This lecture by Zoltan Dienes contrasts Bayes Factors vs classical methods in t-test situations.\nThe best teaching of (mostly) frequentist statistics I know of is in these two courses by TU Eindhoven and it is well worth your time:\n\nImproving your statistical questions, coursera\nImproving your statistical inferences, coursera\n\n\n\n\nWe already worked with multiple parameters, even touching upon the relationship between two variables: counts and time \\(Y_t\\), but not really – it’s more helpful to think about that in terms of stochastic processes. Therefore, we need a new tool, which is a link function, a nonlinear transformation \\(g(x)\\) which maps \\(X\\) to the correct support of \\(Y\\). I recommend to introduce this before jumping into linear regression (LR), in order not to have the (flawed) impression that the LR is the only game in the town.\n\n\n\n\n\n\nLink functions. Golf case-study\n\n\n\nThe domain/geometry inspired, custom model is presented in pymc version, stan by Andrew Gelman, and stan by Aki Vehtari. It is modeling the relationship between distance and the probability of put, which is nonlinear and the sigmoid function won’t work well for this case.\n\n\n\n\nThis is an appropriate point to introduce linear regression and logistic regression. It is not too early, given the importance of those tools, however the presentation should be practical and pretty high level, as there is much nuance to deep-dive later.\nOf course, we cannot forget about the Normal/Gaussian distribution, which is so prevalent in nature and pops up everywhere in the statistical practice. It is the building block of many following models. Remember the key idea of the expectation of independent random variables and estimating the mean from samples. Also, keep in mind any time you’re doing regression that it’s all about the conditional expectation \\(\\mathbb{E}_\\theta[Y|X=x]\\).\n\n\n\n\n\n\nInverseGamma-Normal\n\n\n\n\nYou can find the theory and mathematical exposition in Bayes Rules, with a case-study of football concussions study\nSpeed of light experiment (BDA3, Ch3), data. You will find here and in any statistics textbook the cases of known and unknown variance, and how the \\(t_k\\) test is derived from the first principles.\nAs in the case of proportions, we can use the model above to model the difference between the means of two independent groups.\n\nTODO: more examples from science, business, and human behavior"
  },
  {
    "objectID": "references.html#groups-and-partial-pooling",
    "href": "references.html#groups-and-partial-pooling",
    "title": "Decision Science Course",
    "section": "Groups and Partial Pooling",
    "text": "Groups and Partial Pooling\nWe already worked with groups in the case of difference in means (proportions and continuous variables) and making inferences for three and more groups, treating them as separate. We will see that such an approach is called “no pooling”.\nIn the traditional teaching of statistics, the above would be covered by t-test, tests for proportions, and when it comes to groups, by ANOVA. If you were lucky, these would’ve been treated as particular versions of linear regression, like in “Most common statistical tests are linear models”.\n\n\nOne more reason for this is that groups and categorical variables do not receive the deserved, nuanced exposition in linear regression. Also, comparing groups is so widespread, that having a tool to deal with the challenges which it poses is immediately useful in your practice inside any firm.\nIn this section, the goal is to show the idea of hierarchical models and partial pooling. I agree with BDA3 approach to teach it before regression, as the latter needs a lot of nuance and a long time to learn to apply properly.\n\n\n\n\n\n\nBeta-Binomial for groups. Normal Approximation\n\n\n\nThere is no point in repeating all from the first section, as it is straightforward to apply for groups, as you’ll see when we compare it with a hierarchical approach.\nHowever, it is a good chance for a frequentist detour, to the Agresti-Coull confidence intervals, \\(\\chi^2\\) tests of independence, and nonparametric tests for proportions in multiple groups.\n\nErica Ma has a great talk for hypothesis testing for 2+ groups.\nI think the authoritative resource on Bayesian versions of the distribution-free methods for hypothesis testing is “Bayesian Statistics for Experimental Scientists”, found here. Unfortunately, it is expensive, so I suggest you look at the table of contents and search for the implementations elsewhere.\n\n\n\nForeshadowing the module 2b on A/B testing is the topic of Multi-Armed Bandits. There are cases in which we are testing multiple groups, e.g. which images to show on the website, and we do it at scale. Moreover, we want to experiment countinuously and automatically, trading off between exploration and exploitation to maximize the payoff or minimize regret.\nMABs are a big topic in itself, and a very narrow, particular example of reinforcement learning. It can be very powerful when carefully designed and applied appropriately.\n\n\n\n\n\n\nA/B Testing and Multi-Armed Bandits\n\n\n\nI suggest you start from the didactic examples in Bayesian Methods for Hackers, Chapter 6. If you have an use-case in which this fits perfectly, you can check out the theory and more variants to implement it in more specialized resources.\n\n\nWe encountered the problem of sample size when ranking reddit posts, but it deserves a few lectures in itself. Once you master a few methods of reasoning about \\(n\\), you can cut through so much bullshit in media, research, and literature.\n\n\n\n\n\n\nThe most dangerous equation\n\n\n\nI think that “The most dangerous equation” is a must read for anyone, not just practicing scientists and statisticians. You can see below two resources for the mathematical background of LLT, CLT, and some key properties of estimators.\n\nCentral Limit Theorem (Kolmogorov), deMoivre - the most dangerous equation. Asymptotics. the theorem and simulations here\nEstimator properties: Bias, Consistency, Efficiency - you can find an accessible explanation with R code in openforecast\n\nContinuing on the reddit examples, there are some amazing case-studies in the “Calling Bullshit” website and book. One of them is exactly such a ranking problem: best barbecue in the states. I recommend you watch the whole playlist and work through the case studies: it is fun and an essential skill – to call out the bullshit.\n\n\nFor the continuity with the previous “Building Blocks” section, here are a few example for groups, where the dependent variable is following the Poisson distribution. In this case, the novelty is choosing a gamma prior based on the sample size information of each group. If you think we can do better than this trick – you’re totally right.\n\n\n\n\n\n\nGamma-Poisson for groups\n\n\n\nThe models become more complicated as we attempt to estimate parameters for each group of \\(n\\) observations:\n\nKidney Cancer rates, with priors chosen in accordance to the sample size (BDA3, Ch2). An R visualization\nGuns and suicides, with ideas from empirical and hierarchical Bayes.\n\n\n\n\n\nJust as a remark, complete pooling is when we ignore the fact that we have groups, and make one, global estimate. Of course, it would fall in the category of underfitting or model mis-specification, if the categories or groups are relevant.\nFinally, we’re ready to see how partial pooling and hierarchical (multilevel) models are such an important and powerful innovation, to the point where some practitioners argue (and I agree), that it should be the default way of (regression) modeling. Meaning, a strong justification is needed why your model doesn’t need that structure or modeling of heterogeneity. Keep in mind this advice, but always start with the most simple models when iterating towards the final one.\n\n\n\n\n\n\nHierarchical Models for Groups\n\n\n\nGaussian. The main example is a classic: modeling the level of radon in houses on the first floor and basement, where the groups are counties (BDA3). We will see this example again in the Hierarchical GLMs section, where predictors at house-level and county-level are added.\n\nOmar Sosa - Practical introduction to Bayesian hierarchical modeling with numpyro, focuses on exactly the idea we need.\nPrimary code reference: pymc - A Primer on Bayesian Methods for Multilevel Modeling\n\nBeta-Bionmial examples:\n\nEric Ma tutorial at pycon, about baseball batting, and the equivalent numpyro code. Another baseball example is a classic by Efron, implemented in pymc.\nPolice shooting training – detecting race bias. A full Bayesian workflow in bambi.\nAnother classic example is about Rat Tumors experiment, implemented in pymc, from BDA3, Ch5.\n\nA great Gamma-Poisson example is presented by Richard McElreath in Statistical Rethinking lectures: “Starbucks coffee-shops waiting time in queue”.\n\n\nOne of my favorite business applications of the ideas outlined in this section, is in the seemingly innocent problem of computing the lifetime value of a customer. Just to show how tricky this problem is, there is a niche, but extensive literature on the probem, starting from 2000-2005 by Rossi, Fader, and Hardie, which is entering into the mainstream only now, by 2020-2023.\n\n\nThe below is by far not the only model: there are versions for contractual and non-contractual settings with different assumptions. The culmination of this research, in my opinion, is Eva Ascarza’s 2013 paper.\n\n\n\n\n\n\nEstimating Customer Lifetime Value\n\n\n\nModel: Combining Gamma-Poisson and Beta-Binomial, with parameters at customer level. The math and code in pymc3 can be found here."
  },
  {
    "objectID": "references.html#gaussian-linear-regression",
    "href": "references.html#gaussian-linear-regression",
    "title": "Decision Science Course",
    "section": "Gaussian Linear Regression",
    "text": "Gaussian Linear Regression\nThere is a converging consensus that Linear Regression is the most important thing to master in statistics. I mean it in the most general sense – not only knowing model details, assumptions, extensions and limitations, but how to use it effectively in practice in an end-to-end workflow.\nTo avoid naming confusions in the context of GLMs we’re going to study, and the fact that Regression can be done by many classes of models, I call the good old Linear Regression – “Gaussian Linear Regression”\n\n\n\n\n\n\nExamples. Introduction to the workflow\n\n\n\nI highly recommend the (freely available) book “Regression and Other Stories”, by Andrew Gelman, Jennifer Hill and Aki Vehtari. The latter has a course website for this book, with the examples coded in stan.\nAlso, the second resource, which I would say is even beter, takes on the perspective of causal inference from the very beginning of studying regression. In the course homepage you will find links to the pymc port, slides, and video lectures from 2023.\n\nEugene-Springfield community sample data: OCEAN personality traits as related to drug usage – no categorical variables, \\(Y\\) distribution normal-ish. Demo in bambi.\nEducational outcomes for hearing-impaired children, in pymc\nDangers of spurious correlation and accounting for them are displayed in the marriages and waffles example by McElreath – written in numpyro, but there is also a pymc version.\npymc moderation analysis: muscle mass and age\nThis case study about 100m runs from Calling Bullshit, shows the dangers of extrapolation beyond the range of \\(X\\) that the models were trained on.\n\n\n\nI have a third favorite book, which is freely available, called “Beyond Multiple Linear Regression”. It covers all that we discuss here, but from a frequentist perspective. Despite that, the examples are amazing and it has enough of theory to make it a self-contained resource – therefore, makes a perfect candidate for a port in pymc/numpyro. For a review of linear regression, take a look at the Kentucky derby horse race.\n\n\n\n\n\n\nCommon statistical tests are linear models\n\n\n\nNow, once you got the absolute basics of regression, it’s important to realize that a lot of seemingly unrelated statistical tests in frequentist statistics are particular versions of linear models4.4 This will instantly systematize the zoo of tests, when to use them and what they do. DO NOT TRY TO MEMORIZE THEM! Instead, think very carefully about your use-case.\n\npymc - Krutsche fake data drug trial, t-test, bambi.\nCommon statistical tests are linear models and the python port\n\n\n\nUp until this point we took the sampling from the posterior provided by the probabilistic programming languages for granted, as magically converging to the true distribution. We also used very rudimentary methods of comparing models (via Bayes Factors).\nHowever, things go wrong in practice all the time: poorly specified and parametrized models lead to computational problems. It is important to be able to diagnose the chains, and even more, actively critique and interogate your models. At some point, you have to get an understanding of what MCMC, HMC does under the hood – and when you would trade off the accuracy for faster, approximate inference.\n\n\n\n\n\n\nSplines and Nonlinear Transformations\n\n\n\nThe linearity assumption in linear regression is often under-emphasized, as pointed out by the trio of RoS in a podcast episode. Linear Models are linear in parameters, but we should always think of the appropriate transformations of \\(y\\) and \\(X\\).\n\nSplines from Osvaldo Martin, on Bike Ridership (UCI data, bike sharing)\nSplines, from Statistical Rethinking, on Cherry Blossoms Data\nBe careful and recognize when you should be using a log-log model, expecially since power laws are so widespread in nature and human behavior.\n\n\n\nThere are objections to the Bias-Variance decomposition when seen as a tradeoff, in the context of Deep Learning – however, in the most general sense, it is a universal problem not only in statistics, but also for human cognition.\nBy introducing splines and nonlinear transformations, we can easily increase the dimensionality of \\(X\\), with respect to \\(n\\) – which causes massive problems for inference. This is the appropriate moment to introduce a fundamental tradeoff between model complexity and out-of-sample predictive performance.\n\n\n\n\n\n\nModel Critique and Evaluation\n\n\n\nChecking domain and modeling assumptions, critiquing the models we build are one of the most difficult aspect of statistical practice. It is hard to give a tutorial for this, so I just suggest you read Richard McElreath’s Statistical Rethinking chapters on this topic, along with the more theoretical BDA3, Part II.\n\nFor an intuitive process which can structure your modeling and critique, I suggest McElreath’s “Drawing the Bayesian Owl”.\nChapter 2 of Bayesian Computation in Python, by Osvaldo Martin (BCP) is one of the best at exemplifying these concepts.\nPrior and posterior checks pymc docs\nAn end-to-end example in BCP, modeling the flight delays.\n\nThe frequentists and statistical learning approach have well-established tools for asessing the out-of-sample predictive performance. The fact that training is fast, they can easily use cross-validation and even bootstrap. A recent innovation in Bayes is LOO-PIT, an approximation to leave-one-out cross-validation, which leverages the fact that we have posterior sampling.\n\nA reminder of bootstrap, that is what the frequentists might do - chapter 10 or page 249. It is a powerful tool to have in your arsenal anyways.\nHere are the technical details of LOO-PIT, in the context of model comparison.\n\n\n\nOnce we are able to critique and evaluate a single model, it makes perfect sense to compare models of increasing complexity and with alternative assumptions. The following uses the same tools as before, but in a iterative workflow.\n\n\n\n\n\n\nModel Comparison and Selection\n\n\n\n\npymc. - Model selection with fake data and polynomials\nChapter 3 and 4 of BCP are beautiful, where linear and logistic regression are introduced, along with questions of interactions, nonlinear transformations, hierarchical models and model choice.\nBias-Variance tradeoff - For an intuitive explanation, watch lecture 8, slides. See how this tradeoff needs an update for the modern machine learning.\n\n\n\nThere are cases when the problem is in sampling and one common cause, besides mis-specification, are models with bad posterior geometry (from the perspective of samplers). As a rule of thumb “round and smooth” is good. A prevalent solution in literature is to use better parametrizations, but in order to fix your problem, you first have to be aware that it might be the case.\nThese examples in 2D are intuitive, however things become much more complicated and counterintuitive in multidimensional parameter spaces. The long story short, in practice, is to use heavier regularization (via stronger priors) in those cases.\n\n\n\n\n\n\nBad posterior geometry. Reparametrization\n\n\n\n\nTechnical: (mu, sigma) - Neal’s Funnel numpyro, there is equivalent pymc\nMore solutions in numpyro for bad posterior geometry\n\n\n\n\n\n\n\n\n\nRegularization and Variable Selection\n\n\n\nA big topic in ML is regularization and encouraging sparse solutions (models). The point is that in highly-dimensional spaces, only a small subset of variables is relevant. Bayesians have sneaky methods of constructing prior which act as a mechanism for variable selection.\nThere is an additional topic of robuts methods (to outliers). Some machine learning models are robust(ish) out of the box: quantile regression, gradient boosted trees. Bayesians use heavier-tailed distribution to achieve a similar effect.\n\nSpike and Slab kaggle\nHorseshoe prior pymc3\nRobust LR, bambi, simulated data, also is in pymc\n\n\n\nI can’t resist to suggest a case-study from finance, the good old portfolio returns optimization. The methods used in practice are much more sophisticated than the below, but it’s useful to be aware of the fundamentals. The Wishart distribution, used as a prior for positive semi-definite matrices (covariation), can be an useful building block in more complicated models to capture the covariation structure, let’s say, from different time series.\nAs a very advanced aside, I am fascinated by Michael I. Jordan’s approach to multidimensional time series modeling, where the covariation structure is evolving in time.\n\n\n\n\n\n\nPortfolio optimization a la Markowitz\n\n\n\n\nTechnical: Wishart and Portfolios from BMH – can also do optimization ala markowitz, but with uncertainty from posteriors\n\n\n\nIf you got to this last topic, probably months have passed. There are a reason universities dedicate entire semesters for regression – there are so many nuances to get right, from both practical and theoretical perspective. One of these complexities is how to handle missing data, which deserves a course on its own.\n\n\n\n\n\n\nModeling how Data goes Missing\n\n\n\nIn Bayesian Statistics, we also have to be very explicit and declare our assumptions about how the data went missing – which is a hard, but beneficial thing to do. The good news – missingness is treated like an unobserved variable and is subject to the same inferences we’ve been doing before. We just need to pick the right model of missingness for each individual case.\n\nMissing data imputation, pymc, both for linear and hierarchical\nDiscrete missing data imputation numpyro, with simulated data and causal graphs\nPymcon - missing data tutorial in pymc3\nMissing Data Imputation"
  },
  {
    "objectID": "references.html#generalized-linear-models",
    "href": "references.html#generalized-linear-models",
    "title": "Decision Science Course",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nNot every phenomena we model is continuous or appropriate for gaussian linear regression, but we can easily extend it with the building blocks we learned at the beginning. It’s all about those link functions and relaxing some of the assumptions of the linear regression. I want to point out how the following mirrors the simple, single-variable models we first practiced on.\nI do not want to make the transition from gaussian case to logistic, poisson and other variants sound trivial. Their behavior, evaluation, and interpretation is different and distinct. You can not equivocate them.\nAs a theoretical background of what connects these models into one single family, hence the name of GLMs, I suggest you read the section in the “Beyond MLR” about the exponential family. Richard McElreath has similar information-theoretic arguments in his lectures on Statistical Rethinking.\n\n\n\n\n\n\nLogistic Regression\n\n\n\nAs a fun aside about ML, logistic regression is the only model which is calibrated out-of-the-box, meaning the scores for the classification can be interpreted as probabilities, due to its specific loss function.\nFor the rest of the cases, you would need to calibrate via an isotonic or piecewise-linear regression, on a separate, validation dataset. So, going back to the practice, here are some examples using the logistic regression:\n\nBeetles survival by concentration of chemical, via bambi – tries out different link functions and is a simple example to get started with.\nVote intention by age in the U.S., via bambi, 2016 ANES data, 1200 samples.\nSocio-economic influences on income bracket (\\(\\ge \\$50k\\)), also does model comparison, implementation in bambi\nMultinomial regression iris, via bambi, with the most boring dataset, but you’ll see that you have a solution for the multiclass classification without the multiple comparisons.\n\n\n\nWhen you have a numeric target variable, think twice if it is appropriate for the gaussian distribution. Sometimes, what we model is really, counts, or non-negative, or monotonically increasing with respect to a \\(X_i\\) (that alone deserves its own case-study). Sometimes, even poisson doesn’t work out due to overdispersion or zero-inflation, and it is not a rare exception, especially when modeling aspects of customer behavior.\n\n\n\n\n\n\nPoisson Regression\n\n\n\n\nNumber of laws regarding equality, which includes a discussion for the issue of overdispersion. Not sure at all that this would be a prototypical DGP story for a Poisson.\nStop and frisk data from BDA3 and RoS, the frequentist version\nCampus crime and estimating household size in Philippines from BeyondMLR.\nAlcohol and meds interaction, with simulated data.\n\n\n\n\n\n\n\n\n\nOverdispersion. Negative Binomial. Zero-Inflation\n\n\n\nThe negative binomial distribution is often found in customer behavior, in contractual or non-contractual settings, when it comes to purchases, subscription renewals.\n\nCockroaches and pest management, where Negative-Binomial, Poisson and Zero-Inflated NBD is investigated.\nFishing catches in a park, in numpyro\nStudents’ absence, UCLA data, application of negative binomial, written in bambi\n\n\n\nIn the probability fundamentals section, we discussed the use-case of estimating proportions and named the field of compositional data analysis. It is often found in practice, but disguises itself, which causes the wrong method to be applied.\nIf you encountered such a problem, when you care about proportions, mix of stuff, or compositions, not their absolute values or quantities, you can check out these lecture notes\n\n\n\n\n\n\nProportions. Compositional Data Analysis\n\n\n\nDirichlet regression, pymc3 - fake proportions dataset, but take some real ones from compositional data analysis books"
  },
  {
    "objectID": "references.html#hierarchical-glms",
    "href": "references.html#hierarchical-glms",
    "title": "Decision Science Course",
    "section": "Hierarchical GLMs",
    "text": "Hierarchical GLMs\nWe finally reached to the most exciting point in our journey so far, where we can properly model and explain sources of variation at multiple levels (of analysis). This is where we relax the assumption of iid, replacing it with one of exchangeability.\nThe point is that correlated data causes problems in modeling, when we don’t account properly for it: be it groups, clusters, categorical variables, time series, geography, etc. BeyondMLR has a very well thought, practical motivation for multilevel models.\n\n\n\n\n\n\nHierarchical Gaussian Regression\n\n\n\nWe come back to the radon example, by adding one covariate at each level: house and county. This explains a part of variation which was missed before and leverages pooling to take care of low sample size in some counties.\n\nRadon: Primary code reference: pymc - A Primer on Bayesian Methods for Multilevel Modeling\n\nbambinos a higher level API, models the log-radon. Alternatively, McStanPy implementation\nOmar Sosa - Practical introduction to Bayesian hierarchical modeling with numpyro\n\nBayesian Multilevel Regression numpyro on OSIC Pulmonary Fibrosis Progression data, which assesses the risks of smoking and not only.\nBeyondMLR presents a really interesting psychological study about stage anxiety for music performers at a conservatory.\nA repeated analysis of Stack’s facial feedback hypothesis, in the context of replication crisis, via bambi – full workflow.\n\n\n\nLongitudinal data is a special case of multilevel models, but has the distinct feature, that at the group level, the data isn’t iid, but comes as a realization of the stochastic process.\nOften called Panel Data in economics and social sciences. You will see a different terminology in that literature: of mixed models.\n\n\n\n\n\n\nLongitudinal Data and Studies\n\n\n\nThese examples model the trend with a covariate, as a function of time – which is a happy case when we can do that. However, in practice, things become more complicated if we have to model and identify a stochastic process like \\(AR(k)\\), or \\(MA(n)\\).\n\nLongitudinal data with drinking teens, alcohol consumption per cohorts, pymc.\nSleep study and reaction times in time by subject. The same modeling idea is in pig growth study, both implemented in bambi.\nBeyond MLR charter schools longitudinal study.\n\n\n\nIf in the previous sections, I didn’t have a strong preference for the Bayesian Approach, in the case of multilevel models, I strongly believe Bayesian Inference to be superior and less prone to overfitting and numerical instability.\nAnother aspect of this, is that most mixed models packages will make certain decisions for us, without our consent, which could influence our results and conclusions. As you’ll see in these tutorials, we are forced to construct the model and declare, justify every choice of prior and model structure.\n\n\n\n\n\n\nHierarchical Logistic Regression\n\n\n\n\nBeyond MLR, college basketball referee foul differential here\nGraduate Admissions, from McElreath,  UC Berkeley in Fall 1973 (numpyro)\nRasch item-response model for Nba fouls in crunch time, pymc\n\n\n\n\n\n\n\n\n\nHierarchical Poisson Regression\n\n\n\n\nAirbnb number of reviews\nEstimating the strength of a rugby team\nPaper investigating seat-belt use rates, with data probably taken from the department of transportation crashes website\n\n\n\n\n\n\n\n\n\nModels with 3 Levels\n\n\n\nAt last, it is useful to see an example where we have an even more complicated multilevel structure, as it will be the case in some practical applications.\n\nBeyond MLR 3-level seed germination\n\n\n\nThis is really, the end of the Module 2a. Applied Bayesian Statistics. These tools are so general and powerful, that they will last you a lifetime. However, there is so much more to learn and so many more challenges to tackle – that there is no end to the mastery of Multilevel GLMs.\nEven after you learned all the technical intricacies, we’re hit again with the reality that association doesn’t imply causation. So, after being competent with this amazing tool – we have to go back to ideas from causal inference, if we want to get a more than associative insight about our theory and understanding of phenomena."
  },
  {
    "objectID": "references.html#bayesian-machine-learning",
    "href": "references.html#bayesian-machine-learning",
    "title": "Decision Science Course",
    "section": "Bayesian Machine Learning",
    "text": "Bayesian Machine Learning\nAs a bouns, I’m adding two extremely flexible, general-purpose models, which you can treat as in the Machine Learning practices, but are Bayesian to the core. These are extremely useful if we care more about predictive accuracy, either for decision-making, or as a part of causal inference process, where a certain complicated relationship details aren’t important to us, just the conditional average treatment effects.\n\n\n\n\n\n\nBayesian Additive Regression Trees\n\n\n\n\nBART from osvaldo, on bike shares\nPodcast Episode and an R package\nNonparametric methods - what is the benefit, name a few equivalents. Be able to explain a signed-rank transformation. This is the simplest and the most accessible explanation I know of so far.\n\n\n\n\n\n\n\n\n\nGaussian Processes\n\n\n\n\nGelman: Birthdays, hilbert space approximation repo, also in stan"
  },
  {
    "objectID": "references.html#bibliography",
    "href": "references.html#bibliography",
    "title": "Decision Science Course",
    "section": "Bibliography",
    "text": "Bibliography\n- title : Bayesian Data Analysis\n  title_short: bda3\n  type: book\n  edition: 3\n  author: Andrew Gelman\n  year: 2013\n  link: http://www.stat.columbia.edu/~gelman/book/\n  lectures: https://avehtari.github.io/BDA_course_Aalto/Aalto2022.html\n\n- title: Bayesian Methods for Hackers\n  title_short: bmh\n  type: book\n  edition: 1\n  author: Cameron Davidson\n  year: 2015\n  link: https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/\n  github: https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\n\n- title: Statistical Rethinking\n  title_short: rethinking\n  type: book\n  edition: 2\n  author: Richard McElreath\n  year: 2021\n  link: https://xcelab.net/rm/statistical-rethinking/\n  lectures: https://github.com/rmcelreath/stat_rethinking_2023\n\n- title: The Most Dangerous Equation\n  title_short: danger-eqn\n  type: article\n  author: Howard Wainer\n  year: 2009\n  link: http://assets.press.princeton.edu/chapters/s8863.pdf\n\n- title : Introduction to Probability\n  title_short: probability-blitzstein\n  type: book\n  edition: 2\n  author: Joe Blitzstein\n  year: 2019\n  link: https://projects.iq.harvard.edu/stat110/home\n  lectures: https://projects.iq.harvard.edu/stat110/youtube"
  },
  {
    "objectID": "01_fundamentals/background.html",
    "href": "01_fundamentals/background.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "Arguably, we live in a volatile, uncertain, complex, and ambiguous world that we have to understand in order to navigate it well. By “world”, I mean the economy, society, environment, or any other complex system – especially when human behavior is involved. It is reasonable to ask: wasn’t it always the case, depending on how we define the terms, level of analysis, and point of view?\nI think it’s a matter of scale and magnitude in VUCA dimensions, where an accelerating rate of change poses a great challenge to our capacity to adapt. Instead of an evolutionary or philosophical comment on why we need this field of data/decision science, I have two questions on my mind when it comes to living well or improving business outcomes: what is true and how should I act? 11 This implies that we already framed the problem and figured out what we want to achieve, and that indeed we chose the right goals and objectives. Moreover, the questions of epistemiology and ethics are a never ending topic for discussion and enquiry.\n\n\n\n\n\n\nDecision Science: (not only) truth and action\n\n\n\nLet’s start from a business setting of an e-commerce, where we want to increase sales, customer satisfaction, and reduce costs. Imagine three scenarios, which neatly fall into the SWOT framework: 22 \n\n\nSource: Kim Warren - Status Quo, Desired and Feared Trajectories\n\n\n\n\nWe keep the status quo, doing everyting as before. What is the most likely trajectory of profits? Can we come up with an educated guess? If the trajectory looks good, that is our strenghts and compentencies contributing to it, if not, our weakness.\nA feared trajectory, that is, if our business is hit by a shock in supply chain, inflation, by competition or customer demand. It’s the threats.\nA desired or aspirational trajectory. Is it reasonable and realistically achievable? If yes, what strategy and tactics should we implement, how sould we act? This is our opportunity.\n\nAfter this exercise, we defined more precisely where we stand, that is, quantify the current state of the firm. We framed the problem in terms of most relevant outcomes and we’re in the process of figuring out what is the optimal goals to aim for. Obviously, we need a mechanism, measurements to know that we’re on track and to recognize when to get there. Now, let’s go back to our two questions and unpack them:\n\nWhat is true? In the most general sense, we’re not asking for a mathematical and logical truth, but if it’s plausible, probable, deserves serious consideration, is backed by evidence. I also mean that we understand the underlying causal mechanisms. Not least, an assessment of the current situation. The metaphor which I suggest for this is “seeing clearly”, through the fog, illusions, and biases. 3\nHow should I act? What is plausible doesn’t entirely answer this question, we can’t derive an ought from is.4 In business settings, I would think about action in terms of strategic alignment and optimisation.\n\n3 Recommended reading: Scout Mindset by Julia Galef4 This is one of the most important and applicable philosophical ideas, introduced by David Hume: when one makes claims about what ought to be that are based solely on statements about what is.The only missing pieces from this mental model that I argue for is tremendously important: iteration and feedback. 5 Due to VUCA, we can’t be sure our actions are optimal, or even that we’re solving the right problem, therefore fast iteration and feedback ensures we’re not taking too much risk, that we find out early about problems in our thinking and action, that we can change course to steer the ship back on track.5 Sounds an awflul lot like Cybernetics, doesn’t it? Especially if we have the idea of a firm as a complex adaptive system as a pressuposition for this discussion.\n\n\n\nNow that it’s more clear what I meant in the course introduction by improving business outcomes and bringing value to organizations, I didn’t yet explain what does analytics, data science, machine learning, and AI do or are, and how to they fit in the picture we painted so far.\nYou might’ve noticed that every lecture starts with a brief motivation, and then will have this kind of flowchart of arguments and ideas. It is supposed to be your guide and a roadmap, so that you don’t get lost in various detours taken and keep the big picture in mind. My recommendation is that you go back to this diagram at the end of the reading or lecture, try to remember individual arguments and think for yourself how are they related, what is the golden thread connecting them. 66 This is a course in which we look at the widest possible range of methods and models, without going into depth, as I don’t know which ones will be useful for your particular applications and problems. The idea is to find the optimal tool for the job when you encounter it and learn the details later, how to actually do it. We dive into more detail when presenting “workhorse-models”, proven by practice to apply well in a large variety of use-cases.\n\n\n\n\n\nflowchart TD\n  DM[Decision-Making] --> Domains[Domains] -- case studies --> U[Uber / Rides] --> Ecom[E-Commerce]\n  DM --> AI[What is AI?] --> Cy[Cybernetics Detour] --> Choice[Stats/ML/Analytics]\n  Choice  --> F[Why did you study those?]\n  DM --> BS[on Bullshit] --> Fool[Foolishness] --> Caus[Causal vs Correlational] --> BD[Big Data?]\n\n  Ecom --> BYOP[Bring your own problem]\n  F --> BYOP\n  BD --> BYOP\n\n  style U fill:#f7f5bc\n  style Ecom fill:#f7f5bc\n\n\n\n\n\nFigure 1: A mind map and roadmap of the ideas and topics in this lecture. It is designed to give you a wide context in which AI is used and misused. The classroom version is designed to be participatory and conversational, but I will try to do my best to communicate that spirit in written form\n\n\n\n\nWe start with real-world applications of data science, define what AI, Cybernetics, Big Data, Analytics, Machine Learning mean. Then, we figure out why did we study all those mathy and computer science subjects during the Bachelor’s degree. Next, we discuss what can go wrong while drawing conclusions from data – culminating in a discussion of a problem of choice you’re passionate about.\n\n\nAt this point, you might get tired of me emphasizing the decision-making aspect of data science as the main point of why it is important. It’s time we move from the general and abstact towards particular examples and applications in various industries. This will lead us to acknowledge how prevalent is AI (that we haven’t fully defined yet) in firms, services and technologies we use every day. 77 When reading this section, I want to get you into a mindset of the reverse engineer: step back and think deeply about products and services you use every day, put yourself in the shoes of the business making those decisions and building those systems. What were they thinking about? What challenges were they facing that were an appropriate use-case for ML, Statistics, and AI?\n\n\n\n\n\n\nDuring a lecture, I usually ask students\n\n\n\nCan you give some examples of businesses, sevices, technologies, problems, and domains which you suspect do have AI/ML algorithms and models behind the scenes?\nSee below some very good answers and argumentations provided by the students last year, then we’ll examine in more details one of them. Of course, there is always that one person, very passionate about sports or blockchain.\n\n\n\nDynamic pricing in Bolt and Uber, which takes into account the weather, especially if it rains, peak hours: balancing demand and supply. It is at the intersection of ML and economic theory, as they are a platform or marketplace. Prices also change with respect to competitors, so we see aspects of an oligopoly behavior. 8\n\nStock markets and trading bots: at the intersection of economics, finance and AI. I would add the “good old” and boring portfolio management and venture capital enterprises.\nManagement consulting: what market to enter, whether and how to build a new product (product development). Lots of use-cases in marketing and market research firms.\nMedicine Applications: developing new vaccines and drugs, aided by AI and designing clinical trials for novel treatments.\nBanks and insurance: risk management, predicting credit defaults on mortgages and business loans. Chatbots for customer support, for most frequent and simple questions.\nAutomotive: routine tasks like automated parking, the race towards self-driving, autonomous or semi-autonomous cars, safety warnings. Predictive maintainance is tackling a problem where they leverage predictions to replace risky parts before they go out of function.\nLiverpool F.C. won a title, and a key part of their success was leveraging AI and ML to discover new tactis on the field with the highest payoffs. 9\nNBA teams invested a lot in the data infrastructure and decision-making capabilities: LA Lakers found the best player at the moment for a particular position they were lacking and would play well along with the team. Rockets won the regular championship divison by going all in on the 3-point shot.10 Golden State Warriors simply revolutionised basketball with data, before everyone else was doing it – giving them a competitive edge. 11\n\n8 When it doesn’t work out – I’m pretty upset at their data scientists and domain experts. Here is where ethical issues creep up: jacking up prices, monopolies, drivers struggling to make a living wage.9 TODO: Reference article and maybe dataset for more details10 Moreyball: The Houston Rockets and Analytics – an article in Harvard’s MBA Digital Innovation11 Check out the following video by The Economist on how data transformed the NBA. For more details on the statistical methodology, I enjoyed an youtube channel called Thinking Basketball and their playlist about the statistical methodology.In all of the examples above, those businesses and systems do have to make decisions, under uncertainty from multiple sources, trying to solve complex problems at a large scale, which would be impossible to do manually even with an army of employees.\nI would like to add a few more examples, from an insider and practitioner’s perspective, which might not be as impressive and a bit routine, but no less important. Keep in mind, that if at a closer look, the service seems to do something relatively intelligent very fast, specialized AI might be involved behind the scenes.\n\nDemand Planning: How many SKUs (items) should I order for manufacturing, to satisfy the demand (that last item on the shelf, minimizing lost sales) and to minimize excess inventory.\nLogistics and Supply Chain: routing, distribution center operations and automations for order fulfillment, return management\nRecommender systems for music, videos, books, products, news in social media, services, platforms, and e-commerces like facebook, instagram, tiktok, youtube, spotify, amazon, emag. You can find recommendations in surprising places, like google maps.\nProgrammatic Advertisement: finding best placement for ads on various platforms, right now dominated by meta and google\n\n\n\n\n\n\n\nCheck out the Case Studies for a Deeper Understanding\n\n\n\nWe will explore some challenges and applications from this list in a series of case studies and labs. The idea is to improve our ability to identify opportunities and formulate problems from the point of view of an organisation, such that we can match those with the methods, models and algorithms discussed in the course.\nI’ll have to introduce a lot of new concepts when the language we developed so far will turn out to be insufficient to talk about and understand what’s happening inside these firms. Thus, each case study is an opportunity to play around with a novel idea. 1212 If you have a pretty good idea about what is AI, Analytics, ML, Deep Learning, Big Data, Causal Inference and when to use one approach or another, feel free to skip the history and terminology and go straight to the case studies.\n\nIn the first deep-dive, we will look at Uber and Bolt, with publicly available information, trying to figure out what do their data scientists do.\nThen, we will look at the lingerie e-commerce where I work at, AdoreMe and a different set of problems we’re facing.\n\n\n\n\n\n\nIt’s undeniable that there is a lot of excitement when it comes to AI, ML, and data science – to the point of calling it the sexiest job of 21st century. Data science is an umbrella term, with interdisciplinary at its core, drawing inspiration from multiple fields, sets of tools, practices, methods and it continuously evolves. 13 It is designed to help us tackle increasingly more complicated problems at a large scale. There is also a reasonable worry about ways in which these systems can go wrong or awry.13 There are many questions still unanswered: How does this landscape of Data Science look like? What are the roles and jobs? What is the process for building smarter, data-driven software systems; drawing more reliable inferences and conclusions from data and theory? How does a day in data scientist’s life look like?\nYou will often see a Venn diagram where data science sits at the intersection of mathematics – statistics, computer science – software engineering, and domain knowledge. I think this is not sufficient to characterise data science, therefore, will try to elaborate what it does, and how (which is as important).\n\n\n\n\n\n\nAI in a Nutshell\n\n\n\nFor all pragmatic intents and purposes, especially in businesses, AI is about Decision-Making under Uncertainty at Scale 14.14 C. Kozyrkov - AI is decision-making at scale\nOne important keyword here is uncertainty, as there is no point in building AI solutions based on complex models if we don’t have uncertainty. We have to be able to change our mind and actions in the face of evidence.\nOn the other hand, scale is the reason ML and Deep Learning is so powerful, because you can take lots of small decisions in an automated way, with little curation or guidance from humans. This is why many traditionally “paperwork” industried like legal and accounting embrace digitalisation and automation now.\n\n\nUltimately, why would I build a system which predicts demand for products in a direct-to-consumer ecommerce like Allbirds, Macy’s, or AdoreMe? Either in a marketplace like Emag or Amazon? Why would I try to find out the factors which contribute to a successful advertisement?\n\n\n\n\n\n\nHere are some answers from students\n\n\n\n\nIn order to allocate resources to the stuff which generates growth and profit. Avoid being scattered around (which I would call bad strategy), resulting in costs over targets and inefficiency.\nIn short, we attempt to allocate resources and efforts efficiently.\nWe can view information as a competitive advantage, anticipating and predicting so that we can plan and prepare, outperform competitors.\n\n\n\nWhen we talk about uncertainty, it’s important to recognize its sources: 15 one coming from incomplete information, that we always work with samples in one way or another. For example, even if at a certain point in time we might have real-time data, everything evolves and quickly becomes outdated. Everything is in a state of flux and change. Even in the current state, we don’t know for sure where we stand – sometimes, in economics, this problem is called nowcasting. When talking about the future, making a good prediction is one of the most difficult things.15 We will talk more formally about sources of uncertainty in the next lecture, while reviewing the fundamentals of statistics.\nFor example, who would’ve predicted the pandemic and all its implications on the supply chain and society? It’s important to note the difference between this kind of black swan events and the irreducible, fundamental uncertainty, which can’t be captured by any explanatory factors.\nIn a happy case, we can quantify and reduce it by conditioning the model, that is, joint distribution of random variables with a given structure, on data. That would result in inferences and evidence with respect to our hypothesis and model of the world.16 At the very least we can try to quantify how uncertain are we.16 We want to say something intelligent about the population, technically, to generalize. However, there is ambiguity, as objectives and the meaning/semantics of data fields are not always mathematically clear or without conflict.\nSo, we still have to make decisions. Those have to have a level of robustness and resilience to shocks, in the face of uncertainty. I would go even further, to suggest that we should aim for antifragility, meaning, the system improves after a shock or negative event, but that is very hard to implement and operationalize, therefore, it falls outside the scope of this course – to the realm of systems’ design.\n\n\n\n\n\n\nWhen you don’t need AI and Statistics\n\n\n\nAs a though experiment, itmagine we have an equation or program, with well-defined rules, which perfectly predicts the price on stock markets, or perfectly predicts how many items will a client buy and how she will respond if we change the price (an intervention). We won’t need machine learning, causal inference, or AI there.\nOf course, we don’t have that kind of program. It’s only somewhat true in cases when we have a well-tested theory, which stood the test of time and went through the scientific process to become the best theory with respect to all others. For example newtonian physics, relativity, quantum mechanics, evolution.\n\n\nHowever, when we talk about human behavior, we should resist the temptation and arrogance to say that we have a well-defined theory, be it normative or positive. Our preferences change, and we can “decide” in which direction they change or persist.\nRegardless of the business we work at or own, the place in the value chain, we’ll have to deal with human behavior: customers, employees, decision-makers, engineers. We need other kind of tools to infer perceptible regularities and patterns in their behavior. We will be forced in one way or another to learn from data and observation.\n\n\n\n\n\n\nA model is a simplified representation of reality\n\n\n\nWe need models to make sense of the world around us, because it is so complex and uncomprehensible if we are to represent it faithfully in a simulation. Therefore, we focus on relevant, interestig, essential aspects to us, we simplify by baking in domain knowledge, assumptions, and data into the models and algorithms.\nSo, we can collect data, apply algorithms to train models, in order to make inferences about some relevant quantities. That will help us in making evidence-based decisions which gets us closer to our objective in an efficient way.\n\n\n\n\n\n\n\n\nWeak AI is Domain-Specific\n\n\n\nBy now, you probably figured out that we’re not talking about General AI, trying to surpass human intelligence in general reasoning and problem-solving. Thus, we’re talking about weak or specialized AI, which depends very much on the domain.\nAI in an a fashion e-commerce, like AdoreMe, where we sell lingerie, will have a very different flavor from the tools and methods used in genomics, medicine, social science or psychology.\nDespite the fact that there are a lot of shared fundamentals, when it comes to the principles of building models, it is not straightforward to take something which works in one domain and apply it in another. Significant tweaks and adaptations are needed, which are dependent on the specificities of that domain.\nThe good news is that when these transdisciplinary groups of people work together and successfully adapt a method, it is often a breakthrough in the field borrowing the theory and technology.\n\n\n\n\nAt this point we have a working definition of Weak AI. At a first glance it might be hard to see what does it have in common with Cybernetics and its study of Complex Adaptive Systems.\nI’m not trying to equivocate those two, but argue that weak AI is how Cybernetics evolved and is mostly used in practice now. I will give a definition from P. Novikov, which I found tremendously useful, then explain it. Can you spot the parallels of “decision-making under uncertainty at scale” in this definition?\n\n\n\n\n\n\nA better definition of Cybernetics\n\n\n\nThe science of general regularities of control and information processing in animal, machine and human (socitey)\n\n\n\n\n\n\n\n\nUnpacking this dense/abstract definition\n\n\n\n\nControl means goal-directedness, the ability to reach the goals and objectives by taking action and stirring the system towards a trajectory. The objective can also be perserving the structural-functional organization of the system itself, an autopoesis.\nInformation Processing could be pattern recoginition, perception, how you understand and model the world, what inferences do you draw, what “data inputs” are used\nGeneral regularities means what is true and plausible of control and information processing across fields and a variety of complex systems, not only in particular cases.\nAnimal refers to applications in biology, machine – in engineering, and human – in our society and behavior.\n\nIn economic cybernetics, we’re concerned with economics, society and human behavior, rather than engineering, biology, or natural science applications.\n\n\nTo explain how Cybernetics evolved into Weak AI, there is a conglomeration of fields which went a bit out of fashion and favor: Game Theory, General Systems’ Theory, Agent-Based Modeling, Systems’ Dynamics, Complexity and Chaos, Evolutionary Algorithms. This stuff is fascinating and inspired many other breakthroughs, but it is extremely difficult to implement in practice.\nSo, we kind of settled on a more pragmatic set of tools, which is dominated pattern recognition and optimisation, in one form or another trying to learn from data (ML, DL, Causality) and act optimally (Dynamic Programming, Reinforcement Learning). .\nWait. What’s going on here? Am I saying that we did a bachelor’s degree in AI under the term of Economic Cybernetics? For me, personally, after having this epiphany – everything I studied makes so much more sense in retrospective.\n\n\n\n\n\n\nThe meaning of AI changed in the meanwhile\n\n\n\nYou can make sense of the terminology and general confusion of terms, by reading M. I. Jordan’s brilliant article 17, which tells the history of “AI” and how this confusion arose. He also points out how many of the claims in the field, as of today are a stretch (i.e. the revolution hasn’t happened yet) 18.17 K. Pretz - Stop Calling Everything AI, Machine-Learning Pioneer Says18 M. Jordan - Artificial Intelligence: The Revolution Hasn’t Happened Yet\nI highly encourage you to read the articles by M. Jordan, but until then, here are a few ways people understand AI:\n\nCybernetics and Weak AI, which we discussed before\nGeneral AI is a titanic project. It interweaves with Philosophy, Cognitive Science, in order to understand what makes us intelligent and conscious. On the other hand, trying to build general-purpose problem solving machines.\nSymbolic AI, is still relevant in a few niches, especially in automated proofs and logical reasoning.\nAugmentative AI, like VR, augmenting human capabilities, human-machine interactions\n\n\n\nIn practice, if you’re a data/business analyst, ML/data engineer, data scientist, statistician, product manager – Cybernetics is a way of thinking in systems and formulate problems well. When it comes to implementation, we mostly use data and the tools, models, methods discussed in this course.\n\n\n\n\nAt this point, you should have a pretty good idea why data science is important, what are some possible applications and domains, what does it do and concerned about. It is the motivation, real-world use-cases, and conceptual understanding that I promised at the beginning of the course.\nDisentangling the ambiguity around AI was one of the most difficult aspects of the course to articulate. Now, it’s time to transition to a lower level of analysis (inside data science, not outside it), break down the landscape into manageable chunks and develop our toolbox, in which we learn how to formulate problems well and match them with existing models, methods, and technologies.\nOne of the first tools I want to introduce, is distinguishing three ways of thinking, which have to work harmoniously together, in order for a data science project to be successful:\n\nAnalytics and Data Mining, where the main goal is formulating better research questions and hypotheses, that is, get inspiration, find interesting and relevant patterns and relationships in massive datasets\nMachine Learning, as a way to use training algorithms to go automatically from experience (data) to expertise (a program or recipe for prediction). Put in other words, learning from data, finding invariants, patterns which generalize beyond our sample and training data.\nCausal Inference 19 for making decisions with high stakes, where we have to understand the causal processes of the system in order to intervene optimally. It enables greater transparency, reliability, and rigor in the inferences and conclusions drawn.\n\n19 I find that statistics is not the right term here, as it is too broad, especially recently when the lines between ML and Stats are getting more blurry, as one adopts methods from other.\n\n\n\n\nSource: xkcd; Which one to choose and when? We will have some unlearning to do here, as much of the previous courses were in one way or another focused on analytics. Even in statistics or econometrics, there is little from the field of causality, which is necessary to apply it to real-world challenges. On the other hand, you applied ML methods, but in my opinion, without understanding what ML is, without a rigorous process which would ensure we don’t overfit or snoop the data, without a clear plan of how to deploy it to production and make decisions based on those predictions.\n\n\n\n\n\n\n\n\nThe art of formulating a hypothesis\n\n\n\nIn many intro to statistics or science courses, we take for granted the hypothesis, it is often our starting point. How does one come up with a business or scientific hypothesis with makes sense, is reasonable, plausible, deserves serious consideration? Since there are an infinity of possible ones, how do we pick the most relevant?\nI argue, it is an art which requires a kind of intuition, sensibility, and attunement to the problem. In my opinion, it is the most underrated aspect of scientific enquiry and process: a good problem formulation often gets us halfway towards a solution.\nIn this course we focus on data mining and analytics as a way to get inspiration for good questions to ask and hypotheses to formulate. However, in anthropology or evolutionary biology, it could be done by careful observation of the behavior, coupled with a deep understanding of the field, existing theories and their shortcomings and inconsistencies. Often, these hypotheses follow as a consequence from the theory itself.\n\n\nIf we don’t have to make decisions and want to find interesting patterns in data, to inform our future questions, we have lots of methods for exploratory data analysis – from visualization (manual) to clustering (automated) and model-driven exploration. Sometimes, we just want to monitor and display the facts and current state of a business on a dashboard – this is why your previous class was on BI (Business Intelligence).\nThen, in the decision-making processs, these questions and hypotheses can be communicated to statisticians and decision-makers, so that they have a clearer direction and more promising candidates to experiment with. This doesn’t mean that what we found the causal process which makes some clients more profitable than others, when we notice a difference between groups or clusters of clients.\nIf we do have to make a few, high-stakes, strategic decisions of major importance to business outcomes and user experience, that means we need some rigorous statistics. For example, how to price the products, whether to enter a new market, what products to develop, how to allocate advertisement spending across different platforms, whether to deploy a new recommender system. We will discuss at length what can go wrong in drawing conclusions from data alone (with analytics or ML), and how that can backfire spectacularly.\nIf we have to make lots of decisions at scale and high frequency, for example – doing demand forecasting and inventory optimization for 100k product SKUs, it cannot be done manually or with carefully designed experiments. In this case, an appropriate choice would be to learn from data and get predictions as reliable as possible. Keep in mind, that we will have to be very careful when defining what the model is optimizing for – it has to be aligned with business objectives.\nWhy ML, since we put so much emphasis onto scientific rigor and trying to infer the causal processes? Sometimes – you don’t have a theory. For example, in recommender systems it’s just too complicated, with so many heterogenous users and items, each with their specific preferences and idiosyncracies.\n\n\n\n\n\n\nWhy not use all at various stages of a project?\n\n\n\nIt is not a debate of which one is better: ML vs Stats vs Analytics. One has to cycle through these approaches, gain greater understanding, experience, and skill in order to use the appropriate tools in the right context.\nI recommend the following 4-part presentation 20 by Cassie Kozyrkov, so that you get a good idea of how AI fits into organization and decision-making process. I recommend following her and, basically reading everything she has written on medium.20 C.Kozyrkov - Making Friends with Machine Learning\n\n\nPay close attention to the process of developing data-driven products 21 and what are the prerequisites for an AI project to be successful (or doomed from the very start). It is important not to skip the relevant steps, understand the roles of people involved: from decision-makers, to statisticians, and data engineers. A good blueprint 22 for thinking about how to define and plan an AI project is given by Google’s PAIR (People and AI Research group). We will discuss all of this in detail during our next lectures and case studies.21 C.Kozyrkov - 12 Steps to Applied AI22 People and AI Research | Google - Guidebook\n\n\n\n\n\n\nAre we in the business of ML?\n\n\n\nThe next two questions are tremendously important and will prevent you from embarking on an AI project which is doomed from the start:\n\nIs there a value proposition for AI? In other words, is there an underlying pattern to be discuvered?\nDo I have the (necessary and relevant) data?\n\nIf yes and yes, we MIGHT be in business! But we shall not forget about the pragmatic aspects: is it feasible to be done with a small team, without a huge investment? What is the simplest way we can solve it? Are we solving the right problem? Are we making the job of people in the firm easier and more efficient?\n\n\nMake no mistake, the data science field is fascinating and full of exciting applications, but as you well know from statistics, there are numerous pitfalls we can fall into. I think it is useful to demistify AI and get humble, down to earth about what it can and can’t do – its power, but also the limitations:\n\nJust take a look at how many AI tools have been built to catch covid, and none helped 23\nOne part of the problem is the mismatch between the real/business problem and objectives, versus what models optimize for. Vincent Warmerdam brilliantly explains it in “The profession of solving the wrong problem”24 and “How to constrain artificial stupidity” 25.\n\n23 W.Heaven - Hundreds of AI tools have been built to catch covid. None of them helped.24 V. Warmerdam - The profession of solving the wrong problem25 V. Warmerdam - How to Constrain Artificial Stupidity\n\n\n\n\n\nSplit your damn data! (on data snooping)\n\n\n\nSince we’re engaging in the business of data mining and analytics at one point or another of the project, we have to be extra careful. Intuitively, you understand that discovering hypotheses and testing them on the same set of data is a bad idea, because we’ll get an overly confident estimation of how good it is.\nHowever, sometimes, we don’t shy away from doing an exploratory data analysis, finding relevant and predictive features for our target variable by trying out a few models. Next day, we forget about this, having the conclusions crystalized in our mind, and apply a new, final model … on the same data. Often we get away with this, but it is as bad, meaning equivalent to the first case – we contaminated the data with our mining.\nSo, before we get into the nuances of model validation, selection, and experiment design, get into the habit of always splitting your data. Give that test dataset to a friend, locked under a key and don’t let her give you that data, until you have your final model to be deployed and used for decisions.\n\n\nWhy go through all of that pain to critique our own model with such a vigor? The answer is simple – if it passes this rigorous critique, it has greater chance of finding a real/causal pattern and generalize to examples outside our sample.\nThis is a lot to take in! But there is one more thing to explore – there is one course by Dr. Adam Fleischhacker 26, which has a very similar philosophy, but is much more established and thought out, with many practical examples. Here is what he has to say in the course intro:26 Adam Fleischhacker - Introduction to Business Analytics: Intro to Bayesian Business Analytics in the R Eco-System\n\n“You will translate real-world scenarios into both mathematical and computational representations that yield actionable insight. You will then take that insight back to the real-world to persuade stakeholders to alter and improve their real-world decisions.”\n\nDr. Fleischhacker makes an illuminating distinction between the business analyst’s workflow and a machine learning workflow, and sets up the normative criteria which make it successful. In our course, his workflow falls under the discussions related to causal inference. One interesting thing to note, is the convergence in the approach of an extremely diverse set of people: Cassie Kozyrkov, Vincent Warmerdam, Adam Fleischhacker, Richard McElreath, Andrew Ng – all coming from different backgrounds and activating in different environments and domains.\n\n\n\n\n\nSource: causact.com; “(The workflow) starts with strategy, expertise, and data as inputs and results in the business analyst leading their firms to act on value-adding insights”\n\n\nLet’s briefly review those normative criteria of this workflow. It might be a confirmation bias on my part, but the fact that these are present in the current course in one way or another, means I stumbled upon them by trial-and-error and painful mistakes:\n\nOutcome-focused: What is the point of fancy models, if we don’t achieve good or desired outcomes? If I was implying it so far, for the rest of the course we’ll ask this explicitly every time we tackle a business problem.\nStrategically-aligned: “Not all outcomes are created equal. Companies seeking to capture market share might increase expenses to aid market capture. Companies seeking to be cost leaders might leave some customers unsatisifed to keep expenses low. So a one-size-fits-all approach to defining good outcomes is ill-advised.”\nAction-oriented: We insisted so much on insights influencing, driving actions and decisions that there is little to add here. The remaining question is how can we communicate and articulate it well to convince decision-makers and stakeholders.\nComputationally Rigorous: Refers to the know-how, the engineering in the trenches. Even though we’ll spend most of the time in the frequentist land – I think the future is at the intersection of Causality and Bayesian Inference.\n\nTaking it one step further, this kind of workflow should be reproducible and (mostly) automated. This is why we’ll explore an ecosystem of software engineering tools and practices in the labs.\nIdeally, given in the hands of our clients/users in form of a full stack data app. This is where we take off our consulting hat and start building software products.\n\n\nThis is in contrast with a predictive, machine learning workflow, which we called before “workhorse models”, a “hammer” for which everything is a nail. We got a taste of its power and limitations, and tried to articulate which are appropriate applications for ML. This course gives equal attention to ML and Causality, due to the prevalence of use-cases from which we can learn from data to make tons of decisions at scale and high frequency.\n\n\n\n\n\nSource: causact.com; “The machine learning analyst transforms historical data with known outcomes into future outcome predictions.”"
  },
  {
    "objectID": "01_fundamentals/learning.html",
    "href": "01_fundamentals/learning.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "In the last two sections of this lecture I felt it is necessary to raise two more warning tales, precisely because we will work with such a powerful machinery and attempt to tackle complex problems in our jobs and lives.\nA good metaphor for what we’re doing in this course is the way people learn. In a way we’re pattern recognition machines, with a powerful capacity for implicit learning, meaning we can’t articulate or explain how we did it. For example, riding a bike, catching a baseball, speaking, reading and writing, behaving in social situations and so on.\n\n\n\n\n\n\nImplicit Learning of Artificial Grammars\n\n\n\nThere is a famous experiment, in which researchers invented words from two languages, meaning two set of rules,1 let’s say between 3 and 8 characters, with appropriate vowels and so on. Total gibberish, but here we are, with two sets of words.1 For the curious, they used a Markov Chain artificial grammar, which would be pretty straightforward to implement in R, and is a way of looking at the language which brought the first breakthroughs in NLP (natural language processing)\nParticipants saw the lists and they had to say from what “language” does a word come from. It is a good example of experiment design coming from cognitive science research.\n\n\n\n\n\n\n\nSource: John Kihlstrom; They found that subjects differentiated them much better than chance and the results were statistically significant. During the interviews, when asked to explain how they did it, the responses were either “no idea”, or giving some rules, which when implemented on a computer, were not able to perform better than a coin flip.\n\n\nFollowing the experiment, this means that those rules articulated by the people in the experiment were NOT how they were thinking. There is something going on which can’t really be articulated. This means that we have a capacity to find patterns and regularities in the real world, due to evolution building into us this powerful machinery of implicit learning.\nAnother hypothesis about how animals learn is the idea that some prior 2 knowledge or mechanism – which is there due to evolution optimizing for fittedness, is necessary to kickstart the process. The discussion about the fascinating interaction between nature and nurture is outside the scope of the point I’m trying to make right now. So, here we go with two more experiments:2 We will go into more technical details when discussing Statistical Learning and ML models, of why is it the case that biasing a learning process with prior information is essential to successful learning.\n\n\n\n\n\n\nBait Shyness – Rats Learning to Avoid Poisonous Baits\n\n\n\nWhen rats stumble upon food with new smell or look, they first eat very small amounts. If they got sick, that novel food is likely to be associated with illness, and in the future the rat will avoid it. Quoting Dr. Shai-Ben David:\n\nClearly, there is a learning mechanism in play here – the animal used past experience with some food to acquire expertise in detecting the safety of this food. If past experience with the food was negatively labeled, the animal predicts that it will also have a negative effect when encountered in the future. 33 Shai-Ben David - Understanding Machine Learning, 2014\n\n\n\nThe bait shyness is an example of learning, not just rote memorization. More exactly of a basic inductive reasoning, or in a more statistical language, of generalization from a sample. But we intuitively know, that when generalizing a pattern, regularity is sometimes error prone: we pick up on noise, spurious correlations instead of signal, we’re fooled by randomness.\n\n\n\n\n\n\nB.F. Skinner: Pigeon Superstition\n\n\n\nIn an experiment, B.F. Skinner put a bunch of hungry pigeons in a cage and gave them food at random intervals via an automated mechanism. They were doing something when the food was first delivered, like turning around or pecking – which reinforced that behavior.\nTherefore, they spent more time doing that exact same thing, without regard with the chances of those actions getting them more food. That is a minimal example of superstition, a topic on which philosophers spilled so much ink.\n\n\nShai-Ben David goes on to argue – what stands at the foundations of Machine Learning, are carefully formulated principles which will prevent our automated learners, who don’t have a common sense like us, to reach these foolish and superstitious conclusions. 44 \n\n\n\n\nSource: skewsme; Here’s Skinner’s pigeon chambers\n\n\n\nWhat is the common thread among these three stories about learining? In a nutshell, it’s about cultivating the wisdom and ablility to differentiate between correlational and causal patterns.\n\nWhen all goes well we call it intelligence, intuition, a business knack. It’s our pattern recognition picking up on some real, causal regularities. It’s the common sense working as it is supposed to.\nWhen learning goes awry, it’s a bias and in worst cases – bigoted prejudice.\nI am not a fan of how behavioral economics treats biases, 5 but here are a few so prevalent and harmful outside their intended evolutionary purpose, that we have to mention them: confirmation bias, recency, selection, various discounting biases.\nWe attribute a causal explanation to a phenomena when it’s not. For example, size of the wedding to a long-lasting marriage, extroversion and attractiveness with competence, how religious are people vs years of life.\nThere are numerous examples, and I don’t have to repeat that correlation doesn’t imply causation, due to common causes, hidden variables, mediators, reverse causes and confounders.\n\n5 The reasons come from cutting-edge Cognitive Science research, which challenge the normative position of economic rationality. Moreover, they challenge the economic orthodoxy when it comes to rationality – it is a much more complex beast than a maximisation of expected utility.So, what can we do as individuals and professionals? I think one way to get wiser is to cultivate a kind of active open-mindedness, which tries to scan for those biases and bring them into our awareness, such that we can correct our beliefs, behavior and decisions. Another thing we can do is to update our beliefs often, in the face of new evidence, keeping a scout mindset, trying to see clearly, instead of getting too attached and invested in our beliefs and positions.\nI think we’re extremely lucky to be in a field like data science, where we can use formal tools from probability, causal inference, machine learning, optimization, combined with large amounts of data and domain expertise – in order to practice that kind of a mindset. However, let’s keep in mind how easy researchers are getting fooled, not only by randomness, and that we’ll never be immune to misjudgement.\n\n\n\n\n\n\nThe double edged sword of our intelligence\n\n\n\nThe same machinery which makes us intelligent general problem solvers and extraordinarily adaptable, makes us prone, vulnerable to bullshit and self-deceptive, self-destructive behavior.\nIt is the same, in a more techincal sense, with overfitting ML models and drawing wrong inferences. In business settings, I believe that firms will realize and appreciate that a decison scientist has this exact role – to help others rationally pursue their goals and strategy."
  },
  {
    "objectID": "01_fundamentals/learning.html#calling-bullshit-in-the-age-of-big-data",
    "href": "01_fundamentals/learning.html#calling-bullshit-in-the-age-of-big-data",
    "title": "Decision Science Course",
    "section": "Calling Bullshit in the age of Big Data",
    "text": "Calling Bullshit in the age of Big Data\nYou probably noticed that I used the word bullshit a few times in this lecture. It is not a pejorative, but a technical/formal term introduced by Harry Frankfurt in his essay “on Bullshit”. In romanian, the closest equivalent would be “vrăjeală”, a kind of sophistry.\n\n\n\n\n\n\nThe critical difference between Lie and Bullshit\n\n\n\nA liar functions on respect for the truth, as he inverts parts of a partially true story to convince you of a different conclusion. It is interesting that we can’t really lie to ouselfs, we kind of know it’s a lie – so we do the other, we distract our attention away from it.\nIn Bullshit, you try to convince someone of something without regard for the truth. You distract their attention, drown them in irrelevant, but supersalient stuff.\n\n\nIn our age, BS is much more sophisticated than the “good old” advertisement trying to manipulate you to buy something. I can’t recommend enough that you watch the lectures by Carl T. Bergstrom and Javin West,6 where they explain at length numerous ways we’re being convinced by bullshit arguments, but which are quantitative and have lots of jargon and pretty charts in them.6 Carl T. Bergstrom and Javin West - Calling Bullshit: The art of Skepticism in a Data-Driven World\nThe kind of intimidating jargon comes from finance people, economists, when explaining why interest rates were risen, what happened in 2007-2008 great financial crisis. My “favorite” is cryptocurrency-related sophistry and some particular CEOs expertly making things more ambiguous, mysterious, and complicated with their corporate claptrap.\nThese lectures are short, fun, informative and useful for developing the critical thinking necessary when assesing the quality of the evidence or reasoning which led to a particular conclusion. I will try to incorporate here and there some examples from their course, where it fits naturally with our use-cases and theoretical topics, especially in causality.\nThere are tempting arguments which boil down to this: due to ever increasing volumes and richness of data, together with computing power and innovations in AI – it will lead to the end of theory. I couldn’t disagree more!\n\n\n\n\n\n\nSmall Data problems in Big Data\n\n\n\nIn huge datasets of clients and web events, there are lots of observations and many features/attributes being collected, which theoretically should be excellent for a powerful ML model.\nHowever, at the level of each observation, when we go to a very granular aggregation level, the information can be extremely sparse, with high cardinality, inconsistent (all data quality issues). For example, in an e-commerce, for a customer, you might have no information about their purchases, and just a few basic facts about their website navigation.\nSo, you have the cold start problem, data missing not at random, censoring/survival challenges, selection biases. The data at the lowest level becomes discrete, noisy, heteroskedastic. You know the saying: garbage in garbage out.\nEven in ML when there is a big, clean and rich dataset, we can’t escape theory (which is our understanding of the world), in one way or anover. For example, in demand forecasting, we need to provide the model relevant data, factors which are related, plausibly influencing that demand: like weather, holidays, promotions, competition, and so on.\nWe can’t just pour all this data into a ML model and expect the best. It isn’t clear that feeding irrelevant data doesn’t break or bias our model, such that it picks up on noise and spurious correlation, especially in very powerful DL models. That definitely doesn’t help with better decisions.\n\n\nIn conclusion, there is no magic to AI, no silver bullet: more data and better models are often necessary, but not sufficient to improve outcomes. We have to ask the right questions. We have to set objectives aligned with business strategy. We have to frame and formulate a problem well, understanding it in its context. We have to collect relevant data, clean it, understand the processes of missingness. If we let AI decide who enters into a quarantine during the pandemic, what it would optimize for? It’s just a machinery combining statistics and optimization.\nTherefore, critical thinking becomes that much more important when we have these powerful quantitative tools at out fingerprints. A part of a data scientist’s job is to constrain artificial stupidity (more exactly, foolishness, because it does perfectly fine what you instructed it to do) and making sure we’re solving the right problem (sounds trivial, but ofter we solve the wrong problem, without being aware of it)."
  },
  {
    "objectID": "01_fundamentals/slides.html#better-decisions-in-a-vuca-world-but-how",
    "href": "01_fundamentals/slides.html#better-decisions-in-a-vuca-world-but-how",
    "title": "Data Science in Business Context",
    "section": "Better decisions in a VUCA world, but how?",
    "text": "Better decisions in a VUCA world, but how?\n\n\n\n\nflowchart LR\n    S[Strategy] --> M[Model] --> I[Insight] -- informs --> A[Action] -- drives --> O[Outcome]\n    E[Expertise] --> M\n    D[Good Data] --> M\n\n    subgraph Math / Business \n    M \n    I\n    end\n\n    O -- feeds back --> M\n\n\n\n\n\nInsight = Aha Moment!"
  },
  {
    "objectID": "01_fundamentals/slides.html#two-metaphors",
    "href": "01_fundamentals/slides.html#two-metaphors",
    "title": "Data Science in Business Context",
    "section": "Two metaphors:",
    "text": "Two metaphors:\n\n\n\n\n\n\n\n\nWe see Pollock’s messy reality, which is the data and observations. We want to get to Picasso’s bare bones essence, for better and clearer decision-making\n\n\n\n\n \n\n\n\n\n\nThis is a big picture course, which re-contextualizes everything you have learned before, but didn’t see how it all fits together or can it be implemented in practice to bring value to organisations, that is: be useful\n\n\n\n\n\n\n\nCourse Home"
  },
  {
    "objectID": "01_fundamentals/roadmap.html",
    "href": "01_fundamentals/roadmap.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "The first module is putting the rest of the course on a solid foundation, emphasizing again and again the key idea of Decision-Making Under Uncertainty at Scale. Understand the problem, the methodology, and the tools at your disposal.\nThe most difficult part of the module is the statistical fundamentals, as it is very easy to misunderstand and apply these methods mechanically. Therefore, fundamental – doesn’t mean easy, nor basic, nor trivial. Most courses make you solve puzzles, but I ask you to appropriately define, justify, and apply the choice of method and tool in the context of business applications.\n\n\n\n\n\n\nDo you have a process for Statistics and ML?\n\n\n\nAll too often we jump into data analysis and modeling without formulating well the objectives and the problem. Focusing on how to implement and use a model without following a rigorous process can be error-prone and counterproductive. You have to understand these 3 processes and think how would you structure your project in practice for a particular application.\n\n\nThere are many ways to organize this module in lectures. You can find a possible schedule in the table, with associated readings/resources and a mind-map of topics in the diagram below it. Some lectures are conceptual, some mathematical and in some we code and practice.\nThe courses I teach are usually short and intensive, so the following schedule would take two weeks to cover, meaning the next two weeks I can focus on Bayesian statistics, causal inference, or machine learning – with more emphasis on modeling or engineering, depending on the audience.\n\n\n\n\n\nLecture Agenda\nKeywords\nCase Studies\n\n\n\n\n1\nData Science in Business Context. Decision-Making Under Uncertainty\nAI, Cybernetics, Uncertainty, Scale, Analytics vs Stats vs ML\nsurvey of applications in various domains\n\n\n2\nInterdisciplinarity. Overview of prerequisites.\nCalculus, Linear Algebra, Probability, Stats, Economics. Math Iceberg\nProbability vs Statistics, Why did you study all of that?\n\n\n3\nBusiness Analyst’s Workflow. Newsvendor Problem\naction, outcome, strategy, expertise, data, models, simulation, DAGs\nTIME INC. printing quantity decisions\n\n\n4\nStrategy and the Desired Trajectory\nSWOT, Systems Dynamics, KPIs, Stock vs Flow\nLRB Journal Subscriptions\n\n\n5\nProjects Overview. Full-stack data apps. Overview of Data.\nNLP, RecSys, Time Series, Classification\nMercari, Favoritas, H&M, Olist (e-commerce)\n\n\n6\nProbability Triple. Random Variables. Mathematical Statistics\npopulation, sample, r.v., estimators, mixtures, markov chain\nRetail Fashion - appropriate choice of distributions\n\n\n7\nSet Up the Environment. Reproducible Research I\nterminal, conda, python, vscode, jupyter, git & github, poetry, pytest\nDevXP, productivity, reproducibility\n\n\n8\nBernoulli, Binomial, Poisson, Simulation, DAGs as stories\nnumpy, pandas, pymc, matplotlib, arviz, graphviz\ncouples showing up to safari\n\n\n9\nBayes Rule\nDAGs, Conditioning, Marginalization, Priors\nFootball Spreads, Medical testing\n\n\n10\nBeta-Binomial Model\npymc, praw, priors, conjugate families\nA/B testing\n\n\n11\nNewsvendor Problem. Probability distributions. Stochastic Processes\nmodel specification, inference, simulation, optimization, constraints\ngroceries and perisable goods, price optimization\n\n\n12\nThe most dangerous equation\nestimators properties, CLT, LLN, sample size\nU.S. small schools, reddit upvote share, hackernoon algo\n\n\n13\nEstimator properties. Bias-Variance\n\\(\\chi^2_k, N, F_{m, n}, t_k\\)\nWhat does a statistician want?\n\n\n14\n12 Steps of ML. What is ML?\nPAC Learning, generalization, CRISP-DM\nProject requirements simplification\n\n\n15\n12 Steps of Statistics. A/B Testing Scheme\nDefault Action, Stat. Hypotheses, confidence intervals, relevance\nproblem of p-value, statistics workflow\n\n\n16\nReproducible Research II\nautomation, code, data, environment, documents, publishing\nReplication crisis, Excel and the Dead Salmon\n\n\n\n\nWhy these exact topics, since other courses in 16 lectures teach you either statistical learning, probability, statistics, causality, deep learning? Of course, and they’re excellent at teaching exactly that, so I refer to those amazing resources, books, courses when it comes to the tool/model and didactic examples.\nOur goal in the first module is to have a solid general foundation for all of those topics and domains. In order to achieve that, we need to understand the business environment and challenges it faces, mathematically and conceptually get the main ideas from probability and statistics, and be prepared with our computational skills and tooling to tackle those challenges. Here are all of these concepts illustrated in the following mind-map:\n\n\n\n\n\n\nflowchart LR\n  Prob(Probability) --> Boot(Inference) -- LLN --> Es(Estimators)  --> PS[[Stat process]]\n  Prob -->  Cond(DAGs) -- Cond-g -->  Bayes(Bayes Rule)\n\n  DSc(Decisions, AI) -- statistics  --> Prob(Probability)\n  DSc -- business  --> VUCA(V.U.C.A.) --> App(Applications) --> R(Role in firms)\n\n\n  R --> BAW[[Analyst's Workflow]]\n\n  DSc -- method. --> A(Analytics) --- Caus(Causal Inference) --- ML(Learning from Data)\n  ML --> PML[[ML Process]]\n\n  App --> NV(Newsvendor) --> Proj(Projects Overview)\n\n\n\n\n\n\nFigure 1: A mindmap to help you navigate the first module, with 3 main branhes and programming sprinkled everywhere: business, statistics, and methodology."
  },
  {
    "objectID": "01_fundamentals/prerequisites.html",
    "href": "01_fundamentals/prerequisites.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "Why did we have to go through all those excruciating months doing mathematical analysis, linear algebra, probability, statistics, econometrics, operations research, and lots of economics?\nIt was very frustrating for me, because it wasn’t clear how they fit together, what is the common thread, and more importantly, what part of the theory and particular concepts would be helpful in solving the kind of problems we discussed, and which ones are designed to enhance our academic understanding.\n\n\n\n\n\n\n\n\n\nSounds good – doesn’t work?\n\n\n\nAn important question is what works well in practice and why. On the other hand, what is intellectually fascinating, but not at all straightforward to apply. What is a minimal or most powerful set of the prerequisites that you need?\nLet’s draw a map, stop at each field and in a sentence explain why we learned it and how it contributes to AI, Data Science, and ML. We mentioned form the very beginning that it is an interdisciplinary field, but it is not just an union of those subjects – the inspirations and tools are quite carefully picked.\n\n\n\n\n\n\n\nflowchart TD\n  LA[Linear Algebra] --> OR[Operations Research]\n  MA[Mathematical Analysis] --> OR\n  MA --> SD[Systems Dynamics]\n  %% CS[CS Algorithms] --> OR\n  \n  PT[Probability] --> MS[Statistics] --> EC[Econometrics]\n  EC --> Caus[/Causal Inference\\]\n  EC --> TS[Time Series]\n\n %% subgraph 1\n  Caus --- DM[/Data Mining\\] --- ML[/Machine Learning\\] \n  ML --- Caus\n %% end\n\n  OR --> ML\n  MS --> ML\n  MA --> PT\n  SD --> Caus\n\n  Caus --- Econ[[Economics]]  \n  Econ --- GT[Game Theory]\n  Econ --- DT[Decision Theory]\n\n  style Caus fill:#f7f5bc\n  style ML fill:#f7f5bc\n  style DM fill:#f7f5bc\n\n  DM --- FSDA[/Full-Stack Apps\\]\n  FSDA --- DB[Databases/SQL]\n  FSDA --- OOP[OOP]\n  Econ --- TS\n\n  style FSDA fill:#f7f5bc\n\n\n\n\n\nFigure 1: Think of this as a stuctural organization of the fields and courses you studied before. Some are more useful in analytics, some in ML and some in making causal inferences, that is, based on data + theory.\n\n\n\n\n\nLinear Algebra is a language of data. The vast majority of models and training algorithms can be reduced to operations on matrices. Therefore, it is not a coincidence that is almost the only tool we have, in order to take these models and implement them in code, on a computer.\n\nMy perspective over linear algebra is ultimately computational and geometric, in the sense of the “space” the data points live in and the transformations of that space.\nUltimately, no matter the data type: image, video, text, voice, structured, panel – all can be represented as multidimensional arrays (or tensors, if you wish).\n\nMathematical Analysis is all about change, formalizing how a function behaves with respect to its arguments and parameters. It is an essential building block in optimization and deep learning (automated differentiation).\n\nI would argue that in order to understand any complex system, be it a firm, an economy, the climate or environnment, we have to model how it evolves in time.\nThis suggests the importance of differential equations and systems dynamics, modeling the feedback loops. All of this would be impossible to reason about without the mathematical analysis.\n\nProbability and Statistics is the language of uncertainty, the only instrument we have, that allows us to say something intelligent about how confident are we.\n\nWe will explore the role of statistics at lenght, but as a quote, think of it as a tool which enables us to change our mind and decisions in the face of evidence.\n\nEconometrics, in my personal opinion, tries to separate the signal for the noise and make inferences about the causal processes in economic decisions and phenomena. It specializes statistics in the domain of economics, by infusing economic theory – because you can’t derive a scientific theory from data alone.\n\nTime Series Analysis, senso largo, bridges the gap between Systems Dynamics (which takes a more theoretical perspective) with statistics and probability (stochastic processes). It adapts those tools to make inferences and predictions about phenomena which evolves in time, that are dynamic in their nature.\n\nI like the metaphor of Data Assimilation, which is actually an entire field trying to introduce the empirical dimension to differential equations.\n\nOperations Research is about optimization with constraints and efficiency. However, the problem is that often, we start from an Integer/Linear Programming problem formulation, and that is easy part – to apply an existing algorithm. The hard part is to reduce a messy real world problem at a large scale to that formulation, especially under uncertainty and nonlinearity.\nEconomics touches upon a wide range of aspects of our society and human behavior. In mathematically-oriented courses, you can think of it as optimization with constraints, the constraints being given by our positive or normative theory of economic decision-making.\n\nIn this course and in practice, we care more about business economics. It’s a very different beast from theoretical ideas in macroeconomics (ISLM, DSGE type of models) or microeconomic utilitarianism.\nBy business economics, we mean marketing, management, corporate finance, decision theory, supply chain, and logistics.\nWhat you have to know about marketing, especially if you are skeptical like me, is that it became innovative, mathematical, rigorous and data-driven. Look at any marketing journal: how much econometrics and ML models are in there.\nSo, if you hold the opinion that marketing and management are fields full of fluff – I advise you to rethink your positions. In the context of tech firms, you can’t bullshit your way through it.\nMoreover, when you combine marketing with behavioral economics and psychology, it introduces another layer of nuance and understanding over our decision-making.\nWhen we make decisions, we like to think of ourselfs as objective, but we have lots of biases and blind spots which prevent us to see the reality clearly. We often find patterns and regularities which are just noise, not causal. So, the usefulness of this kind of domain knowledge from economics about human behavior helps us to be wiser, that is, to prevent self-deception and self-sabotage towards achieving the goals and objectives.\n\n\n\n\n\n\n\n\nA word of encouragement\n\n\n\nNone of those courses were useless. Think of how can we take parts from each of those prerequisites, which are relevant in ML/DSc, so that we have more tools to solve problems of the complexity we encounter. To reiterate, data science is an umbrella term, borrowing from them all."
  },
  {
    "objectID": "01_fundamentals/prerequisites.html#give-statistics-another-chance",
    "href": "01_fundamentals/prerequisites.html#give-statistics-another-chance",
    "title": "Decision Science Course",
    "section": "Give statistics another chance",
    "text": "Give statistics another chance\n\nRealize that lots of common statistical tests are particular versions of linear models 1. It takes a few hours to go through the theory and the code in the referenced book.\nHere’s the approach taken by 2, which integrates data analysis and simulation in learning of statistics.\nUpgrade your statistical thinking for the 21st century challenges and be aware of the pitfalls and problems in the field. A good reference is 3, which takes multiple perspectives (frequentist, bayesian, causal inference), while going through the workhorse models and methods.\nBe comfortable with exploring, wrangling, visualizing and analyzing data in R or/and Python, get familiar with reproducible research best practices. A good starting point is 4, which is a course in collaboration with the RStudio team.\n\n1 Doogue - Common statistical tests are linear models2 Speegle, Clair - Probability, Statistics, and Data: A fresh approach using R3 Poldrack - Statistical Thinking for the 21st Century has two companion books, one with R and another in Python4 Bryan - STAT 545 It is also notable for its focus on teaching using modern R packages, Git and GitHub, its extensive sharing of teaching materials openly online, and its strong emphasis on practical data cleaning, exploration, and visualization skills, rather than algorithms and theory.There are a gazillion books, courses on statistics, which basically do/teach the same thing. For reference and your curiosity, I curated a few which stand out with the right balance of data, code, simulation, theoretical rigor and real-world applications:\n\nCetinkaya-Runde, Hardin - Introduction to Modern Statistics goes through all the fundamentals in a clear, concrete, extensive way, with code!\nCrump - Answering questions with data – introductory statistics for Psychology Students has an interesting approach, focused on the challenges of Psychology\nThulin - Modern Statistics with R goes through the whole process, with R, with additional topics, normally not present in a statistics course\nHolmes, Huber - Modern Statistics for Modern Biology is for biologists, but we can see how central to the field are multidimensional methods, clustering and high-performance computing, working with big and messy data"
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html",
    "href": "01_fundamentals/stat_foundations.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "In order to be a successful Data Scientist, one has to speak the language of probability and statistics. It is the foundation on which we can build towards more realistic and avanced models, with the purpose of improving decision-making under uncertainty. This foundation is a prerequisite for all three perspectives: analytics/mining, machine learning and causal inference.\n\nIn order to build adequate models of economic and other complex phenomena, we have to take into account their inherent stochastic nature.\n\nData is just the appearance, an external manifestation of some latent processes (seen as random mechanisms). Even though we won’t know the exact outcome for sure, we can model general regularities and relationships as a result of the large scale of phenomena. 11 Think of measurements taken from the Large Hadron Collider experiments: the volume, noise, and complexity of data. Ultimately, we want to make inferences about the underlying, physical processes which govern the behavior and interaction of particles. How do we know that inference is accurate and true? A necessary, but not sufficient condition is that the conditions of that experiment are repeatable – such that “on average”, a pattern emerges.\n\n\n\n\n\n\nA word of encouragement\n\n\n\nReviewing the fundamentals of statistics doesn’t have to be boring! We can put “the classics” in context of modern statistics, big data challenges, and use simulation instead of heavy mathematics and proof-based approaches.\nMoreover, theoretical ideas underlying statistical practice, which we often take for granted, deserve an explicit articulation. This will improve our awareness, understanding, and grasp of the field – such that we can become more effective practitioners.\n\n\n\n\n\n\n\nflowchart TD\n    Motiv[Why again?] --> ProbT[Probability Triple] --> SI[Uncertainty]\n    SI --> MS[Mathematical Statistics] --> GP[Prague Golemn]\n\n    Motiv --> Estim[Estimators] --> Prop[Desired Properties] --> US[Example: US Schools] --> Cond[Conditioning] --> Marg[Marginalisation] --> Ex[Exchangeability]\n\n    Motiv --> DM[Data Mining Process] --> G[Prediction & Geocentrism] --> Sci[Scientific Process]\n\n    Ex --> Dead[Dead Salmon]\n    GP --> Dead\n    Sci --> Dead\n\n    Dead --> Mod[Modern Stats]\n\n\n\n\n\nFigure 1: In this chapter and set of lectures, I attempt to reformulate fundamental statistical ideas, concepts, and tools, in order to show their relevance in the day-to-day practice and decision-making. A secondary objective is to fill in the conceptual gaps left by the fact that it’s hard to make sense of it all the first time we’re encountering it in the classroom.\n\n\n\n\nThe plan for this lecture is the following: we start by constructing the probability triple, formalize sources of uncertainty, and investigate the building blocks of a statistical model. I then highlight in a few case-studies what can go wrong in the process of statistical inference and how difficult it is to choose the right model.\n\n\nHow far you go back into the abstract foundations? For practical intents and purposes, you won’t need measure theory and proof-based mathematical analysis.\nI find those interesting for their own sake and understanding the foundations of higher-level tools we use. However, I can’t argue it’s an efficient use of your time.\nThe most technical, heavy, and mathematical part is about estimators and their properties, but it is necessary both for hypothesis testing and machine learning. To make it more accessible and intuitive, we will use simulation and visualization during the labs to get an understanding and intuition on how estimators behave. We wrap up the lecture by looking at the statistical process in firms and put it in contrast with the process for ML, predictive systems. Last, but not least, there is one more cautionary tale about multiple testing – which will be our gateway into truly modern statistics.\n\n\n\n\n\n\nComplementary resources if you feel like starting over\n\n\n\n\nRealize that lots of common statistical tests are particular versions of linear models 2. It takes a few hours to go through the theory and the code in the referenced book.\nHere’s the approach taken by 3, which integrates data analysis and simulation in learning of statistics.\nUpgrade your statistical thinking for the 21st century challenges and be aware of the pitfalls and problems in the field. A good reference is 4, which takes multiple perspectives (frequentist, bayesian, causal inference), while going through the workhorse models and methods.\nBe comfortable with exploring, wrangling, visualizing and analyzing data in R or/and Python, get familiar with reproducible research best practices. A good starting point is 5, which is a course in collaboration with the RStudio team.\n\n2 Doogue - Common statistical tests are linear models3 Speegle, Clair - Probability, Statistics, and Data: A fresh approach using R4 Poldrack - Statistical Thinking for the 21st Century has two companion books, one with R and another in Python5 Bryan - STAT 545 It is also notable for its focus on teaching using modern R packages, Git and GitHub, its extensive sharing of teaching materials openly online, and its strong emphasis on practical data cleaning, exploration, and visualization skills, rather than algorithms and theory.There are a gazillion books, courses on statistics, which basically do/teach the same thing. For reference and your curiosity, I curated a few which stand out with the right balance of data, code, simulation, theoretical rigor and real-world applications:\n\nCetinkaya-Runde, Hardin - Introduction to Modern Statistics goes through all the fundamentals in a clear, concrete, extensive way, with code!\nCrump - Answering questions with data – introductory statistics for Psychology Students has an interesting approach, focused on the challenges of Psychology\nThulin - Modern Statistics with R goes through the whole process, with R, with additional topics, normally not present in a statistics course\nHolmes, Huber - Modern Statistics for Modern Biology is for biologists, but we can see how central to the field are multidimensional methods, clustering and high-performance computing, working with big and messy data\n\n\n\n\n\nAs a warm-up, try working out through the following problem, sometimes called “The Birthday Paradox”.6 If outcome space, complementarity, sampling with replacement, pigenhole principle or the simulation code you’re seeing seem puzzling, you might benefit from a refresher on probability theory. If you haven’t seen this problem before, you will probably find the results surprising.6 First, an analytical solution on paper, then simulate it in R or Python to check your answer.\n\n\n\n\n\n\nThe Birthday Problem\n\n\n\nWhat is the probability at least two people have the same birthday, in a room of \\(n=25\\) people? Assuming independence, for example no twins, every day is equally likely and ignoring the problem of 29th February. Can you generalize it to an arbitrary \\(n\\)?\nFor most practical problems, simulation is an extremely important skill. We might start with it to quickly get an answer, however, for simpler problems, the process of working out the solution analytically sheds light on its structure and easily generalizes to other applications. This simulation indeed, converges to the true results, but it takes a lot of computation, so it might not work when the code needs to run really fast!\n\nnr_people <- 25 \nsimulate_birthdays <- function(nr_people, nr_sim = 10000) {\n    birthday_events <- replicate(n = nr_sim, {\n        birthdays <- sample(x = 1:365, size = nr_people, replace = TRUE)\n        anyDuplicated(birthdays) > 0\n    })\n    mean(birthday_events)  # this returns implicitly!\n}\npr_same_bday <- simulate_birthdays(nr_people)\nbday_match_size <- sapply(2:90, simulate_birthdays, nr_sim = 10000)\n\n\n\nSo, the probability that a room of 25 people has two people with the same birth date is \\(\\approx\\) 0.577. For 50 people, it’s around 0.9683\n\n\nShow the visualization code\npar(mar = c(3, 3, 2, 1), # Dist' from plot to side of page\n    mgp = c(2, 0.4, 0), # Dist' plot to label\n    las = 1, # Rotate y-axis text\n    tck = -.01, # Reduce tick length\n    xaxs = \"i\", yaxs = \"i\") # Remove plot padding\n\nplot(\n    x = 2:90, y = bday_match_size, \n    type = \"l\", lty = 1, lwd = 5,\n    xlab = \"\", ylab = \"\", # Labels\n    axes = FALSE, # Don't plot the axes\n    frame.plot = FALSE, # Remove the frame\n    xlim = c(0, 80), ylim = c(0, 1), # Limits\n    panel.first = abline(h = seq(0, 1, 0.25), col = \"grey80\")\n)\nsegments(49, bday_match_size[50], 49, 0)\n\nat <- pretty(2:90)\nmtext(side = 1, text = at, at = at,\n      col = \"grey20\", line = 1, cex = 0.9)\nat = pretty(bday_match_size)\nmtext(side = 2, text = at, at = at, col = \"grey20\", line = 1, cex = 0.9)\n\n\n\n\n\n\n\n\n\nProbability of matching birthdays as a function of the number of people\n\n\nHow this counter-intuitive statistical “paradox” relates to satellite collisions, DNA evidence and other coincidences\nThe source of confusion within the Birthday Paradox is that the probability grows relative to the number of possible pairings of people, not just the group’s size. The number of pairings grows with respect to the square of the number of participants, such that a group of 23 people contains 253 (23 x 22 / 2) unique pairs of people.\nIn each of these pairings, there is a 364/365 chance of having different birthdays, but this needs to happen for every pair for there to be no matching birthdays across the entire group. Therefore the probability of two people having the same birthday in a group of 23 is:\nGo to richard McElreath’s garden of forking paths next\n\n\n\nIn the previous lecture, I mentioned why did we study probability theory. However, there is one more useful metaphor: remember how important is logic in mathematics, computer science, and philosophy; it’s one of the prerequisites in each of those, an essential tool for reasoning. Then, probability theory is the logic of uncertainty, a formal language. 77 If you studied fuzzy set theory, you might have a case for it being the candidate, however, it fell out of favor in practice – so I would suggest to focus on probability and Bayesian reasoning.\nOften, probability and mathematical statistics are bundled together, as they make perfect sense in sequence, but they have different objectives in mind. Probability theory is concerned with the construction of probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\).\nThat is the foundation for developing useful tools like random variables and probability/cumulative density functions. Then, extending those to joint, conditional probabilities and multidimensional cases, introducing the machinery to operate all of that: like expectation, variance, moment-generating functions, conditioning, marginalization, Bayes theorem. This leads to the laws of large numbers, properties of distributions and their transformations.\nWe use all of those results to model relationships between observed and latent variables in business processes and phenomena. It’s a building block for answering the “simple” question: does X cause Y?\n\n\n\n\n\n\n“I haven’t used Poisson outside that probability class”\n\n\n\nIf you empathise with this statement, you’re probably aware that it’s important, but wonder if it didn’t come up in practice – what about the rest of the machinery I described? My answer is that it’s so, so ubiquitous, but we need to learn to “see” the opportunities to use this set of tools in decision-making.\nPoisson distribution and process can be a good choice to model counts of events per unit of time, space, with a large number of “trials”, each with a small probability of success.\n\\[\nP(X=k) = \\frac{e^{−\\lambda} \\lambda^k}{k!}; \\space k=0, 1, ...\n\\]\n\nArrivals per hour: requests in a call center, arrivals at a restaurant, website visits. We can use it for capacity planning.\nBankrupcies filed per month, mechanical piece failures per week, engine shutdowns, work-related accidents. We can use these insights to assess risk and improve safety.\nForecasting slow-moving items in a retail store, e.g. for clothing. We’ll investigate the newsvendor problem in excruciating detail, where we have to purchase the inventories ahead of time.\nA famous example is of L. Bortkiewicz: in Prussian army there were 0.70 deaths per one corps per one year caused by horse kicks. (“Law of small numbers”).\n\nJust before you get all excited about these applications, keep in mind that every distribution has a story behind it, and a set of assumptions that have to be met.\n\n\nOther tools are as prevalent and useful: Bayes rule, DAGs (directed acyclic graphs of random variables), the exponential family, laws of large numbers and the cental limit theorem. It holds both in applications and statistics itself.\nIn probability theory, we’re still in a mathematical world trying to capture the essence of the real world, but ultimately, we need statistical inference to estimate those parameters from data. Before we get into it in more detail: mathematically, computationally and in terms of business cases, we have to define that foundation – the probability triple.\nTo make the following more clear, let’s start with an experiment where we show a customer ten clothing items (a collection), and they have to pick the one they like best. If we repeated it with many customers, preferably representative for the population of interest,8 a pattern would emerge. This is an example of a discrete Outcome Set (or universal set, of all possible results). Alternatively, think of pebbles in an urn, where each one represents an outcome.8 You might intuitively know what a population is, but there are surprisingly many pitfalls, so we’ll investigate that notion with lots of care.\nAs an academic aside, we have to thank Kolmogorov for putting probability into a rigorous, axiomatic framework based on set theory and making it open for mathematical enquiry with tools from mathematical analysis, which by that time were well-established. That is important, because we can’t work the same naive way with continuous measurements and phenomena, as we do with pebbles.\n\nA random experiment (\\(\\mathscr{E}\\)) is a set of conditions which are favorable for an event in a given form. It is that real-world process of interest which we try to simplify, with the following properties:\n\nPossible results and outcomes are known apriori and exhaustably. For example: a coin/dice toss, quantity sold, time in a subscription, a default on the credit, a choice between subscriptions.\nIt’s never known which of the results of \\(\\mathscr{E}\\) will manifest or appear before we run the experiment, the “experiment” amounting to randomly picking that clothing item or a pebble from the urn.\nDespite that, there is a perceptible regularity, which can be eventually measured and quantified, that is, encoding the idea of a probabilistic “law” in the results. That regularity could be a result of the large scale of the phenomena, for example, many customers seeing a product on the shelf.\nRepeatability of the conditions in which the experiment runs, like the comparability and perservation of context. This is optional in the Bayesian perspective, where we’re not thinking in long-run frequency terms. 9\n\nElementary event as an auxiliaty construction: one of the possible results of \\(\\mathscr{E}\\), usually denoted by \\(\\omega_i \\in \\Omega\\).\nUniversal set \\(\\Omega = \\{ \\omega_1, \\omega_2, \\dots \\}\\) is also called (Outcome/ State/ Selection space). It suggests the idea of complementarity and stochasticity: we don’t know which \\(\\omega_i\\) will manifest, thus is a key object for a further formalization of probability measures.\nWe care not only about an event \\(A = \\bigcup\\limits_{i = 1}^n \\omega_i\\) (which is an union of elementary events) and its realization, but also about other events in the Universal Set, because they might contribute with additional information (about the probability) of our event of interest – remember conditioning? This means that we we’re interested in “all” other events.\nThe event space \\(\\mathcal{F}\\) is a sigma-algebra, should be defined on sets of subsets of \\(\\Omega\\) and this is where measure theory shines. For technical reasons, we usually can’t define a probability measure on all sets of subsets. On an intuitive note, we define the probability measure on sigma-algebras because if those conditions did not hold, the measure wouldn’t make sense, unions of events would step out of the bounds of event space.\nProbability as an extension of the measure: chance of events realizing. Note that the perceptible regularity can be thought as the ability to assign a probability (number between 0 and 1) to elementary events: \\(\\mathbb{P}(\\omega_i)\\). This is why additivity properties are key, as we care about random events, not only elementary events.\n\n9 \n\n\nSource: Ross This is the view of probability as a long-run frequency of events occuring, for example, flipping a coin\n\n\n\nDef: Algebra and Sigma-Algebra\nA set of subsets \\(\\mathcal{F} \\subset 2^\\Omega\\) is an algebra (field) if the following holds:\n\n\\(\\Omega \\in \\mathcal{F}\\) and \\(\\varnothing \\in \\mathcal{F}\\)\nIf \\(A \\in \\mathcal{F}\\) then \\(A^C \\in \\mathcal{F}\\) (closed under complements)\nIf \\(A, B \\in \\mathcal{F}\\) then \\(A \\cup B \\in \\mathcal{F}\\) (closed under union). Note that 2 and 3 imply that it’s closed under countable intersection\nThe additional condition for sigma-algebra: sigma refers to countability. If \\(\\{ A_i \\}_{i \\ge 1} \\in \\mathcal{F}\\) then \\(\\bigcup\\limits_{i \\ge 1} A_i \\in \\mathcal{F}\\) (closed under countable union)\n\n\n\n\nSource: Wikimedia A beautiful visual representation\n\n\n\n\n\n\n\n\n\nProbability Measure\n\n\n\nSuppose we have defined a measurable space \\((\\Omega, \\mathcal{F})\\), where \\(\\mathcal{F}\\) is a sigma-algebra. A probability measure is the function \\(\\mathbb{P}:\\mathcal{F} \\rightarrow [0, 1]\\) such that:\n\n\\(\\mathbb{P}(\\Omega) = 1\\) \nFor countable sequences of mutually disjoint effects, i.e. \\(\\forall \\{ A_i \\}_{i \\ge 1}\\) where \\(A_i \\bigcap\\limits_{i \\ne j} A_j = \\varnothing\\), the following holds \\(\\mathbb{P}(\\bigcup\\limits_{i \\ge 1} A_i) = \\sum\\limits_{i \\ge 1} \\mathbb{P}(A_i)\\)\n\n\n\nA probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is the fundamental object the whole probability theory is constructed upon. Again, Kolmogorov took the informal, gambling-type probability and put it onto axiomatic foundations – which enabled future breakthroughs. Notice that this definition of probability is not the naive one, of number of successes over the total possible numbers an event could arise.\n\n\n\n\n\n\nMeasure theory rabbit hole: Why not all subsets?\n\n\n\nThe reasons for this are very technical, and the concept of a sigma-algebra is essential in resolving the resulting paradoxes. If you’re interested in these technical details, you can check out my relatively accessible introudction to measure theory and the Caratheodori extension theorem.\n\nEven though you can go a long way as a practitioner with standard tools in probability theory, deeply understanding its measure-theoretic foundations could open up a whole new world to the researcher. It’s easy to take the results from statistics and probability for granted, but it’s useful to be aware what hides beneath the surface.\nEvans Lawrence gives the following example of a function which is neither discrete nor continuous, for which you flip a coin and if it comes heads, draw from an uniform distribution and in case of tails a unit mass at one. If \\(\\chi_{[0,1]}(x) = (e^{ix} - 1)/ix\\) is the characteristic function of the interval from zero to one, in a way you can formulate its density, but usually it’s not the case, nor is it very helpful to think about it in such terms.\n\\[\\begin{equation}\n    p(x) = w_1 \\chi_{[0,1]}(x) +  w_2\\delta_1(x)\n\\end{equation}\\]\nEven though you can visualize this in two dimensions as the uniform and a spike, or as a CDF with a discontinuity, this approach just breaks down in higher dimensions or more complicated combinations of functions.\n\n\n\nJeffrey Rosenthal begins his book by a similar motivation, constructing the following random variable as a coin toss between a discrete \\(X \\sim Pois(\\lambda)\\) and continuous \\(Y \\sim \\mathcal{N}(0,1)\\) r.v.\n\\[\\begin{equation}\n    Z = \\begin{cases}\n    X, p = 0.5 \\\\\n    Y, p = 0.5\n    \\end{cases}\n\\end{equation}\\]\nHe then challenges the readers to come up with the expected value \\(\\mathbb{E}[Z^2]\\) and asks on what is it defined? It is indeed a hard question.\nWe stumble upon different interpretations of probability (frequentists vs bayesians), when trying to clarify that “perceptible regularity”, despite the mathematics of probability theory being exactly the same. These are not “theorems” to prove, but rather axioms and philosophies often taken in practice as the starting point.\nThe frequency theory defines the probability of an outcome as the limiting frequency with which that outcome appears in a long series of similar events. If our experiment or investigation is such that those relative frequencies converge, then we can prove the LLN and the CLT. Basically, that we should view probability and predictions based on historical data. 1010 “To a frequentist, the probability of an event is intrinsic to that event’s nature and not something perceived, only discovered.”\nFormally, we can represent this statement by Bernoulli’s theorem, which is a special case of Law of Large Numbers, where \\(m_n\\) is the number of times an event \\(A\\) occurs in \\(n\\) trials.\n\\[\n\\lim_{n \\to \\infty} \\frac{m_n}{n} = p\n\\]\n\n\n\n\n\n\nFrequency Convergence as Statistical Stability\n\n\n\nSince we don’t have an infinite number of trials, the best we can do is to say that “experimental evidence strongly suggests statistical stability”. This applies really well in gambling (by design) and physics, but is it plausible in human behavior? I don’t think so, but it might be unreasonably effective, even if not true.\n\n\n\n\nSo, which one do you recommend? It depends, learn both! But I would agree with Richard McElreath’s meme: it’s the scientific process that has the last laugh\n\nBayesians view probability as the degree of confidence in a belief, an apriori guess or knowledge, that is, before seeing the data. Then, using inverse probability, you update your beliefs in the face of evidence and data. Often, you do have knowledge and can define your prior probabilities by a process of ellicitation – it should’t be too vague, neither dogmatic. Other Bayesians put in great effort to pick non-informative priors, as the data should quickly overwhelm it.\nTODO: Ellicitation of probability -> we’ll do more later with inference\n\n\nThank you for bearing with me through the theory you have probably seen before, but we’re not done. We’re still in the land of set theory, and it is very hard to operate that way in practice – so, we need a new concept which will allow us to use the tools of mathematical analysis in probability, in order to make it feasible for practical uses.\n\n\n\n\n\n\nThe breakthrough idea of a Random Variable\n\n\n\nWe started from some phenomena of interest and a random experiment. The random variable is a necessary abstraction in order to mathematically define quantifiable characteristics of the objects. Meaning, we start working with numbers instead of some qualitative properties. Now, we’re in business!\n\n\n\n\n\n\n\n\nRandom Variable is not a variable, nor random\n\n\n\nA random variable is quantificator of elementary events, a function defined on the outcome space which maps the elementary events to the real number line. That mapping can’t be done in any way we wish, it has to perserve the informational structure of the sample space. That is one of the technical reasons for sigma-algebras we mentioned before and is related to the idea of measurability, meaning we can assign a meaningful “volume”.\n\\[\\begin{align}\nX(\\omega):\\Omega \\rightarrow \\mathbb{R} \\\\\ns.t. ~~ \\{\\omega \\in \\Omega | X(\\omega) \\leq r, \\forall r \\in \\mathbb{R} \\} \\in \\mathcal{F}\n\\end{align}\\]\n\n\nLet’s figure out what the hell do we mean by that fine print condition, using the diagram below. The idea of conservation of the informational structure is actually equivalent to the one of measurablility. If this property doesn’t hold, it’s not possible to explicitly and uniquely refer to the sets (events) of interest.\nThe idea is that the preimage defined above \\(X^{-1}((-\\infty,r]) = E \\in \\mathcal{F}\\) on the following interval corresponds to an event E which should be in the event space \\(\\mathcal{F}\\). Because the only thing that varies is the limit of the interval r, the randomness comes from it. Also, it automatically suggests the idea of the Cumulative Distribution Function, which is \\(F_X(X \\le r)\\).\n\n\nCDF, one of the most important constructs in probability is a direct consequence of the definition of the random variable:\n\\[\nP(A ≤ r) = F_X(X ≤ r)\n\\]\nIn the practice of modeling, we often work with probability density functions, because it is more convenient in many cases. Then, in order to translate to probabilities, we would think in terms of areas under that curve. For sure, you remember the following duality between CFD and PDF:\n\\[\nF'(x) = p(x)\n\\]\n\n\n\n\n\nIdea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable. It is the construct that enables us to define the statistical population (some relevant aspect of it to us)!\n\n\n\nAs a motivation of why do we have to understand all of this, when for most practical applications we can get by just fine with using the results and tools from probability, I will introduce two examples: one of compositional data analysis11 and time series analysis. What I want to say, is that for more “exotic” applications, we might need to tweak that probability triple because of the nature of the problem, which has downstream consequences for all the statistical machinery we use in those applications.11 \n\n\n\n\nSource: Dumuid: Data in a Simplex, which is later translated to \\(R^n\\) by a carefully constructed basis expansion\n\n\n\n\n\n\n\n\n\nThe curious case of Compositional Data Analysis\n\n\n\nSometimes, the data doesn’t “live” in our normal, intuitive, euclidian space \\(\\mathbb{R}^n\\). There are cases when the object of our analysis are proportions or compositions: think of what a material is made of, the proportion of the demand for different sizes of a shoe or garment.\nWe don’t necessarily care about their absolute value, but about their relative proportion. If we blindly apply traditional methods, or even statistical summaries, we will quickly hit weird results and paradoxes. So, we have to tweak existing methods we have make sense for compositions.\nCompositional data analysis solves those issues by defining a probability triple over the simplex (instead of \\(\\mathbb{R}^n\\)): \\((\\mathcal{S}^n, \\mathcal{F}, \\mathbb{P})\\). This leads to a different definition of the event space \\(\\mathcal{F}\\), which is also a sigma-algebra and a different definition of the probability measure \\(\\mathbb{P}\\).\n\n\nRemember our exaple of pigeon superstition in the context of learning? It is not surprising to me that measure theory becomes important in Learning Theory,12 which is exactly those carefully formulated principles that will prevent our automated learners to become supersitious.12 Even though most courses from which I studied don’t mention it explicitly (Yaser Abu-Mostafa, Shai Ben-David, Reza Shadmehr), according to Mikio’s Brown answer it’s essential in the idea of uniform convergence and its bounds, where “you consider the probability of a supremum over an infinite set of functions, but out of the box measure theory only allows for constructions with countably infinite index sets”.\nFor the next example, you don’t have to understand what Gaussian Processes are or are used for. However, later in the course, we will discuss nonparametric methods for hypothesis testing. Their usefulness comes from the fact that we make less distributional assumptions about our population, therefore getting more robust results, in contrast with choosing a wrong model or distribution.\nIt’s not that these methods don’t have parameters, but the parametrization varies depending on how much data we have, which makes them very flexible in a wide variety of applications, where we just don’t know what is a reasonable distribution or parametric functional form for the relationship that we model.\n\n\n\n\n\n\nNonparametrics and Gaussian Processes\n\n\n\n\nIf we’re thinking about a regression from the nonparametric perspective: that is, over a set of abstract functions: \\(f(x) \\in \\mathscr{C}^2:X \\rightarrow \\mathbb{R}\\), 13 we might want to know how a draw of samples from an infinite set of continuous differentiable functions might look like.13 \n\n\n\n\nSource: Bizovi: A posterior distribution of the Gaussian Processes, when conditioned on data\n\n\n\n\\[\nf(x) \\sim GP(\\mu(x); K(x,x'))\n\\]\nThe questions arises: how to define a PDF (probability density function) in this space? In my bachelor thesis, I got away with using Gaussian Processes, which are a very special class of stochastic processes. In this special case I could informally define an apriori distribution by defining the mean vector and Kernel (covariance function), then condition it on observed data with a Normal Likelihood.\n\\[\np(f(x) \\, |\\left \\{ x\\right \\})=\\frac{p(\\left \\{ x\\right \\}| \\, f) \\, \\mathbf{p(f)}}{p(\\left \\{ x\\right \\})}\n\\]\n\n\n\n\nIn the case of stochastic processes, we work with a sequence of random variables \\(\\{X_t, t \\in T \\}\\) and start asking questions:\n\nWhat kind of time dependency is there? (autocorrelation)\nWhat is the long-run behavior?\nCan we say something about extreme events?\n\nA lot of important applications in economics and finance are dynamic, so we have to work with time series very often. It gets worse when data is correlated not only in time, but also geographically – which is why the field of spatio-temporal data analysis is in such demand right now for policy-making.\nThus, a natural extension of this probability machinery we discussed so far is stochastic processes, underlying these dynamical systems. We can look at our time series as a manifestation, a particular instantiation of this latent process. Depending on which one we choose, we can model a wide range of phenomena.\n\n\n\n\nWe already raised the issue of where does uncertainty come from, mentioning that this question deserves further investigation. In order to do that, we will take an AI, Decision-Making, or Reinforcement Learning perspective, rather than a statistical or econometric one, which will be also very clear later.\nThink of agents who act and interact with their environment and have particular objectives or payoffs. Hopefully, they act in an intelligent way to achieve their goals in time. Given a past sequence of observations \\((x_1, x_2, ..., x_t)\\) and knowledge about the environment, the agent has to choose an action that best achieves its objectives in the presence of various sources of uncertainty. 1414 This paragraph is taken from the brilliant, open book Algorithms for Decision Making, MIT Press - 2022 by M. Kochenderfer, T. Wheeler, and K. Wray\n\n\n\n\n\nflowchart LR\n    Env(Environment) -- observation --> A(Agent)\n    A -- action --> Env\n\n\n\n\n\nFigure 2: This diagram captures the story from the previous chapter in a way which allows itself to a more formal treatment, a kind of joint learning, representation, and dynamic optimization.\n\n\n\n\n\nOutcome uncertainty, as the consequences and effects of actions are uncertain. Can we be sure that an intervention works out? We can’t take everything into account when making a decision – it becomes combinatorialy explosive.\nModel uncertainty, that is we can’t be sure that our understanding, model as a simplification of reality is the right one. In decision-making, we often mis-frame problems and in statistics, well, choose the wrong model.\nState uncertainty was discussed at length before, where the true state of the environment is uncertain and moreover, everything changes and is in flux.\nInteraction uncertainty, due to the behavior of the other agents interacting in the environment. For example, competitive firms, social network effects.\n\nWe will focus very little on this last aspect of uncertainty, but you have some tools to reason about it: game-theoretic arguments, ideas from multi-agent systems and agent-based modeling in cybernetics, and finally, graph theory. I think it is an important dimension of reality, however, taking it into account in this course would make it incomparably more complicated and advanced.\nTODO: Provide examples"
  }
]